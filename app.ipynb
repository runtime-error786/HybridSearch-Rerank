{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = r\"C:\\Users\\musta\\OneDrive\\Desktop\\hybrid search\\data\\Searching_for_Best_Practices_in_Retrieval-Augmente.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyMuPDFLoader(pdf_path)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for Best Practices in Retrieval-Augmented\n",
      "Generation\n",
      "Xiaohua Wang, Zhenghua Wang, Xuan Gao, Feiran Zhang,\n",
      "Yixin Wu, Zhibo Xu, Tianyuan Shi, Zhengyuan Wang, Shizheng Li,\n",
      "Qi Qian, Ruicheng Yin, Changze Lv, Xiaoqing Zheng∗\n",
      ", Xuanjing Huang\n",
      "School of Computer Science, Fudan University, Shanghai, China\n",
      "Shanghai Key Laboratory of Intelligent Information Processing\n",
      "{xiaohuawang22,zhenghuawang23}@m.fudan.edu.cn\n",
      "{zhengxq,xjhuang}@fudan.edu.cn\n",
      "Abstract\n",
      "Retrieval-augmented generation (RAG) techniques have proven to be effective\n",
      "in integrating up-to-date information, mitigating hallucinations, and enhancing\n",
      "response quality, particularly in specialized domains. While many RAG approaches\n",
      "have been proposed to enhance large language models through query-dependent\n",
      "retrievals, these approaches still suffer from their complex implementation and\n",
      "prolonged response times. Typically, a RAG workflow involves multiple processing\n",
      "steps, each of which can be executed in various ways. Here, we investigate\n",
      "existing RAG approaches and their potential combinations to identify optimal\n",
      "RAG practices. Through extensive experiments, we suggest several strategies\n",
      "for deploying RAG that balance both performance and efficiency. Moreover,\n",
      "we demonstrate that multimodal retrieval techniques can significantly enhance\n",
      "question-answering capabilities about visual inputs and accelerate the generation\n",
      "of multimodal content using a “retrieval as generation” strategy. Resources are\n",
      "available at https://github.com/FudanDNN-NLP/RAG.\n",
      "1\n",
      "Introduction\n",
      "Generative large language models are prone to producing outdated information or fabricating facts,\n",
      "although they were aligned with human preferences by reinforcement learning [1] or lightweight\n",
      "alternatives [2–5]. Retrieval-augmented generation (RAG) techniques address these issues by com-\n",
      "bining the strengths of pretraining and retrieval-based models, thereby providing a robust framework\n",
      "for enhancing model performance [6]. Furthermore, RAG enables rapid deployment of applications\n",
      "for specific organizations and domains without necessitating updates to the model parameters, as\n",
      "long as query-related documents are provided.\n",
      "Many RAG approaches have been proposed to enhance large language models (LLMs) through\n",
      "query-dependent retrievals [6–8]. A typical RAG workflow usually contains multiple intervening\n",
      "processing steps: query classification (determining whether retrieval is necessary for a given input\n",
      "query), retrieval (efficiently obtaining relevant documents for the query), reranking (refining the\n",
      "order of retrieved documents based on their relevance to the query), repacking (organizing the\n",
      "retrieved documents into a structured one for better generation), summarization (extracting key\n",
      "information for response generation from the repacked document and eliminating redundancies)\n",
      "modules. Implementing RAG also requires decisions on the ways to properly split documents into\n",
      "chunks, the types of embeddings to use for semantically representing these chunks, the choice of\n",
      "∗Corresponding Author.\n",
      "Preprint. Under review.\n",
      "arXiv:2407.01219v1  [cs.CL]  1 Jul 2024\n",
      "\n",
      "Large Language Model\n",
      "Query Classification\n",
      "Reranking\n",
      "DLM-based\n",
      "monoT5\n",
      "monoBERT\n",
      "RankLLaMA\n",
      "TILDE\n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "Retrieval\n",
      "Original Query\n",
      "BM25\n",
      "Contriever\n",
      "LLM-Embedder\n",
      "Query Rewriting\n",
      "Query Decomposition\n",
      "HyDE\n",
      "Hybrid Search\n",
      "HyDE+Hybrid Search\n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      "   \n",
      "Summarization\n",
      "Extractive\n",
      "Recomp\n",
      "BM25\n",
      "Contriever\n",
      "Abstractive\n",
      "LongLLMlingua\n",
      "SelectiveContext\n",
      "Recomp\n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "Vector Database\n",
      "Milvus\n",
      "Faiss\n",
      "Weaviate\n",
      "Qdrant\n",
      "Chroma\n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "Chunking\n",
      "Chunking Size\n",
      "Small2big\n",
      "Sliding Windows\n",
      "Chunk Metadata\n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      " \n",
      "Embedding\n",
      "LLM-Embedder\n",
      "intfloat/e5\n",
      "BAAI/bge\n",
      "Jina-embeddings-v2\n",
      "Gte\n",
      "all-mpnet-base-v2\n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      "     \n",
      "Repacking\n",
      "Sides\n",
      "Forward\n",
      "Reverse\n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      "  \n",
      "Evaluation\n",
      "General Performance\n",
      "Specific Domains\n",
      "Retrieval Capability\n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      "  \n",
      "Fine-tune\n",
      "Disturb\n",
      "Random\n",
      "Normal\n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      "    \n",
      "Retrieval Source\n",
      "Figure 1: Retrieval-augmented generation workflow. This study investigates the contribution of\n",
      "each component and provides insights into optimal RAG practices through extensive experimentation.\n",
      "The optional methods considered for each component are indicated in bold fonts, while the methods\n",
      "underlined indicate the default choice for individual modules. The methods indicated in blue font\n",
      "denote the best-performing selections identified empirically.\n",
      "vector databases to efficiently store feature representations, and the methods for effectively fine-tuning\n",
      "LLMs (see Figure 1).\n",
      "What adds complexity and challenge is the variability in implementing each processing step. For\n",
      "example, in retrieving relevant documents for an input query, various methods can be employed.\n",
      "One approach involves rewriting the query first and using the rewritten queries for retrieval [9].\n",
      "Alternatively, pseudo-responses to the query can be generated first, and the similarity between\n",
      "these pseudo-responses and the backend documents can be compared for retrieval [10]. Another\n",
      "option is to directly employ embedding models, typically trained in a contrastive manner using\n",
      "positive and negative query-response pairs [11, 12]. The techniques chosen for each step and their\n",
      "combinations significantly impact both the effectiveness and efficiency of RAG systems. To the best\n",
      "of our knowledge, there has been no systematic effort to pursue the optimal implementation of RAG,\n",
      "particularly for the entire RAG workflow.\n",
      "In this study, we aim to identify the best practices for RAG through extensive experimentation. Given\n",
      "the infeasibility of testing all possible combinations of these methods, we adopt a three-step approach\n",
      "to identify optimal RAG practices. First, we compare representative methods for each RAG step (or\n",
      "module) and select up to three of the best-performing methods. Next, we evaluate the impact of each\n",
      "method on the overall RAG performance by testing one method at a time for an individual step, while\n",
      "keeping the other RAG modules unchanged. This allows us to determine the most effective method\n",
      "for each step based on its contribution and interaction with other modules during response generation.\n",
      "Once the best method is chosen for a module, it is used in subsequent experiments. Finally, we\n",
      "empirically explore a few promising combinations suitable for different application scenarios where\n",
      "efficiency might be prioritized over performance, or vice versa. Based on these findings, we suggest\n",
      "several strategies for deploying RAG that balance both performance and efficiency.\n",
      "The contributions of this study are three-fold:\n",
      "• Through extensive experimentation, we thoroughly investigated existing RAG approaches and their\n",
      "combinations to identify and recommend optimal RAG practices.\n",
      "2\n",
      "\n",
      "• We introduce a comprehensive framework of evaluation metrics and corresponding datasets to\n",
      "comprehensively assess the performance of retrieval-augmented generation models, covering\n",
      "general, specialized (or domain-specific), and RAG-related capabilities.\n",
      "• We demonstrate that the integration of multimodal retrieval techniques can substantially improve\n",
      "question-answering capabilities on visual inputs and speed up the generation of multimodal content\n",
      "through a strategy of “retrieval as generation”.\n",
      "2\n",
      "Related Work\n",
      "Ensuring the accuracy of responses generated by Large Language Models (LLMs) such as Chat-\n",
      "GPT [13] and LLaMA [14] is essential. However, simply enlarging model size does not fundamentally\n",
      "address the issue of hallucinations [15, 16], especially in knowledge-intensive tasks and specialized\n",
      "domains. Retrieval-augmented generation (RAG) addresses these challenges by retrieving relevant\n",
      "documents from external knowledge bases, providing accurate, real-time, domain-specific context to\n",
      "LLMs [6]. Previous works have optimized the RAG pipeline through query and retrieval transfor-\n",
      "mations, enhancing retriever performance, and fine-tuning both the retriever and generator. These\n",
      "optimizations improve the interaction between input queries, retrieval mechanisms, and generation\n",
      "processes, ensuring the accuracy and relevance of responses.\n",
      "2.1\n",
      "Query and Retrieval Transformation\n",
      "Effective retrieval requires queries accurate, clear, and detailed. Even when converted into embed-\n",
      "dings, semantic differences between queries and relevant documents can persist. Previous works have\n",
      "explored methods to enhance query information through query transformation, thereby improving\n",
      "retrieval performance. For instance, Query2Doc [17] and HyDE [10] generate pseudo-documents\n",
      "from original queries to enhance retrieval, while TOC [18] decomposes queries into subqueries,\n",
      "aggregating the retrieved content for final results.\n",
      "Other studies have focused on transforming retrieval source documents. LlamaIndex [19] provides an\n",
      "interface to generate pseudo-queries for retrieval documents, improving matching with real queries.\n",
      "Some works employ contrastive learning to bring query and document embeddings closer in semantic\n",
      "space [12, 20, 21]. Post-processing retrieved documents is another method to enhance generator\n",
      "output, with techniques like hierarchical prompt summarization [22] and using abstractive and\n",
      "extractive compressors [23] to reduce context length and remove redundancy [24].\n",
      "2.2\n",
      "Retriever Enhancement Strategy\n",
      "Document chunking and embedding methods significantly impact retrieval performance. Common\n",
      "chunking strategies divide documents into chunks, but determining optimal chunk length can be\n",
      "challenging. Small chunks may fragment sentences, while large chunks might include irrelevant\n",
      "context. LlamaIndex [19] optimizes the chunking method like Small2Big and sliding window.\n",
      "Retrieved chunks can be irrelevant and numbers can be large, so reranking is necessary to filter\n",
      "irrelevant documents. A common reranking approach employs deep language models such as\n",
      "BERT [25], T5 [26], or LLaMA [27], which requires slow inference steps during reranking but grants\n",
      "better performance. TILDE [28, 29] achieves efficiency by precomputing and storing the likelihood\n",
      "of query terms, ranking documents based on their sum.\n",
      "2.3\n",
      "Retriever and Generator Fine-tuning\n",
      "Fine-tuning within the RAG framework is crucial for optimizing both retrievers and generators. Some\n",
      "research focuses on fine-tuning the generator to better utilize retriever context [30–32], ensuring\n",
      "faithful and robust generated content. Others fine-tune the retriever to learn to retrieve beneficial\n",
      "passages for the generator [33–35]. Holistic approaches treat RAG as an integrated system, fine-tuning\n",
      "both retriever and generator together to enhance overall performance [36–38], despite increased\n",
      "complexity and integration challenges.\n",
      "Several surveys have extensively discussed current RAG systems, covering aspects like text genera-\n",
      "tion [7, 8], integration with LLMs [6, 39], multimodal [40], and AI-generated content [41]. While\n",
      "these surveys provide comprehensive overviews of existing RAG methodologies, selecting the appro-\n",
      "3\n",
      "\n",
      "Which city will the next World Cup be held?                                         \n",
      " \n",
      " \n",
      "           < Search >\n",
      "\"French.Washington played a \n",
      "crucial role in the American \n",
      "Revolutionary War, leading the \n",
      "Continental Army against the \n",
      "British. \"\n",
      "Please continue writing the \n",
      "above paragraph.      \n",
      "              < Continuation writing >\n",
      "Background Knowledge\n",
      "\"To be, or not to be, that is the \n",
      "question.\" \n",
      "Please translate this sentence into \n",
      "French. \n",
      "      \n",
      "< Translation >\n",
      "Insufficient information\n",
      "Sufficient information\n",
      "Please give me a plan for holding a graduation party.              \n",
      " \n",
      " \n",
      "       < Planning >\n",
      "If you're currently a computer science student and your \n",
      "computer system encounters a malfunction, what should \n",
      "you do?               \n",
      " \n",
      "      < Role-play >\n",
      "Write an article about the geography of Europe, focusing \n",
      "on the changes in rainfall in the western part of the \n",
      "country.                                    \n",
      "          < Writing >\n",
      "No Retrieval Needed\n",
      "Need to Retrieval\n",
      "Please find a novel that is as \n",
      "famous as \"One Hundred Years \n",
      "of Solitude\".               < Search >\n",
      "\"Dave is attending his aunt's \n",
      "brother funeral today.\"\n",
      "Paraphrase the given information \n",
      "effectively. \n",
      "              < Rewriting >\n",
      "\"The Renaissance was a \n",
      "cultural transformation in \n",
      "European history, marking the \n",
      "revival of arts, sciences, and \n",
      "humanistic thought. The \n",
      "fervor of artists and scholars \n",
      "propelled prosperity and \n",
      "innovation in arts, literature, \n",
      "and science.\" Give me a \n",
      "summary.\n",
      "                    < Summarization >\n",
      "Identify who is football players: \n",
      "Messi, Jordan, Kobe. \n",
      "          \n",
      "          < Closed QA >\n",
      "Tom has three sisters, and each \n",
      "sister has a brother. How many \n",
      "siblings are there in total?  \n",
      "          \n",
      "< Reasonning >\n",
      "Q: 3,1 A: 3   Q: 2,5 A: 5   \n",
      "Q: 5,7 A: ?\n",
      "   < In-context learning >                              \n",
      "\"ChatGPT is a product of \n",
      "OpenAI.\" \n",
      "Please provide the ownership \n",
      "relationship. \n",
      "       < Information extraction >\n",
      "No Background Knowledge\n",
      "If I want to travel from Los Angeles to New York and I \n",
      "want to choose the cheapest mode of transportation, \n",
      "should I drive or take a plane?           < Decision making >\n",
      "I had a quarrel with my parents because they oppose my \n",
      "relationship with my boyfriend, but we genuinely love \n",
      "each other. How should I persuade my parents to accept \n",
      "our relationship?            \n",
      " \n",
      "   < Suggestion >\n",
      "Figure 2: Classification of retrieval requirements for different tasks. In cases where information is\n",
      "not provided, we differentiate tasks based on the functions of the model.\n",
      "priate algorithm for practical implementation remains challenging. In this paper, we focus on best\n",
      "practices for applying RAG methods, advancing the understanding and application of RAG in LLMs.\n",
      "3\n",
      "RAG Workflow\n",
      "In this section, we detail the components of the RAG workflow. For each module, we review\n",
      "commonly used approaches and select the default and alternative methods for our final pipeline.\n",
      "Section 4 will discuss best practices. Figure 1 presents the workflow and methods for each module.\n",
      "Detailed experimental setups, including datasets, hyperparameters, and results are provided in\n",
      "Appendix A.\n",
      "3.1\n",
      "Query Classification\n",
      "Not all queries require retrieval-augmented due to the inherent capabilities of LLMs. While RAG can\n",
      "enhance information accuracy and reduce hallucinations, frequent retrieval can increase response\n",
      "time. Therefore, we begin by classifying queries to determine the necessity of retrieval. Queries\n",
      "requiring retrieval proceed through the RAG modules; others are handled directly by LLMs.\n",
      "Retrieval is generally recommended when knowledge beyond the model’s parameters is needed.\n",
      "However, the necessity of retrieval varies by task. For instance, an LLM trained up to 2023 can\n",
      "handle a translation request for “Sora was developed by OpenAI” without retrieval. Conversely, an\n",
      "introduction request for the same topic would require retrieval to provide relevant information.\n",
      "Therefore, we propose classifying tasks by type to determine if a query needs retrieval. We categorize\n",
      "Model\n",
      "Metrics\n",
      "Acc Prec Rec\n",
      "F1\n",
      "BERT-base-multilingual 0.95 0.96 0.94 0.95\n",
      "Table 1: Results of the Query Classifier.\n",
      "15 tasks based on whether they provide suffi-\n",
      "cient information, with specific tasks and exam-\n",
      "ples illustrated in Figure 2. For tasks entirely\n",
      "based on user-given information, we denote as\n",
      "“sufficient”, which need not retrieval; otherwise,\n",
      "we denote as “insufficient”, and retrieval may\n",
      "be necessary. We train a classifier to automate\n",
      "this decision-making process. Experimental de-\n",
      "tails are presented in Appendix A.1. Section 4\n",
      "explores the impact of query classification on the workflow, comparing scenarios with and without\n",
      "classification.\n",
      "4\n",
      "\n",
      "Embedding Model\n",
      "namespace-Pt/msmarco\n",
      "MRR@1 MRR@10 MRR@100\n",
      "R@1\n",
      "R@10 R@100\n",
      "BAAI/LLM-Embedder [20]\n",
      "24.79\n",
      "37.58\n",
      "38.62\n",
      "24.07\n",
      "66.45\n",
      "90.75\n",
      "BAAI/bge-base-en-v1.5 [12]\n",
      "23.34\n",
      "35.80\n",
      "36.94\n",
      "22.63\n",
      "64.12\n",
      "90.13\n",
      "BAAI/bge-small-en-v1.5 [12]\n",
      "23.27\n",
      "35.78\n",
      "36.89\n",
      "22.65\n",
      "63.92\n",
      "89.80\n",
      "BAAI/bge-large-en-v1.5 [12]\n",
      "24.63\n",
      "37.48\n",
      "38.59\n",
      "23.91\n",
      "65.57\n",
      "90.60\n",
      "BAAI/bge-large-en [12]\n",
      "24.84\n",
      "37.66\n",
      "38.73\n",
      "24.13\n",
      "66.09\n",
      "90.64\n",
      "BAAI/bge-small-en [12]\n",
      "23.28\n",
      "35.79\n",
      "36.91\n",
      "22.62\n",
      "63.96\n",
      "89.67\n",
      "BAAI/bge-base-en [12]\n",
      "23.47\n",
      "35.94\n",
      "37.07\n",
      "22.73\n",
      "64.17\n",
      "90.14\n",
      "Alibaba-NLP/gte-large-en-v1.5 [21]\n",
      "8.93\n",
      "15.60\n",
      "16.71\n",
      "8.67\n",
      "32.28\n",
      "60.36\n",
      "thenlper/gte-base [21]\n",
      "7.42\n",
      "13.23\n",
      "14.30\n",
      "7.21\n",
      "28.27\n",
      "56.20\n",
      "thenlper/gte-small [21]\n",
      "7.97\n",
      "14.81\n",
      "15.95\n",
      "7.71\n",
      "32.07\n",
      "61.08\n",
      "jinaai/jina-embeddings-v2-small-en [42]\n",
      "8.07\n",
      "15.02\n",
      "16.12\n",
      "7.87\n",
      "32.55\n",
      "60.36\n",
      "intfloat/e5-small-v2 [11]\n",
      "10.04\n",
      "18.23\n",
      "19.41\n",
      "9.74\n",
      "38.92\n",
      "68.42\n",
      "intfloat/e5-large-v2 [11]\n",
      "9.58\n",
      "17.94\n",
      "19.03\n",
      "9.35\n",
      "39.00\n",
      "66.11\n",
      "sentence-transformers/all-mpnet-base-v2\n",
      "5.80\n",
      "11.26\n",
      "12.26\n",
      "5.66\n",
      "25.57\n",
      "50.94\n",
      "Table 2: Results for different embedding models on namespace-Pt/msmarco.\n",
      "3.2\n",
      "Chunking\n",
      "Chunking documents into smaller segments is crucial for enhancing retrieval precision and avoiding\n",
      "length issues in LLMs. This process can be applied at various levels of granularity, such as token,\n",
      "sentence, and semantic levels.\n",
      "• Token-level Chunking is straightforward but may split sentences, affecting retrieval quality.\n",
      "• Semantic-level Chunking uses LLMs to determine breakpoints, context-preserving but time-\n",
      "consuming.\n",
      "• Sentence-level Chunking balances preserving text semantics with simplicity and efficiency.\n",
      "In this study, we use sentence-level chunking, balancing simplicity and semantic preservation. We\n",
      "examine chunking from four dimensions.\n",
      "3.2.1\n",
      "Chunk Size\n",
      "Chunk size significantly impacts performance. Larger chunks provide more context, enhancing\n",
      "comprehension but increasing process time. Smaller chunks improve retrieval recall and reduce time\n",
      "but may lack sufficient context.\n",
      "Finding the optimal chunk size involves a balance between some metrics such as faithfulness, and\n",
      "relevancy. Faithfulness measures whether the response is hallucinated or matches the retrieved texts.\n",
      "Chunk Size\n",
      "lyft_2021\n",
      "Average\n",
      "Faithfulness\n",
      "Average\n",
      "Relevancy\n",
      "2048\n",
      "80.37\n",
      "91.11\n",
      "1024\n",
      "94.26\n",
      "95.56\n",
      "512\n",
      "97.59\n",
      "97.41\n",
      "256\n",
      "97.22\n",
      "97.78\n",
      "128\n",
      "95.74\n",
      "97.22\n",
      "Table 3: Comparison of different chunk sizes.\n",
      "Relevancy measures whether the retrieved texts\n",
      "and responses match queries.\n",
      "We use the\n",
      "evaluation module of LlamaIndex [43] to cal-\n",
      "culate the metrics above.\n",
      "For embedding,\n",
      "we use the text-embedding-ada-0022 model,\n",
      "which supports long input length. We choose\n",
      "zephyr-7b-alpha3 and gpt-3.5-turbo4 as\n",
      "generation model and evaluation model respec-\n",
      "tively. The size of the chunk overlap is 20 tokens.\n",
      "First sixty pages of the document lyft_20215\n",
      "are used as corpus, then prompting LLMs to\n",
      "generate about one hundred and seventy queries\n",
      "according to chosen corpus. The impact of different chunk sizes is shown in Table 3.\n",
      "2https://platform.openai.com/docs/guides/embeddings/embedding-models\n",
      "3https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha\n",
      "4https://www.openai.com/\n",
      "5https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/\n",
      "data/10k/lyft_2021.pdf\n",
      "5\n",
      "\n",
      "3.2.2\n",
      "Chunking Techniques\n",
      "Advanced techniques such as small-to-big and sliding window improve retrieval quality by organizing\n",
      "chunk block relationships. Small-sized blocks are used to match queries, and larger blocks that\n",
      "include the small ones along with contextual information are returned.\n",
      "To demonstrate the effectiveness of advanced chunking techniques, we use the LLM-Embedder [20]\n",
      "model as an embedding model. The smaller chunk size is 175 tokens, the larger chunk size is 512\n",
      "tokens and the chunk overlap is 20 tokens. Techniques like small-to-big and sliding window improve\n",
      "retrieval quality by maintaining context and ensuring relevant information is retrieved. Detailed\n",
      "results are shown in Table 4.\n",
      "3.2.3\n",
      "Embedding Model Selection\n",
      "Choosing the right embedding model is crucial for effective semantic matching of queries\n",
      "and chunk blocks. We use the evaluation module of FlagEmbedding6 which uses the dataset\n",
      "Chunk Skill\n",
      "lyft_2021\n",
      "Average\n",
      "Faithfulness\n",
      "Average\n",
      "Relevancy\n",
      "Original\n",
      "95.74\n",
      "95.37\n",
      "small2big\n",
      "96.67\n",
      "95.37\n",
      "sliding window\n",
      "97.41\n",
      "96.85\n",
      "Table 4: Comparison of different chunk skills.\n",
      "namespace-Pt/msmarco7\n",
      "as\n",
      "queries\n",
      "and\n",
      "dataset namespace-Pt/msmarco-corpus8 as\n",
      "corpus to choose the appropriate open source\n",
      "embedding model.\n",
      "As shown in Table 2,\n",
      "LLM-Embedder [20] achieves comparable\n",
      "results with BAAI/bge-large-en [12], however,\n",
      "the size of the former is three times smaller\n",
      "than that of the latter.\n",
      "Thus, we select the\n",
      "LLM-Embedder [20] for its balance of\n",
      "performance and size.\n",
      "3.2.4\n",
      "Metadata Addition\n",
      "Enhancing chunk blocks with metadata like titles, keywords, and hypothetical questions can improve\n",
      "retrieval, provide more ways to post-process retrieved texts, and help LLMs better understand\n",
      "retrieved information. A detailed study on metadata inclusion will be addressed in future work.\n",
      "3.3\n",
      "Vector Databases\n",
      "Vector databases store embedding vectors with their metadata, enabling efficient retrieval of doc-\n",
      "uments relevant to queries through various indexing and approximate nearest neighbor (ANN)\n",
      "methods.\n",
      "To select an appropriate vector database for our research, we evaluated several options based on\n",
      "four key criteria: multiple index types, billion-scale vector support, hybrid search, and cloud-native\n",
      "Database\n",
      "Multiple\n",
      "Index Type\n",
      "Billion-\n",
      "Scale\n",
      "Hybrid\n",
      "Search\n",
      "Cloud-\n",
      "Native\n",
      "Weaviate\n",
      "✗\n",
      "✗\n",
      "✓\n",
      "✓\n",
      "Faiss\n",
      "✓\n",
      "✗\n",
      "✗\n",
      "✗\n",
      "Chroma\n",
      "✗\n",
      "✗\n",
      "✓\n",
      "✓\n",
      "Qdrant\n",
      "✗\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "Milvus\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "Table 5: Comparison of Various Vector Databases\n",
      "capabilities. These criteria were chosen for their\n",
      "impact on flexibility, scalability, and ease of\n",
      "deployment in modern, cloud-based infrastruc-\n",
      "tures. Multiple index types provide the flexibil-\n",
      "ity to optimize searches based on different data\n",
      "characteristics and use cases. Billion-scale vec-\n",
      "tor support is crucial for handling large datasets\n",
      "in LLM applications. Hybrid search combines\n",
      "vector search with traditional keyword search,\n",
      "enhancing retrieval accuracy. Finally, cloud-\n",
      "native capabilities ensure seamless integration, scalability, and management in cloud environments.\n",
      "Table 5 presents a detailed comparison of five open-source vector databases: Weaviate, Faiss,\n",
      "Chroma, Qdrant, and Milvus.\n",
      "Our evaluation indicates that Milvus stands out as the most comprehensive solution among the\n",
      "databases evaluated, meeting all the essential criteria and outperforming other open-source options.\n",
      "6https://github.com/FlagOpen/FlagEmbedding\n",
      "7https://huggingface.co/datasets/namespace-Pt/msmarco\n",
      "8https://huggingface.co/datasets/namespace-Pt/msmarco-corpus\n",
      "6\n",
      "\n",
      "Method\n",
      "TREC DL19\n",
      "TREC DL20\n",
      "mAP\n",
      "nDCG@10\n",
      "R@50\n",
      "R@1k\n",
      "Latency\n",
      "mAP\n",
      "nDCG@10\n",
      "R@50\n",
      "R@1k\n",
      "Latency\n",
      "unsupervised\n",
      "BM25\n",
      "30.13\n",
      "50.58\n",
      "38.32\n",
      "75.01\n",
      "0.07\n",
      "28.56\n",
      "47.96\n",
      "46.18\n",
      "78.63\n",
      "0.29\n",
      "Contriever\n",
      "23.99\n",
      "44.54\n",
      "37.54\n",
      "74.59\n",
      "3.06\n",
      "23.98\n",
      "42.13\n",
      "43.81\n",
      "75.39\n",
      "0.98\n",
      "supervised\n",
      "LLM-Embedder\n",
      "44.66\n",
      "70.20\n",
      "49.06\n",
      "84.48\n",
      "2.61\n",
      "45.60\n",
      "68.76\n",
      "61.36\n",
      "84.41\n",
      "0.71\n",
      "+ Query Rewriting\n",
      "44.56\n",
      "67.89\n",
      "51.45\n",
      "85.35\n",
      "7.80\n",
      "45.16\n",
      "65.62\n",
      "59.63\n",
      "83.45\n",
      "2.06\n",
      "+ Query Decomposition\n",
      "41.93\n",
      "66.10\n",
      "48.66\n",
      "82.62\n",
      "14.98\n",
      "43.30\n",
      "64.95\n",
      "57.74\n",
      "84.18\n",
      "2.01\n",
      "+ HyDE\n",
      "50.87\n",
      "75.44\n",
      "54.93\n",
      "88.76\n",
      "7.21\n",
      "50.94\n",
      "73.94\n",
      "63.80\n",
      "88.03\n",
      "2.14\n",
      "+ Hybrid Search\n",
      "47.14\n",
      "72.50\n",
      "51.13\n",
      "89.08\n",
      "3.20\n",
      "47.72\n",
      "69.80\n",
      "64.32\n",
      "88.04\n",
      "0.77\n",
      "+ HyDE + Hybrid Search\n",
      "52.13\n",
      "73.34\n",
      "55.38\n",
      "90.42\n",
      "11.16\n",
      "53.13\n",
      "72.72\n",
      "66.14\n",
      "90.67\n",
      "2.95\n",
      "Table 6: Results for different retrieval methods on TREC DL19/20. The best result for each method\n",
      "is made bold and the second is underlined.\n",
      "Configuration\n",
      "TREC DL19\n",
      "TREC DL20\n",
      "mAP\n",
      "nDCG@10\n",
      "R@50\n",
      "R@1k\n",
      "latency\n",
      "mAP\n",
      "nDCG@10\n",
      "R@50\n",
      "R@1k\n",
      "Latency\n",
      "HyDE\n",
      "w/ 1 pseudo-doc\n",
      "48.77\n",
      "72.49\n",
      "53.20\n",
      "87.73\n",
      "8.08\n",
      "51.31\n",
      "70.37\n",
      "63.28\n",
      "87.81\n",
      "2.09\n",
      "w/ 1 pseudo-doc + query\n",
      "50.87\n",
      "75.44\n",
      "54.93\n",
      "88.76\n",
      "7.21\n",
      "50.94\n",
      "73.94\n",
      "63.80\n",
      "88.03\n",
      "2.14\n",
      "w/ 8 pseudo-doc + query\n",
      "51.64\n",
      "75.12\n",
      "54.51\n",
      "89.17\n",
      "14.15\n",
      "53.14\n",
      "73.65\n",
      "65.79\n",
      "88.67\n",
      "3.44\n",
      "Table 7: HyDE with different concatenation of hypothetical documents and queries.\n",
      "3.4\n",
      "Retrieval Methods\n",
      "Given a user query, the retrieval module selects the top-k relevant documents from a pre-built corpus\n",
      "based on the similarity between the query and the documents. The generation model then uses\n",
      "these documents to formulate an appropriate response to the query. However, original queries often\n",
      "underperform due to poor expression and lack of semantic information [6], negatively impacting the\n",
      "retrieval process. To address these issues, we evaluated three query transformation methods using the\n",
      "LLM-Embedder recommended in Section 3.2 as the query and document encoder:\n",
      "• Query Rewriting: Query rewriting refines queries to better match relevant documents. Inspired\n",
      "by the Rewrite-Retrieve-Read framework [9], we prompt an LLM to rewrite queries to enhance\n",
      "performance.\n",
      "• Query Decomposition: This approach involves retrieving documents based on sub-questions\n",
      "derived from the original query, which is more complex to comprehend and handle.\n",
      "• Pseudo-documents Generation: This approach generates a hypothetical document based on the\n",
      "user query and uses the embedding of hypothetical answers to retrieve similar documents. One\n",
      "notable implement is HyDE [10],\n",
      "Recent studies, such as [44], indicate that combining lexical-based search with vector search signifi-\n",
      "cantly enhances performance. In this study, we use BM25 for sparse retrieval and Contriever [45], an\n",
      "unsupervised contrastive encoder, for dense retrieval, serving as two robust baselines based on Thakur\n",
      "et al. [46].\n",
      "3.4.1\n",
      "Results for different retrieval methods\n",
      "We evaluated the performance of different search methods on the TREC DL 2019 and 2020 passage\n",
      "ranking datasets. The results presented in Table 6 show that supervised methods significantly\n",
      "outperformed unsupervised methods. Combining with HyDE and hybrid search, LLM-Embedder\n",
      "achieves the highest scores. However, query rewriting and query decomposition did not enhance\n",
      "retrieval performance as effectively. Considering the best performance and tolerated latency, we\n",
      "recommend Hybrid Search with HyDE as the default retrieval method. Taking efficiency into\n",
      "consideration, Hybrid Search combines sparse retrieval (BM25) and dense retrieval (Original\n",
      "embedding) and achieves notable performance with relatively low latency.\n",
      "3.4.2\n",
      "HyDE with Different Concatenation of Documents and Query\n",
      "Table 7 shows the impact of different concatenation strategies for hypothetical documents and queries\n",
      "using HyDE. Concatenating multiple pseudo-documents with the original query can significantly\n",
      "7\n",
      "\n",
      "Hyperparameter\n",
      "TREC DL19\n",
      "TREC DL20\n",
      "mAP\n",
      "nDCG@10\n",
      "R@50\n",
      "R@1k\n",
      "latency\n",
      "mAP\n",
      "nDCG@10\n",
      "R@50\n",
      "R@1k\n",
      "Latency\n",
      "Hybrid Search\n",
      "α = 0.1\n",
      "46.00\n",
      "70.87\n",
      "49.24\n",
      "88.89\n",
      "2.98\n",
      "46.54\n",
      "69.05\n",
      "63.36\n",
      "87.32\n",
      "0.90\n",
      "α = 0.3\n",
      "47.14\n",
      "72.50\n",
      "51.13\n",
      "89.08\n",
      "3.20\n",
      "47.72\n",
      "69.80\n",
      "64.32\n",
      "88.04\n",
      "0.77\n",
      "α = 0.5\n",
      "47.36\n",
      "72.24\n",
      "52.71\n",
      "88.09\n",
      "3.02\n",
      "47.19\n",
      "68.12\n",
      "64.90\n",
      "87.86\n",
      "0.87\n",
      "α = 0.7\n",
      "47.21\n",
      "71.89\n",
      "52.40\n",
      "88.01\n",
      "3.15\n",
      "45.82\n",
      "67.30\n",
      "64.23\n",
      "87.92\n",
      "1.02\n",
      "α = 0.9\n",
      "46.35\n",
      "70.67\n",
      "52.64\n",
      "88.22\n",
      "2.74\n",
      "44.02\n",
      "65.55\n",
      "63.22\n",
      "87.76\n",
      "1.20\n",
      "Table 8: Results of hybrid search with different alpha values.\n",
      "Method\n",
      "MS MARCO Passage ranking\n",
      "Base Model\n",
      "# Params\n",
      "MRR@1\n",
      "MRR@10\n",
      "MRR@1k\n",
      "Hit Rate@10\n",
      "Latency\n",
      "w/o Reranking\n",
      "Random Ordering\n",
      "-\n",
      "-\n",
      "0.011\n",
      "0.027\n",
      "0.068\n",
      "0.092\n",
      "-\n",
      "BM25\n",
      "-\n",
      "-\n",
      "6.52\n",
      "11.65\n",
      "12.59\n",
      "24.63\n",
      "-\n",
      "DLM Reranking\n",
      "monoT5\n",
      "T5-base\n",
      "220M\n",
      "21.62\n",
      "31.78\n",
      "32.40\n",
      "54.07\n",
      "4.5\n",
      "monoBERT\n",
      "BERT-large\n",
      "340M\n",
      "21.65\n",
      "31.69\n",
      "32.35\n",
      "53.38\n",
      "15.8\n",
      "RankLLaMA\n",
      "Llama-2-7b\n",
      "7B\n",
      "22.08\n",
      "32.35\n",
      "32.97\n",
      "54.53\n",
      "82.4\n",
      "TILDE Reranking\n",
      "TILDEv2\n",
      "BERT-base\n",
      "110M\n",
      "18.57\n",
      "27.83\n",
      "28.60\n",
      "49.07\n",
      "0.02\n",
      "Table 9: Results of different reranking methods on the dev set of the MS MARCO Passage ranking\n",
      "dataset. For each query, the top-1000 candidate passages retrieved by BM25 are reranked. Latency is\n",
      "measured in seconds per query.\n",
      "enhance retrieval performance, though at the cost of increased latency, suggesting a trade-off between\n",
      "retrieval effectiveness and efficiency. However, indiscriminately increasing the number of hypothetical\n",
      "documents does not yield significant benefits and substantially raises latency, indicating that using a\n",
      "single hypothetical document is sufficient.\n",
      "3.4.3\n",
      "Hybrid Search with Different Weight on Sparse Retrieval\n",
      "Table 8 presents the impact of different α values in hybrid search, where α controls the weighting\n",
      "between sparse retrieval and dense retrieval components. The relevance score is calculated as follows:\n",
      "Sh = α · Ss + Sd\n",
      "(1)\n",
      "where Ss, Sd are the normalized relevance scores from sparse retrieval and dense retrieval respectively,\n",
      "and Sh is the total retrieval score.\n",
      "We evaluated five different α values to determine their influence on performance. The results indicate\n",
      "that an α value of 0.3 yields the best performance, demonstrating that appropriate adjustment of\n",
      "α can enhance retrieval effectiveness to a certain extent. Therefore, we selected α = 0.3 for our\n",
      "retrieval and main experiments. Additional implementation details are presented in Appendix A.2.\n",
      "3.5\n",
      "Reranking Methods\n",
      "After the initial retrieval, a reranking phase is employed to enhance the relevance of the retrieved\n",
      "documents, ensuring that the most pertinent information appears at the top of the list. This phase uses\n",
      "more precise and time-intensive methods to reorder documents effectively, increasing the similarity\n",
      "between the query and the top-ranked documents.\n",
      "We consider two approaches in our reranking module: DLM Reranking, which utilizes classifi-\n",
      "cation, and TILDE Reranking, which focuses on query likelihoods. These approaches prioritize\n",
      "performance and efficiency, respectively.\n",
      "• DLM Reranking:\n",
      "This method leverages deep language models (DLMs) [25–27] for reranking.\n",
      "These models are fine-tuned to classify document relevancy to a query as “true” or “false”. During\n",
      "fine-tuning, the model is trained with concatenated query and document inputs, labeled by relevancy.\n",
      "At inference, documents are ranked based on the probability of the “true” token.\n",
      "• TILDE Reranking:\n",
      "TILDE [28, 29] calculates the likelihood of each query term independently\n",
      "by predicting token probabilities across the model’s vocabulary. Documents are scored by summing\n",
      "8\n",
      "\n",
      "Method\n",
      "NQ\n",
      "TQA\n",
      "HotPotQA\n",
      "Avg.\n",
      "Avg. Token\n",
      "F1\n",
      "#token\n",
      "F1\n",
      "#token\n",
      "F1\n",
      "#token\n",
      "w/o Summarization\n",
      "Origin Prompt\n",
      "27.07\n",
      "124\n",
      "33.61\n",
      "152\n",
      "33.92\n",
      "141\n",
      "31.53\n",
      "139\n",
      "Extractive Method\n",
      "BM25\n",
      "27.97\n",
      "40\n",
      "32.44\n",
      "59\n",
      "28.00\n",
      "63\n",
      "29.47\n",
      "54\n",
      "Contriever\n",
      "23.62\n",
      "42\n",
      "33.79\n",
      "65\n",
      "23.64\n",
      "60\n",
      "27.02\n",
      "56\n",
      "Recomp (extractive)\n",
      "27.84\n",
      "34\n",
      "35.32\n",
      "60\n",
      "29.46\n",
      "58\n",
      "30.87\n",
      "51\n",
      "Abstractive Method\n",
      "SelectiveContext\n",
      "25.05\n",
      "65\n",
      "34.25\n",
      "70\n",
      "34.43\n",
      "66\n",
      "31.24\n",
      "67\n",
      "LongLLMlingua\n",
      "21.32\n",
      "51\n",
      "32.81\n",
      "56\n",
      "30.79\n",
      "57\n",
      "28.29\n",
      "55\n",
      "Recomp (abstractive)\n",
      "33.68\n",
      "59\n",
      "35.87\n",
      "61\n",
      "29.01\n",
      "57\n",
      "32.85\n",
      "59\n",
      "Table 10: Comparison between different summarization methods.\n",
      "the pre-calculated log probabilities of query tokens, allowing for rapid reranking at inference.\n",
      "TILDEv2 improves this by indexing only document-present tokens, using NCE loss, and expanding\n",
      "documents, thus enhancing efficiency and reducing index size.\n",
      "Our experiments were conducted on the MS MARCO Passage ranking dataset [47], a large-scale\n",
      "dataset for machine reading comprehension. We follow and make modifications to the implementation\n",
      "provided by PyGaggle [26] and TILDE [28], using the models monoT5, monoBERT, RankLLaMA\n",
      "and TILDEv2. Reranking results are shown in Table 9. We recommend monoT5 as a comprehensive\n",
      "method balancing performance and efficiency. RankLLaMA is suitable for achieving the best\n",
      "performance, while TILDEv2 is ideal for the quickest experience on a fixed collection. Details on\n",
      "the experimental setup and results are presented in Appendix A.3.\n",
      "3.6\n",
      "Document Repacking\n",
      "The performance of subsequent processes, such as LLM response generation, may be affected by the\n",
      "order documents are provided. To address this issue, we incorporate a compact repacking module into\n",
      "the workflow after reranking, featuring three repacking methods: “forward”, “reverse” and “sides”.\n",
      "The “forward” method repacks documents by descending relevancy scores from the reranking phase,\n",
      "whereas the “reverse” arranges them in ascending order. Inspired by Liu et al. [48], concluding that\n",
      "optimal performance is achieved when relevant information is placed at the head or tail of the input,\n",
      "we also include a “sides” option.\n",
      "Since the repacking method primarily affects subsequent modules, we select the best repacking\n",
      "method in Section 4 by testing it in combination with other modules. In this section, we choose the\n",
      "“sides” method as the default repacking method.\n",
      "3.7\n",
      "Summarization\n",
      "Retrieval results may contain redundant or unnecessary information, potentially preventing LLMs\n",
      "from generating accurate responses. Additionally, long prompts can slow down the inference process.\n",
      "Therefore, efficient methods to summarize retrieved documents are crucial in the RAG pipeline.\n",
      "Summarization tasks can be extractive or abstractive. Extractive methods segment text into sen-\n",
      "tences, then score and rank them based on importance. Abstractive compressors synthesize infor-\n",
      "mation from multiple documents to rephrase and generate a cohesive summary. These tasks can be\n",
      "query-based or non-query-based. In this paper, as RAG retrieves information relevant to queries, we\n",
      "focus exclusively on query-based methods.\n",
      "• Recomp:\n",
      "Recomp [23] has extractive and abstractive compressors. The extractive compressor\n",
      "selects useful sentences, while the abstractive compressor synthesizes information from multiple\n",
      "documents.\n",
      "• LongLLMLingua:\n",
      "LongLLMLingua [49] improves LLMLingua by focusing on key informa-\n",
      "tion related to the query.\n",
      "• Selective Context\n",
      "Selective Context enhances LLM efficiency by identifying and removing\n",
      "redundant information in the input context. It evaluates the informativeness of lexical units using\n",
      "9\n",
      "\n",
      "self-information computed by a base causal language model. This method is non-query-based,\n",
      "allowing a comparison between query-based and non-query-based approaches.\n",
      "We evaluate these methods on three benchmark datasets: NQ, TriviaQA, and HotpotQA. Comparative\n",
      "results of different summarization methods are shown in Table 10. We recommend Recomp for\n",
      "its outstanding performance. LongLLMLingua does not perform well but demonstrates better\n",
      "generalization capabilities as it was not trained on these experimental datasets. Therefore, we consider\n",
      "it as an alternative method. Additional implementation details and discussions on non-query-based\n",
      "methods are provided in Appendix A.4.\n",
      "3.8\n",
      "Generator Fine-tuning\n",
      "In this section, we focus on fine-tuning the generator while leaving retriever fine-tuning for future\n",
      "exploration. We aim to investigate the impact of fine-tuning, particularly the influence of relevant or\n",
      "irrelevant contexts on the generator’s performance.\n",
      "Formally, we denote x as the query fed into the RAG system, and D as the contexts for this input.\n",
      "The fine-tuning loss of the generator is the negative log-likelihood of the ground-truth output y.\n",
      "To explore the impact of fine-tuning, especially relevant and irrelevant contexts, we define dgold as a\n",
      "context relevant to the query, and drandom as a randomly retrieved context. We train the model by\n",
      "varying the composition of D as follows:\n",
      "• Dg: The augmented context consists of query-relevant documents, denoted as Dg = {dgold}.\n",
      "• Dr: The context contains one randomly sampled document, denoted as Dr = {drandom}.\n",
      "• Dgr: The augmented context comprises a relevant document and a randomly-selected one, denoted\n",
      "as Dgr = {dgold, drandom}.\n",
      "• Dgg: The augmented context consists of two copies of a query-relevant document, denoted as\n",
      "Dgg = {dgold, dgold}.\n",
      "We denote the base LM generator not fine-tuned as Mb , and the model fine-tuned under the\n",
      "corresponding D as Mg, Mr, Mgr, Mgg. We fine-tuned our model on several QA and reading\n",
      "D\n",
      "Dg\n",
      "Dr\n",
      "Dgr\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "Coverage Score\n",
      "Models Trained with Different Method\n",
      "Mb\n",
      "Mg\n",
      "Mr\n",
      "Mgr\n",
      "Mgg\n",
      "Figure 3: Results of generator fine-tuning.\n",
      "comprehension datasets. Ground-truth coverage\n",
      "is used as our evaluation metric since QA task\n",
      "answers are relatively short. We select Llama-2-\n",
      "7B [50] as the base model. Similar to training,\n",
      "we evaluate all trained models on validation sets\n",
      "with Dg, Dr, Dgr, and D∅, where D∅indicates\n",
      "inference without retrieval. Figure 3 presents\n",
      "our main results. Models trained with a mix of\n",
      "relevant and random documents (Mgr) perform\n",
      "best when provided with either gold or mixed\n",
      "contexts. This suggests that mixing relevant and\n",
      "random contexts during training can enhance the\n",
      "generator’s robustness to irrelevant information\n",
      "while ensuring effective utilization of relevant\n",
      "contexts. Therefore, we identify the practice of\n",
      "augmenting with a few relevant and randomly-selected documents during training as the best\n",
      "approach. Detailed dataset information, hyperparameters and experimental results can be found in\n",
      "Appendix A.5.\n",
      "4\n",
      "Searching for Best RAG Practices\n",
      "In the following section, we investigate the optimal practices for implementing RAG. To begin\n",
      "with, we used the default practice identified in Section 3 for each module. Following the workflow\n",
      "depicted in Figure 1, we sequentially optimized individual modules and selected the most effective\n",
      "option among alternatives. This iterative process continued until we determined the best method for\n",
      "implementing the final summarization module. Based on Section 3.8, we used the Llama2-7B-Chat\n",
      "model fine-tuned where each query was augmented by a few random-selected and relevant documents\n",
      "10\n",
      "\n",
      "Method\n",
      "Commonsense\n",
      "Fact Check\n",
      "ODQA\n",
      "Multihop\n",
      "Medical\n",
      "RAG\n",
      "Avg.\n",
      "Acc\n",
      "Acc\n",
      "EM\n",
      "F1\n",
      "EM\n",
      "F1\n",
      "Acc\n",
      "Score\n",
      "Score\n",
      "F1\n",
      "Latency\n",
      "classification module , Hybrid with HyDE, monoT5, sides, Recomp\n",
      "w/o classification\n",
      "0.719\n",
      "0.505\n",
      "0.391\n",
      "0.450\n",
      "0.212\n",
      "0.255\n",
      "0.528\n",
      "0.540\n",
      "0.465\n",
      "0.353\n",
      "16.58\n",
      "+ classification\n",
      "0.727\n",
      "0.595\n",
      "0.393\n",
      "0.450\n",
      "0.207\n",
      "0.257\n",
      "0.460\n",
      "0.580\n",
      "0.478\n",
      "0.353\n",
      "11.71\n",
      "with classification,\n",
      "retrieval module , monoT5, sides, Recomp\n",
      "+ HyDE\n",
      "0.718\n",
      "0.595\n",
      "0.320\n",
      "0.373\n",
      "0.170\n",
      "0.213\n",
      "0.400\n",
      "0.545\n",
      "0.443\n",
      "0.293\n",
      "11.58\n",
      "+ Original\n",
      "0.721\n",
      "0.585\n",
      "0.300\n",
      "0.350\n",
      "0.153\n",
      "0.197\n",
      "0.390\n",
      "0.486\n",
      "0.428\n",
      "0.273\n",
      "1.44\n",
      "+ Hybrid\n",
      "0.718\n",
      "0.595\n",
      "0.347\n",
      "0.397\n",
      "0.190\n",
      "0.240\n",
      "0.750\n",
      "0.498\n",
      "0.477\n",
      "0.318\n",
      "1.45\n",
      "+ Hybrid with HyDE\n",
      "0.727\n",
      "0.595\n",
      "0.393\n",
      "0.450\n",
      "0.207\n",
      "0.257\n",
      "0.460\n",
      "0.580\n",
      "0.478\n",
      "0.353\n",
      "11.71\n",
      "with classification, Hybrid with HyDE, reranking module , sides, Recomp\n",
      "w/o reranking\n",
      "0.720\n",
      "0.591\n",
      "0.365\n",
      "0.429\n",
      "0.211\n",
      "0.260\n",
      "0.512\n",
      "0.530\n",
      "0.470\n",
      "0.334\n",
      "10.31\n",
      "+ monoT5\n",
      "0.727\n",
      "0.595\n",
      "0.393\n",
      "0.450\n",
      "0.207\n",
      "0.257\n",
      "0.460\n",
      "0.580\n",
      "0.478\n",
      "0.353\n",
      "11.71\n",
      "+ monoBERT\n",
      "0.723\n",
      "0.593\n",
      "0.383\n",
      "0.443\n",
      "0.217\n",
      "0.259\n",
      "0.482\n",
      "0.551\n",
      "0.475\n",
      "0.351\n",
      "11.65\n",
      "+ RankLLaMA\n",
      "0.723\n",
      "0.597\n",
      "0.382\n",
      "0.443\n",
      "0.197\n",
      "0.240\n",
      "0.454\n",
      "0.558\n",
      "0.470\n",
      "0.342\n",
      "13.51\n",
      "+ TILDEv2\n",
      "0.725\n",
      "0.588\n",
      "0.394\n",
      "0.456\n",
      "0.209\n",
      "0.255\n",
      "0.486\n",
      "0.536\n",
      "0.476\n",
      "0.355\n",
      "11.26\n",
      "with classification, Hybrid with HyDE, monoT5, repacking module , Recomp\n",
      "+ sides\n",
      "0.727\n",
      "0.595\n",
      "0.393\n",
      "0.450\n",
      "0.207\n",
      "0.257\n",
      "0.460\n",
      "0.580\n",
      "0.478\n",
      "0.353\n",
      "11.71\n",
      "+ forward\n",
      "0.722\n",
      "0.599\n",
      "0.379\n",
      "0.437\n",
      "0.215\n",
      "0.260\n",
      "0.472\n",
      "0.542\n",
      "0.474\n",
      "0.349\n",
      "11.68\n",
      "+ reverse\n",
      "0.728\n",
      "0.592\n",
      "0.387\n",
      "0.445\n",
      "0.219\n",
      "0.263\n",
      "0.532\n",
      "0.560\n",
      "0.483\n",
      "0.354\n",
      "11.70\n",
      "with classification, Hybrid with HyDE, monoT5, reverse, summarization module\n",
      "w/o summarization\n",
      "0.729\n",
      "0.591\n",
      "0.402\n",
      "0.457\n",
      "0.205\n",
      "0.252\n",
      "0.528\n",
      "0.533\n",
      "0.480\n",
      "0.355\n",
      "10.97\n",
      "+ Recomp\n",
      "0.728\n",
      "0.592\n",
      "0.387\n",
      "0.445\n",
      "0.219\n",
      "0.263\n",
      "0.532\n",
      "0.560\n",
      "0.483\n",
      "0.354\n",
      "11.70\n",
      "+ LongLLMLingua\n",
      "0.713\n",
      "0.581\n",
      "0.362\n",
      "0.423\n",
      "0.199\n",
      "0.245\n",
      "0.530\n",
      "0.539\n",
      "0.466\n",
      "0.334\n",
      "16.17\n",
      "Table 11: Results of the search for optimal RAG practices. Modules enclosed in a boxed module\n",
      "are under investigation to determine the best method. The underlined method represents the selected\n",
      "implementation. The “Avg” (average score) is calculated based on the Acc, EM, and RAG scores for\n",
      "all tasks, while the average latency is measured in seconds per query. The best scores are highlighted\n",
      "in bold.\n",
      "as the generator. We used Milvus to build a vector database that includes 10 million text of English\n",
      "Wikipedia and 4 million text of medical data. We also investigated the impact of removing the Query\n",
      "Classification, Reranking, and Summarization modules to assess their contributions.\n",
      "4.1\n",
      "Comprehensive Evaluation\n",
      "We conducted extensive experiments across various NLP tasks and datasets to assess the perfor-\n",
      "mance of RAG systems. Specifically: (I) Commonsense Reasoning; (II) Fact Checking; (III)\n",
      "Open-Domain QA; (IV) MultiHop QA; (V) Medical QA. For further details on the tasks and\n",
      "their corresponding datasets, please refer to Appendix A.6. Furthermore, we evaluated the RAG\n",
      "capabilities on subsets extracted from these datasets, employing the metrics recommended in RA-\n",
      "GAs [51], including Faithfulness, Context Relevancy, Answer Relevancy, and Answer Correctness.\n",
      "Additionally, we measured Retrieval Similarity by computing the cosine similarity between retrieved\n",
      "documents and gold documents.\n",
      "We used accuracy as the evaluation metric for the tasks of Commonsense Reasoning, Fact Checking,\n",
      "and Medical QA. For Open-Domain QA and Multihop QA, we employed token-level F1 score and\n",
      "Exact Match (EM) score. The final RAG score was calculated by averaging the aforementioned five\n",
      "RAG capabilities. We followed Trivedi et al. [52] and sub-sampled up to 500 examples from each\n",
      "dataset.\n",
      "4.2\n",
      "Results and Analysis\n",
      "Based on the experimental results presented in Table 11, the following key insights emerge:\n",
      "• Query Classification Module: This module is referenced and contributes to both effectiveness\n",
      "and efficiency, leading to an average improvement in the overall score from 0.428 to 0.443 and a\n",
      "reduction in latency time from 16.41 to 11.58 seconds per query.\n",
      "11\n",
      "\n",
      "• Retrieval Module: While the “Hybrid with HyDE” method attained the highest RAG score of\n",
      "0.58, it does so at a considerable computational cost with 11.71 second per query. Consequently,\n",
      "the “Hybrid” or “Original” methods are recommended, as they reduce latency while maintaining\n",
      "comparable performance.\n",
      "• Reranking Module: The absence of a reranking module led to a noticeable drop in performance,\n",
      "highlighting its necessity. MonoT5 achieved the highest average score, affirming its efficacy in\n",
      "augmenting the relevance of retrieved documents. This indicates the critical role of reranking in\n",
      "enhancing the quality of generated responses.\n",
      "• Repacking Module: The Reverse configuration exhibited superior performance, achieving an\n",
      "RAG score of 0.560. This indicates that positioning more relevant context closer to the query leads\n",
      "to optimal outcomes.\n",
      "• Summarization Module: Recomp demonstrated superior performance, although achieving compa-\n",
      "rable results with lower latency was possible by removing the summarization module. Nevertheless,\n",
      "Recomp remains the preferred choice due to its capability to address the generator’s maximum\n",
      "length constraints. In time-sensitive applications, removing summarization could effectively reduce\n",
      "response time.\n",
      "The experimental results demonstrate that each module contributes uniquely to the overall perfor-\n",
      "mance of the RAG system. The query classification module enhances accuracy and reduces latency,\n",
      "while the retrieval and reranking modules significantly improve the system’s ability to handle diverse\n",
      "queries. The repacking and summarization modules further refine the system’s output, ensuring\n",
      "high-quality responses across different tasks.\n",
      "5\n",
      "Discussion\n",
      "5.1\n",
      "Best Practices for Implementing RAG\n",
      "According to our experimental findings, we suggest two distinct recipes or practices for implementing\n",
      "RAG systems, each customized to address specific requirements: one focusing on maximizing\n",
      "performance, and the other on striking a balance between efficiency and efficacy.\n",
      "Best Performance Practice: To achieve the highest performance, it is recommended to incorporate\n",
      "query classification module, use the “Hybrid with HyDE” method for retrieval, employ monoT5 for\n",
      "reranking, opt for Reverse for repacking, and leverage Recomp for summarization. This configuration\n",
      "yielded the highest average score of 0.483, albeit with a computationally-intensive process.\n",
      "Balanced Efficiency Practice: In order to achieve a balance between performance and efficiency,\n",
      "it is recommended to incorporate the query classification module, implement the Hybrid method\n",
      "for retrieval, use TILDEv2 for reranking, opt for Reverse for repacking, and employ Recomp for\n",
      "summarization. Given that the retrieval module accounts for the majority of processing time in the\n",
      "system, transitioning to the Hybrid method while keeping other modules unchanged can substantially\n",
      "reduce latency while preserving a comparable performance.\n",
      "5.2\n",
      "Multimodal Extension\n",
      "We have extended RAG to multimodal applications. Specifically, we have incorporated text2image\n",
      "and image2text retrieval capabilities into the system with a substantial collection of paired image and\n",
      "textual descriptions as a retrieval source. As depicted in Figure 4, the text2image capability speeds\n",
      "up the image generation process when a user query aligns well with the textual descriptions of stored\n",
      "images (i.e., “retrieval as generation” strategy), while the image2text functionality comes into play\n",
      "when a user provides an image and engages in conversation about the input image. These multimodal\n",
      "RAG capabilities offer the following advantages:\n",
      "• Groundedness: Retrieval methods provide information from verified multimodal materials, thereby\n",
      "ensuring authenticity and specificity. In contrast, on-the-fly generation relies on models to generate\n",
      "new content, which can occasionally result in factual errors or inaccuracies.\n",
      "• Efficiency: Retrieval methods are typically more efficient, especially when the answer already\n",
      "exists in stored materials. Conversely, generation methods may require more computational\n",
      "resources to produce new content, particularly for images or lengthy texts.\n",
      "12\n",
      "\n",
      "A dog is sleeping\n",
      "Retrieval\n",
      "Retrieval\n",
      "Retrieval\n",
      "A dog is sleeping.\n",
      "A dog is drinking water\n",
      "A dog is sleeping\n",
      "Retrieval\n",
      "A dog is sleeping\n",
      "Image2text Retrieval\n",
      "Text2image Retrieval\n",
      "High Similarity\n",
      "Low Similarity\n",
      "User Query\n",
      "Image Caption Model\n",
      "Image Generation Model\n",
      "Figure 4: Workflow of multimodal retrieval. The upper section illustrates the text-to-image retrieval\n",
      "process. Initially, a text query is used to find images in the database with the highest similarity. If a\n",
      "high similarity is found, the image is returned directly. If not, an image generation model is employed\n",
      "to create and return an appropriate image. The lower section demonstrates the image-to-text retrieval\n",
      "process. Here, a user-provided image is matched with images in the database to find the highest\n",
      "similarity. If a high similarity is identified, the pre-stored caption of the matching image is returned.\n",
      "Otherwise, an image captioning model generates and returns a new caption.\n",
      "• Maintainability: Generation models often necessitate careful fine-tuning to tailor them for new\n",
      "applications. In contrast, retrieval-based methods can be improved to address new demands by\n",
      "simply enlarging the size and enhancing the quality of retrieval sources.\n",
      "We plan to broaden the application of this strategy to include other modalities, such as video and\n",
      "speech, while also exploring efficient and effective cross-modal retrieval techniques.\n",
      "6\n",
      "Conclusion\n",
      "In this study, we aim to identify optimal practices for implementing retrieval-augmented generation\n",
      "in order to improve the quality and reliability of content produced by large language models. We\n",
      "systematically assessed a range of potential solutions for each module within the RAG framework\n",
      "and recommended the most effective approach for each module. Furthermore, we introduced a\n",
      "comprehensive evaluation benchmark for RAG systems and conducted extensive experiments to\n",
      "determine the best practices among various alternatives. Our findings not only contribute to a deeper\n",
      "understanding of retrieval-augmented generation systems but also establish a foundation for future\n",
      "research.\n",
      "Limitations\n",
      "We have evaluated the impact of various methods for fine-tuning LLM generators. Previous studies\n",
      "have demonstrated the feasibility of training both the retriever and generator jointly. We would\n",
      "like to explore this possibility in the future. In this study, we embraced the principle of modular\n",
      "13\n",
      "\n",
      "design to simplify the search for optimal RAG implementations, thereby reducing complexity. Due\n",
      "to the daunting costs associated with constructing vector databases and conducting experiments, our\n",
      "evaluation was limited to investigating the effectiveness and influence of representative chunking\n",
      "techniques within the chunking module. It would be intriguing to further explore the impact of\n",
      "different chunking techniques on the entire RAG systems. While we have discussed the application of\n",
      "RAG in the domain of NLP and extended its scope to image generation, an enticing avenue for future\n",
      "exploration would involve expanding this research to other modalities such as speech and video.\n",
      "Acknowledgments\n",
      "The authors would like to thank the anonymous reviewers for their valuable comments. This work\n",
      "was supported by National Natural Science Foundation of China (No. 62076068).\n",
      "References\n",
      "[1] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin,\n",
      "Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton,\n",
      "Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano,\n",
      "Jan Leike, and Ryan Lowe. Training language models to follow instructions with human\n",
      "feedback. In Proceedings of the Conference on Neural Information Processing Systems (NeurIPS\n",
      "2022), 2022.\n",
      "[2] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and\n",
      "Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model.\n",
      "arXiv preprint arXiv:2305.18290, 2023.\n",
      "[3] Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J Liu. SLIC-\n",
      "HF: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425,\n",
      "2023.\n",
      "[4] Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. RRHF:\n",
      "Rank responses to align language models with human feedback without tears. arXiv preprint\n",
      "arXiv:2304.05302, 2023.\n",
      "[5] Wenhao Liu, Xiaohua Wang, Muling Wu, Tianlong Li, Changze Lv, Zixuan Ling, Jianhao Zhu,\n",
      "Cenyuan Zhang, Xiaoqing Zheng, and Xuanjing Huang. Aligning large language models with\n",
      "human preferences through representation engineering. arXiv preprint arXiv:2312.15997, 2023.\n",
      "[6] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun,\n",
      "and Haofen Wang. Retrieval-augmented generation for large language models: A survey. arXiv\n",
      "preprint arXiv:2312.10997, 2023.\n",
      "[7] Huayang Li, Yixuan Su, Deng Cai, Yan Wang, and Lemao Liu. A survey on retrieval-augmented\n",
      "text generation. arXiv preprint arXiv:2202.01110, 2022.\n",
      "[8] Deng Cai, Yan Wang, Lemao Liu, and Shuming Shi. Recent advances in retrieval-augmented\n",
      "text generation. In Proceedings of the 45th international ACM SIGIR conference on research\n",
      "and development in information retrieval, pages 3417–3419, 2022.\n",
      "[9] Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. Query rewriting for\n",
      "retrieval-augmented large language models. arXiv preprint arXiv:2305.14283, 2023.\n",
      "[10] Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. Precise zero-shot dense retrieval\n",
      "without relevance labels. arXiv preprint arXiv:2212.10496, 2022.\n",
      "[11] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan\n",
      "Majumder, and Furu Wei. Text embeddings by weakly-supervised contrastive pre-training.\n",
      "arXiv preprint arXiv:2212.03533, 2022.\n",
      "[12] Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. C-pack: Packaged resources\n",
      "to advance general chinese embedding, 2023.\n",
      "14\n",
      "\n",
      "[13] OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. doi: 10.48550/ARXIV.2303.\n",
      "08774. URL https://doi.org/10.48550/arXiv.2303.08774.\n",
      "[14] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\n",
      "thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open\n",
      "and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n",
      "[15] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo\n",
      "Zhao, Yu Zhang, Yulong Chen, et al. Siren’s song in the ai ocean: a survey on hallucination in\n",
      "large language models. arXiv preprint arXiv:2309.01219, 2023.\n",
      "[16] Xiaohua Wang, Yuliang Yan, Longtao Huang, Xiaoqing Zheng, and Xuan-Jing Huang. Halluci-\n",
      "nation detection for generative large language models by bayesian sequential estimation. In\n",
      "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing,\n",
      "pages 15361–15371, 2023.\n",
      "[17] Liang Wang, Nan Yang, and Furu Wei. Query2doc: Query expansion with large language\n",
      "models. arXiv preprint arXiv:2303.07678, 2023.\n",
      "[18] Gangwoo Kim, Sungdong Kim, Byeongguk Jeon, Joonsuk Park, and Jaewoo Kang. Tree of\n",
      "clarifications: Answering ambiguous questions with retrieval-augmented large language models.\n",
      "arXiv preprint arXiv:2310.14696, 2023.\n",
      "[19] Jerry Liu. LlamaIndex, 11 2022. URL https://github.com/jerryjliu/llama_index.\n",
      "[20] Peitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou, and Jian-Yun Nie. Retrieve anything to\n",
      "augment large language models. arXiv preprint arXiv:2310.07554, 2023.\n",
      "[21] Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang.\n",
      "Towards general text embeddings with multi-stage contrastive learning.\n",
      "arXiv preprint\n",
      "arXiv:2308.03281, 2023.\n",
      "[22] Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu.\n",
      "Llmlingua:\n",
      "Compressing prompts for accelerated inference of large language models. arXiv preprint\n",
      "arXiv:2310.05736, 2023.\n",
      "[23] Fangyuan Xu, Weijia Shi, and Eunsol Choi. Recomp: Improving retrieval-augmented lms with\n",
      "compression and selective augmentation. arXiv preprint arXiv:2310.04408, 2023.\n",
      "[24] Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan Parvez, and Graham Neubig. Learning\n",
      "to filter context for retrieval-augmented generation. arXiv preprint arXiv:2311.08377, 2023.\n",
      "[25] Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, and Jimmy Lin. Multi-stage document ranking\n",
      "with bert. arXiv preprint arXiv:1910.14424, 2019.\n",
      "[26] Rodrigo Nogueira, Zhiying Jiang, and Jimmy Lin.\n",
      "Document ranking with a pretrained\n",
      "sequence-to-sequence model. arXiv preprint arXiv:2003.06713, 2020.\n",
      "[27] Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. Fine-tuning llama for\n",
      "multi-stage text retrieval. arXiv preprint arXiv:2310.08319, 2023.\n",
      "[28] Shengyao Zhuang and Guido Zuccon. Tilde: Term independent likelihood model for passage\n",
      "re-ranking. In Proceedings of the 44th International ACM SIGIR Conference on Research and\n",
      "Development in Information Retrieval, pages 1483–1492, 2021.\n",
      "[29] Shengyao Zhuang and Guido Zuccon. Fast passage re-ranking with contextualized exact term\n",
      "matching and efficient passage expansion. arXiv preprint arXiv:2108.08513, 2021.\n",
      "[30] Hongyin Luo, Yung-Sung Chuang, Yuan Gong, Tianhua Zhang, Yoon Kim, Xixin Wu, Danny\n",
      "Fox, Helen M. Meng, and James R. Glass. Sail: Search-augmented instruction learning.\n",
      "In Conference on Empirical Methods in Natural Language Processing, 2023. URL https:\n",
      "//api.semanticscholar.org/CorpusID:258865283.\n",
      "15\n",
      "\n",
      "[31] Tianjun Zhang, Shishir G. Patil, Naman Jain, Sheng Shen, Matei A. Zaharia, Ion Stoica,\n",
      "and Joseph E. Gonzalez. Raft: Adapting language model to domain specific rag. ArXiv,\n",
      "abs/2403.10131, 2024.\n",
      "[32] Zihan Liu, Wei Ping, Rajarshi Roy, Peng Xu, Chankyu Lee, Mohammad Shoeybi, and Bryan\n",
      "Catanzaro. Chatqa: Surpassing gpt-4 on conversational qa and rag. 2024. URL https:\n",
      "//api.semanticscholar.org/CorpusID:267035133.\n",
      "[33] Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick,\n",
      "Jane A. Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with\n",
      "retrieval augmented language models. ArXiv, abs/2208.03299, 2022.\n",
      "[34] Lingxi Zhang, Yue Yu, Kuan Wang, and Chao Zhang. Arl2: Aligning retrievers for black-box\n",
      "large language models via self-guided adaptive relevance labeling. ArXiv, abs/2402.13542,\n",
      "2024.\n",
      "[35] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke\n",
      "Zettlemoyer, and Wen-tau Yih. Replug: Retrieval-augmented black-box language models. arXiv\n",
      "preprint arXiv:2301.12652, 2023.\n",
      "[36] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang.\n",
      "Realm:\n",
      "Retrieval-augmented language model pre-training. ArXiv, abs/2002.08909, 2020.\n",
      "[37] Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro\n",
      "Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, and Scott Yih.\n",
      "Ra-dit: Retrieval-augmented dual instruction tuning. ArXiv, abs/2310.01352, 2023.\n",
      "[38] Hamed Zamani and Michael Bendersky. Stochastic rag: End-to-end retrieval-augmented gen-\n",
      "eration through expected utility maximization. 2024. URL https://api.semanticscholar.\n",
      "org/CorpusID:269605438.\n",
      "[39] Yizheng Huang and Jimmy Huang. A survey on retrieval-augmented text generation for large\n",
      "language models. arXiv preprint arXiv:2404.10981, 2024.\n",
      "[40] Ruochen Zhao, Hailin Chen, Weishi Wang, Fangkai Jiao, Xuan Long Do, Chengwei Qin,\n",
      "Bosheng Ding, Xiaobao Guo, Minzhi Li, Xingxuan Li, et al. Retrieving multimodal information\n",
      "for augmented generation: A survey. arXiv preprint arXiv:2303.10868, 2023.\n",
      "[41] Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling\n",
      "Yang, Wentao Zhang, and Bin Cui. Retrieval-augmented generation for ai-generated content: A\n",
      "survey. arXiv preprint arXiv:2402.19473, 2024.\n",
      "[42] Michael Günther, Jackmin Ong, Isabelle Mohr, Alaeddine Abdessalem, Tanguy Abel, Moham-\n",
      "mad Kalim Akram, Susana Guzman, Georgios Mastrapas, Saba Sturua, Bo Wang, et al. Jina\n",
      "embeddings 2: 8192-token general-purpose text embeddings for long documents. arXiv preprint\n",
      "arXiv:2310.19923, 2023.\n",
      "[43] LlamaIndex. Llamaindex website. https://www.llamaindex.com. Accessed: 2024-06-08.\n",
      "[44] Kunal Sawarkar, Abhilasha Mangal, and Shivam Raj Solanki. Blended rag: Improving rag\n",
      "(retriever-augmented generation) accuracy with semantic search and hybrid query-based retriev-\n",
      "ers. arXiv preprint arXiv:2404.07220, 2024.\n",
      "[45] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand\n",
      "Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning.\n",
      "arXiv preprint arXiv:2112.09118, 2021.\n",
      "[46] Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. Beir:\n",
      "A heterogenous benchmark for zero-shot evaluation of information retrieval models. arXiv\n",
      "preprint arXiv:2104.08663, 2021.\n",
      "[47] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan\n",
      "Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, et al. Ms marco: A human\n",
      "generated machine reading comprehension dataset. arXiv preprint arXiv:1611.09268, 2016.\n",
      "16\n",
      "\n",
      "[48] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni,\n",
      "and Percy Liang. Lost in the middle: How language models use long contexts. Transactions of\n",
      "the Association for Computational Linguistics, 12:157–173, 2024.\n",
      "[49] Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and\n",
      "Lili Qiu. Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt\n",
      "compression. arXiv preprint arXiv:2310.06839, 2023.\n",
      "[50] Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\n",
      "Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas\n",
      "Blecher, Cristian Cantón Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes,\n",
      "Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S.\n",
      "Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian\n",
      "Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut\n",
      "Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov,\n",
      "Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta,\n",
      "Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh\n",
      "Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov,\n",
      "Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert\n",
      "Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat\n",
      "models. ArXiv, abs/2307.09288, 2023.\n",
      "[51] ES Shahul, Jithin James, Luis Espinosa Anke, and Steven Schockaert. Ragas: Automated\n",
      "evaluation of retrieval augmented generation. In Conference of the European Chapter of the\n",
      "Association for Computational Linguistics, 2023. URL https://api.semanticscholar.org/\n",
      "CorpusID:263152733.\n",
      "[52] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique:\n",
      "Multihop questions via single-hop question composition. Transactions of the Association\n",
      "for Computational Linguistics, page 539–554, May 2022. doi: 10.1162/tacl_a_00475. URL\n",
      "http://dx.doi.org/10.1162/tacl_a_00475.\n",
      "[53] Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi,\n",
      "Patrick Wendell, Matei Zaharia, and Reynold Xin. Free dolly: Introducing the world’s first truly\n",
      "open instruction-tuned llm, 2023. URL https://www.databricks.com/blog/2023/04/12/\n",
      "dolly-first-open-commercially-viable-instruction-tuned-llm.\n",
      "[54] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Fernando Campos, and Ellen M. Voorhees.\n",
      "Overview of the trec 2019 deep learning track. ArXiv, abs/2003.07820, 2020. URL https:\n",
      "//api.semanticscholar.org/CorpusID:253234683.\n",
      "[55] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Fernando Campos, and Ellen M. Voorhees.\n",
      "Overview of the trec 2020 deep learning track. ArXiv, abs/2102.07662, 2021. URL https:\n",
      "//api.semanticscholar.org/CorpusID:212737158.\n",
      "[56] Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak Pradeep, and Rodrigo\n",
      "Nogueira. Pyserini: A python toolkit for reproducible information retrieval research with sparse\n",
      "and dense representations. In Proceedings of the 44th International ACM SIGIR Conference on\n",
      "Research and Development in Information Retrieval, pages 2356–2362, 2021.\n",
      "[57] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur P. Parikh,\n",
      "Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova,\n",
      "Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc V. Le,\n",
      "and Slav Petrov. Natural questions: A benchmark for question answering research. Transactions\n",
      "of the Association for Computational Linguistics, 7:453–466, 2019.\n",
      "[58] Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: A large scale\n",
      "distantly supervised challenge dataset for reading comprehension. ArXiv, abs/1705.03551,\n",
      "2017.\n",
      "[59] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhut-\n",
      "dinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop\n",
      "question answering. arXiv preprint arXiv:1809.09600, 2018.\n",
      "17\n",
      "\n",
      "[60] Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. Asqa: Factoid questions meet\n",
      "long-form answers. ArXiv, abs/2204.06092, 2022.\n",
      "[61] Tomáš Koˇ\n",
      "cisk`\n",
      "y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor\n",
      "Melis, and Edward Grefenstette. The narrativeqa reading comprehension challenge. Transac-\n",
      "tions of the Association for Computational Linguistics, 6:317–328, 2018.\n",
      "[62] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions\n",
      "for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.\n",
      "[63] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic\n",
      "human falsehoods. arXiv preprint arXiv:2109.07958, 2021.\n",
      "[64] J. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and\n",
      "Weizhu Chen. Lora: Low-rank adaptation of large language models. ArXiv, abs/2106.09685,\n",
      "2021.\n",
      "[65] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\n",
      "Jacob Steinhardt. Measuring massive multitask language understanding. Cornell University -\n",
      "arXiv,Cornell University - arXiv, Sep 2020.\n",
      "[66] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,\n",
      "and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning chal-\n",
      "lenge. ArXiv, abs/1803.05457, 2018. URL https://api.semanticscholar.org/CorpusID:\n",
      "3922816.\n",
      "[67] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.\n",
      "Can a suit of armor\n",
      "conduct electricity? a new dataset for open book question answering. In Proceedings of\n",
      "the 2018 Conference on Empirical Methods in Natural Language Processing, Jan 2018. doi:\n",
      "10.18653/v1/d18-1260. URL http://dx.doi.org/10.18653/v1/d18-1260.\n",
      "[68] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. Fever: a\n",
      "large-scale dataset for fact extraction and verification. ArXiv, abs/1803.05355, 2018. URL\n",
      "https://api.semanticscholar.org/CorpusID:4711425.\n",
      "[69] Tianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei Fang, Luc Gaitskell, Thomas\n",
      "Hartvigsen, Xixin Wu, Danny Fox, Helen M. Meng, and James R. Glass. Interpretable unified\n",
      "language checking. ArXiv, abs/2304.03728, 2023. URL https://api.semanticscholar.\n",
      "org/CorpusID:258041307.\n",
      "[70] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase\n",
      "from question-answer pairs. Empirical Methods in Natural Language Processing,Empirical\n",
      "Methods in Natural Language Processing, Oct 2013.\n",
      "[71] Xanh Ho, A. Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing a multi-hop qa\n",
      "dataset for comprehensive evaluation of reasoning steps. ArXiv, abs/2011.01060, 2020. URL\n",
      "https://api.semanticscholar.org/CorpusID:226236740.\n",
      "[72] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, NoahA. Smith, and Mike Lewis.\n",
      "Measuring and narrowing the compositionality gap in language models. Oct 2022.\n",
      "[73] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W. Cohen, and Xinghua Lu. Pubmedqa: A\n",
      "dataset for biomedical research question answering. In Conference on Empirical Methods in\n",
      "Natural Language Processing, 2019. URL https://api.semanticscholar.org/CorpusID:\n",
      "202572622.\n",
      "[74] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning\n",
      "to retrieve, generate, and critique through self-reflection. arXiv preprint arXiv:2310.11511,\n",
      "2023.\n",
      "18\n",
      "\n",
      "A\n",
      "Experimental Details\n",
      "In this section, we provide detailed experimental settings for each module, covering dataset specifics,\n",
      "training parameters, and any additional experimental results.\n",
      "A.1\n",
      "Query Classification\n",
      "Datasets\n",
      "We utilized a subset of the Databricks-Dolly-15K [53] and generated additional data\n",
      "using GPT-4.The prompt template for generating questions is shown in Table 14.\n",
      "Implementation Details\n",
      "We choose BERT-base-multilingual-cased as our classifier, with a batch\n",
      "size of 16 and a learning rate of 1e-5. The evaluation of results is showcased in Table 1.\n",
      "A.2\n",
      "Experimental Details of Retrieval Methods\n",
      "Implementation details of the comparative experiments of different retrieval methods are as below:\n",
      "Datasets\n",
      "We use the TREC DL 2019 [54] and 2020 [55] passage ranking datasets to evaluate the\n",
      "performance of different retrieval methods.\n",
      "Metrics\n",
      "Widely-used evaluation metrics for retrieval include mAP, nDCG@10, R@50 and R@1k.\n",
      "Both mAP and nDCG@10 are order-aware metrics that take the ranking of search results into account.\n",
      "In contrast, R@k is an order-unaware metric. We also report the average latency incurred by each\n",
      "method per query.\n",
      "Implementation Details\n",
      "For sparse retrieval, we use the BM25 algorithm, which relies on the TF-\n",
      "IDF algorithm. For dense retrieval, we employ Contriever as our unsupervised contrastive text encoder.\n",
      "Based on our evaluation of embedding models, we implement our supervised dense retrieval using\n",
      "LLM-Embedder. We use the default implementation of BM25 and Contriever from Pyserini [56].\n",
      "The BM25 index is constructed using Lucene on MS MARCO collections, while the dense vector\n",
      "index is generated with Faiss employing Flat configuration on the same dataset. For query rewriting,\n",
      "we prompt Zephyr-7b-alpha9, a model trained to act as a helpful assistant, to rewrite the original\n",
      "query. For query decomposition, we employ GPT-3.5-turbo-0125 to break down the original query\n",
      "into multiple sub-queries. We closely follow the implementation from HyDE [10], utilizing the more\n",
      "advanced instruction-following language model, GPT-3.5-turbo-instruct, to generate hypothetical\n",
      "answers. The model infers with a default temperature of 0.7, sampling up to a maximum of 512\n",
      "tokens. Retrieval experiments and evaluation are conducted using the Pyserini toolkit.\n",
      "A.3\n",
      "Experimental Details of Reranking Methods\n",
      "Datasets\n",
      "Our experiments utilize the MS MARCO Passage ranking dataset, a substantial corpus\n",
      "designed for machine reading comprehension tasks. This dataset comprises over 8.8 million passages\n",
      "and 1 million queries. The training set contains approximately 398M tuples of queries paired with\n",
      "corresponding positive and negative passages, while the development set comprises 6,980 queries,\n",
      "paired with their BM25 retrieval results, and preserves the top-1000 ranked candidate passages for\n",
      "each query. We evaluate the effectiveness of the methods on the development set, as the test set is not\n",
      "publicly available.\n",
      "Metrics\n",
      "The evaluation metrics MRR@1, MRR@10, MRR@1k and Hit Rate@10 are used.\n",
      "MRR@10 is the official metric proposed by MS MARCO.\n",
      "Implementation Details\n",
      "We follow and make modifications to the implementation provided by\n",
      "PyGaggle [26] and TILDE [28]. For DLM-based reranking, we use monoT5 [26] based on T5-base,\n",
      "monoBERT [25] based on BERT-large and RankLLaMA [27] based on Llama-2-7b. For TILDE\n",
      "reranking, we use TILDEv2 [29] based on BERT-base.\n",
      "Typically, 50 documents are retrieved as input for the reranking module. The documents remaining\n",
      "after the reranking and repacking phase can be further concentrated by assigning a top-k value or a\n",
      "relevancy score threshold.\n",
      "Result Analysis\n",
      "Reranking results are shown in Table 9. We compare our results with a randomly\n",
      "shuffled ordering and the BM25 retrieval baseline. All reranking methods demonstrate a notable\n",
      "9https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha\n",
      "19\n",
      "\n",
      "Context\n",
      "Model\n",
      "NQ\n",
      "TriviaQA\n",
      "HotpotQA\n",
      "ASQA\n",
      "Avg.\n",
      "D∅\n",
      "Mb\n",
      "29.78\n",
      "60.44\n",
      "23.73\n",
      "37.89\n",
      "37.96\n",
      "Mg\n",
      "26.23\n",
      "58.26\n",
      "26.67\n",
      "32.30\n",
      "35.87\n",
      "Mr\n",
      "31.10\n",
      "61.37\n",
      "28.40\n",
      "39.96\n",
      "40.21\n",
      "Mgr\n",
      "25.92\n",
      "57.62\n",
      "26.43\n",
      "32.99\n",
      "35.70\n",
      "Mgg\n",
      "26.69\n",
      "58.07\n",
      "27.04\n",
      "33.75\n",
      "36.39\n",
      "Dg\n",
      "Mb\n",
      "44.78\n",
      "79.90\n",
      "56.72\n",
      "71.64\n",
      "63.26\n",
      "Mg\n",
      "85.72\n",
      "88.16\n",
      "79.82\n",
      "85.51\n",
      "84.80\n",
      "Mr\n",
      "60.98\n",
      "80.20\n",
      "65.73\n",
      "67.49\n",
      "68.60\n",
      "Mgr\n",
      "87.60\n",
      "87.94\n",
      "81.07\n",
      "87.58\n",
      "86.05\n",
      "Mgg\n",
      "86.72\n",
      "88.35\n",
      "79.59\n",
      "83.44\n",
      "84.53\n",
      "Dr\n",
      "Mb\n",
      "16.49\n",
      "50.03\n",
      "21.57\n",
      "28.79\n",
      "29.22\n",
      "Mg\n",
      "22.15\n",
      "46.98\n",
      "24.36\n",
      "29.40\n",
      "30.72\n",
      "Mr\n",
      "36.92\n",
      "58.42\n",
      "29.64\n",
      "39.54\n",
      "41.13\n",
      "Mgr\n",
      "23.63\n",
      "45.01\n",
      "24.17\n",
      "27.95\n",
      "30.19\n",
      "Mgg\n",
      "21.08\n",
      "43.83\n",
      "23.23\n",
      "27.33\n",
      "28.87\n",
      "Dgr\n",
      "Mb\n",
      "34.65\n",
      "81.27\n",
      "52.75\n",
      "65.42\n",
      "58.52\n",
      "Mg\n",
      "85.00\n",
      "87.33\n",
      "78.18\n",
      "83.02\n",
      "83.38\n",
      "Mr\n",
      "60.28\n",
      "79.32\n",
      "63.82\n",
      "67.29\n",
      "67.68\n",
      "Mgr\n",
      "87.63\n",
      "87.14\n",
      "79.95\n",
      "87.78\n",
      "85.63\n",
      "Mgg\n",
      "86.31\n",
      "86.90\n",
      "78.10\n",
      "83.85\n",
      "83.79\n",
      "Table 12: Results of the model augmented with different contexts on various QA datasets.\n",
      "increase in performance across all metrics. Approximately equal performance is achieved by monoT5\n",
      "and monoBERT, and RankLLaMA performs best, each ascending in latency. TILDEv2 is the fastest,\n",
      "taking approximately 10 to 20 milliseconds per query at the cost of performance. Additionally,\n",
      "TILDEv2 requires that the passages reranked be identically included in the previously indexed\n",
      "collection. Preprocessing must be redone at inference for new unseen passages, negating the efficiency\n",
      "advantages.\n",
      "A.4\n",
      "Experimental Details of Summarization Methods\n",
      "Selective Context\n",
      "Selective Context enhances LLM efficiency by identifying and removing\n",
      "redundant information in the input context. It evaluates the informativeness of lexical units using\n",
      "self-information computed by a base causal language model. This method is non-query-based,\n",
      "allowing a comparison between query-based and non-query-based approaches.\n",
      "Datasets\n",
      "We evaluated these methods on three datasets: Natural Questions (NQ) [57], Trivi-\n",
      "aQA [58], and HotpotQA [59].\n",
      "Metrics\n",
      "Evaluation metrics include the F1 score and the number of tokens changed after summa-\n",
      "rization to measure conciseness.\n",
      "Implementation Details\n",
      "For all methods, we use Llama3-8B-Instruct as the generator model\n",
      "and set a summarization ratio of 0.4. For extractive methods, importance scores determine the\n",
      "sentences retained. For abstractive methods, we control the maximum generation length using the\n",
      "summarization ratio to align with extractive methods. Experiments are conducted on the NQ test set,\n",
      "TriviaQA test set, and HotpotQA development set.\n",
      "A.5\n",
      "Experimental Details of Generator Fine-tuning\n",
      "Datasets\n",
      "We fine-tune our model on several question answering(QA) and reading comprehension\n",
      "datasets, including ASQA [60], HotpotQA [59], NarrativeQA [61], NQ [57], SQuAD [62], Trivi-\n",
      "aQA [58], TruthfulQA [63]. We use their train splits (for those containing significantly more data\n",
      "20\n",
      "\n",
      "[Instruction]\n",
      "Please generate ten descriptions for the continuation task.\n",
      "[Context]\n",
      "For example:\n",
      "1.“French.Washington played a crucial role in the American Revolutionary War, leading\n",
      "the Continental Army against the British.” Please continue writing the above paragraph.\n",
      "2.“The discovery of the double helix structure of DNA by James Watson and Francis\n",
      "Crick revolutionized the field of genetics, laying the foundation for modern molecular\n",
      "biology and biotechnology.” Please continue by discussing recent developments in\n",
      "genetic research, such as CRISPR gene editing, and their potential ethical implications.\n",
      "Table 14: Template for generating task classification data.\n",
      "entries than others, we conducted a random sample). For evaluation, ASQA [60], HotpotQA [59],\n",
      "NQ [57], TriviaQA [58] are used. We evaluate our model on their validation splits or manually split a\n",
      "Dataset\n",
      "#Train\n",
      "#Eval\n",
      "ASQA\n",
      "2, 090\n",
      "483\n",
      "HotpotQA\n",
      "15, 000\n",
      "7, 405\n",
      "TriviaQA\n",
      "9, 000\n",
      "6, 368\n",
      "NQ\n",
      "15, 000\n",
      "8, 006\n",
      "NarrativeQA\n",
      "7, 000\n",
      "−−\n",
      "SQuAD\n",
      "67, 00\n",
      "−−\n",
      "TruthfulQA\n",
      "817\n",
      "−−\n",
      "Table 13: Number of examples in each Dataset\n",
      "used in the fine-tuning experiments.\n",
      "subset from the training set to avoid overlapping.\n",
      "The exact number of entries in each train and\n",
      "test set is detailed in Table 13.\n",
      "We use the dataset-provided documents as dgold\n",
      "for each data entry. To obtain drandom we sam-\n",
      "ple the context of different entries within the\n",
      "same dataset, to make sure the distributions of\n",
      "drandom and dgold are roughly similar.\n",
      "Metrics\n",
      "We use the ground-truth coverage\n",
      "as our evaluation metric, considering that the\n",
      "answers of QA tasks are relatively short, while\n",
      "the generation length of the model is sometimes hard to limit.\n",
      "Implementation Details\n",
      "We select Llama-2-7b [50] as the base model. For efficiency, we use\n",
      "LoRA [64] and int8 quantization during training. The prompt templates used for fine-tuning and\n",
      "evaluation mainly follow Lin et al. [37]. We train our generator for 3 epochs and constrain the\n",
      "maximum length of the sequence to 1600, using a batch size of 4 and a learning rate of 5e-5. During\n",
      "testing, we use a zero-shot setting.\n",
      "Detailed Results\n",
      "Table 12 shows our evaluation results on each dataset.\n",
      "A.6\n",
      "Experimental Details of Comprehensive Evaluation\n",
      "Tasks and Datasets\n",
      "We conducted extensive experiments across various NLP tasks and datasets to\n",
      "assess the performance of RAG systems. Specifically: (1) Commonsense Reasoning: We evaluated\n",
      "on MMLU [65], ARC-Challenge [66], and OpenbookQA [67] datasets. (2) Fact Checking: Our\n",
      "evaluation encompassed the FEVER [68] and PubHealth [69] datasets. (3) Open-Domain QA:\n",
      "We assessed on NQ [57], TriviaQA [58], and WebQuestions [70] datasets. (4) MultiHop QA:\n",
      "Our evaluation included the HotPotQA [59], 2WikiMultiHopQA [71], and MuSiQue [52] datasets.\n",
      "For MuSiQue, we followed the approach outlined in [72] and focused solely on answerable 2-hop\n",
      "questions. (5) Medical QA: We also assessed on the PubMedQA [73] dataset. In each dataset, we\n",
      "randomly sub-sample 500 entries from the test set for our experiments. For datasets without test set,\n",
      "we use develop set instead.\n",
      "To assess RAG capabilities, we evenly collect a total of 500 entries from NQ, TriviaQA, HotPotQA,\n",
      "2WikiMultiHopQA and MuSiQue. Each entry is a “question, gold document, gold answer” triple.\n",
      "Metrics\n",
      "We use token-level F1 score and EM score for Open-Domain QA and MultiHop QA tasks,\n",
      "and accuracy for others. We use a more lenient EM score, which evaluates performance based on\n",
      "whether the model generations include gold answers instead of strictly exact matching [74].\n",
      "Towards RAG capabilities evaluation, we adopt four metrics from RAGAs, including Faithfulness,\n",
      "Context Relevancy, Answer Relevancy, and Answer Correctness. Faithfulness measures how\n",
      "factually consistent the generated answer is with the retrieved context. An answer is considered\n",
      "faithful if all claims made can be directly inferred from the provided context. Context Relevancy\n",
      "evaluates how relevant the retrieved context is to the original query. Answer Relevancy assesses the\n",
      "21\n",
      "\n",
      "pertinence of the generated answer to the original query. Answer Correctness involves the accuracy\n",
      "of the generated answer when compared to the ground truth. For example, Context Relevancy is\n",
      "calculated from the proportion of sentences within the retrieved context that are relevant for answering\n",
      "the given question to all sentences:\n",
      "context relevancy =\n",
      "|S|\n",
      "|Total|\n",
      "(2)\n",
      "where |S| denotes the number of relevant sentences, |Total| denotes the total number of sentences\n",
      "retrieved. All these metrics are evaluated using the RAGAs framework, with GPT-4 serving as the\n",
      "judge.\n",
      "Additionally, we compute the cosine similarity between the retrieved document and the gold document\n",
      "as Retrieval Similarity. The retrieved document and gold document are fed into an embedding\n",
      "model, then the resulting embeddings are used to compute the cosine similarity.\n",
      "Implementation Details\n",
      "For Open-Domain QA and MultiHop QA datasets, we set the generation\n",
      "model’s maximum new token number to 100 tokens. For other datasets, we set it to 50 tokens. To\n",
      "deal with excessively long retrieved documents, we truncated the documents to 2048 words when\n",
      "evaluating RankLLaMA and LongLLMLingua.\n",
      "For all datasets, we use greedy decoding during generation. To better compare the capabilities of\n",
      "different RAG modules, we adopt the 0-shot evaluation setting, i.e., no in-context examples are\n",
      "offered. In the multiple choice and fact checking tasks, answers generated by the model may take\n",
      "a variety of forms (e.g., “the answer is A” instead of “A”). Therefore, we preprocess the responses\n",
      "generated by the model, applying regular expression templates to match them with gold labels.\n",
      "22\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in documents:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "searching for best practices in retrieval-augmented\n",
      "generation\n",
      "xiaohua wang, zhenghua wang, xuan gao, feiran zhang,\n",
      "yixin wu, zhibo xu, tianyuan shi, zhengyuan wang, shizheng li,\n",
      "qi qian, ruicheng yin, changze lv, xiaoqing zheng∗\n",
      ", xuanjing huang\n",
      "school of computer science, fudan university, shanghai, china\n",
      "shanghai key laboratory of intelligent information processing\n",
      "{xiaohuawang22,zhenghuawang23}@m.fudan.edu.cn\n",
      "{zhengxq,xjhuang}@fudan.edu.cn\n",
      "abstract\n",
      "retrieval-augmented generation (rag) techniques have proven to be effective\n",
      "in integrating up-to-date information, mitigating hallucinations, and enhancing\n",
      "response quality, particularly in specialized domains. while many rag approaches\n",
      "have been proposed to enhance large language models through query-dependent\n",
      "retrievals, these approaches still suffer from their complex implementation and\n",
      "prolonged response times. typically, a rag workflow involves multiple processing\n",
      "steps, each of which can be executed in various ways. here, we investigate\n",
      "existing rag approaches and their potential combinations to identify optimal\n",
      "rag practices. through extensive experiments, we suggest several strategies\n",
      "for deploying rag that balance both performance and efficiency. moreover,\n",
      "we demonstrate that multimodal retrieval techniques can significantly enhance\n",
      "question-answering capabilities about visual inputs and accelerate the generation\n",
      "of multimodal content using a “retrieval as generation” strategy. resources are\n",
      "available at https://github.com/fudandnn-nlp/rag.\n",
      "1\n",
      "introduction\n",
      "generative large language models are prone to producing outdated information or fabricating facts,\n",
      "although they were aligned with human preferences by reinforcement learning [1] or lightweight\n",
      "alternatives [2–5]. retrieval-augmented generation (rag) techniques address these issues by com-\n",
      "bining the strengths of pretraining and retrieval-based models, thereby providing a robust framework\n",
      "for enhancing model performance [6]. furthermore, rag enables rapid deployment of applications\n",
      "for specific organizations and domains without necessitating updates to the model parameters, as\n",
      "long as query-related documents are provided.\n",
      "many rag approaches have been proposed to enhance large language models (llms) through\n",
      "query-dependent retrievals [6–8]. a typical rag workflow usually contains multiple intervening\n",
      "processing steps: query classification (determining whether retrieval is necessary for a given input\n",
      "query), retrieval (efficiently obtaining relevant documents for the query), reranking (refining the\n",
      "order of retrieved documents based on their relevance to the query), repacking (organizing the\n",
      "retrieved documents into a structured one for better generation), summarization (extracting key\n",
      "information for response generation from the repacked document and eliminating redundancies)\n",
      "modules. implementing rag also requires decisions on the ways to properly split documents into\n",
      "chunks, the types of embeddings to use for semantically representing these chunks, the choice of\n",
      "∗corresponding author.\n",
      "preprint. under review.\n",
      "arxiv:2407.01219v1  [cs.cl]  1 jul 2024\n",
      "\n",
      "large language model\n",
      "query classification\n",
      "reranking\n",
      "dlm-based\n",
      "monot5\n",
      "monobert\n",
      "rankllama\n",
      "tilde\n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "retrieval\n",
      "original query\n",
      "bm25\n",
      "contriever\n",
      "llm-embedder\n",
      "query rewriting\n",
      "query decomposition\n",
      "hyde\n",
      "hybrid search\n",
      "hyde+hybrid search\n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      "   \n",
      "summarization\n",
      "extractive\n",
      "recomp\n",
      "bm25\n",
      "contriever\n",
      "abstractive\n",
      "longllmlingua\n",
      "selectivecontext\n",
      "recomp\n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "vector database\n",
      "milvus\n",
      "faiss\n",
      "weaviate\n",
      "qdrant\n",
      "chroma\n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "chunking\n",
      "chunking size\n",
      "small2big\n",
      "sliding windows\n",
      "chunk metadata\n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      " \n",
      "embedding\n",
      "llm-embedder\n",
      "intfloat/e5\n",
      "baai/bge\n",
      "jina-embeddings-v2\n",
      "gte\n",
      "all-mpnet-base-v2\n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      "     \n",
      "repacking\n",
      "sides\n",
      "forward\n",
      "reverse\n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      "  \n",
      "evaluation\n",
      "general performance\n",
      "specific domains\n",
      "retrieval capability\n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      "  \n",
      "fine-tune\n",
      "disturb\n",
      "random\n",
      "normal\n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      "    \n",
      "retrieval source\n",
      "figure 1: retrieval-augmented generation workflow. this study investigates the contribution of\n",
      "each component and provides insights into optimal rag practices through extensive experimentation.\n",
      "the optional methods considered for each component are indicated in bold fonts, while the methods\n",
      "underlined indicate the default choice for individual modules. the methods indicated in blue font\n",
      "denote the best-performing selections identified empirically.\n",
      "vector databases to efficiently store feature representations, and the methods for effectively fine-tuning\n",
      "llms (see figure 1).\n",
      "what adds complexity and challenge is the variability in implementing each processing step. for\n",
      "example, in retrieving relevant documents for an input query, various methods can be employed.\n",
      "one approach involves rewriting the query first and using the rewritten queries for retrieval [9].\n",
      "alternatively, pseudo-responses to the query can be generated first, and the similarity between\n",
      "these pseudo-responses and the backend documents can be compared for retrieval [10]. another\n",
      "option is to directly employ embedding models, typically trained in a contrastive manner using\n",
      "positive and negative query-response pairs [11, 12]. the techniques chosen for each step and their\n",
      "combinations significantly impact both the effectiveness and efficiency of rag systems. to the best\n",
      "of our knowledge, there has been no systematic effort to pursue the optimal implementation of rag,\n",
      "particularly for the entire rag workflow.\n",
      "in this study, we aim to identify the best practices for rag through extensive experimentation. given\n",
      "the infeasibility of testing all possible combinations of these methods, we adopt a three-step approach\n",
      "to identify optimal rag practices. first, we compare representative methods for each rag step (or\n",
      "module) and select up to three of the best-performing methods. next, we evaluate the impact of each\n",
      "method on the overall rag performance by testing one method at a time for an individual step, while\n",
      "keeping the other rag modules unchanged. this allows us to determine the most effective method\n",
      "for each step based on its contribution and interaction with other modules during response generation.\n",
      "once the best method is chosen for a module, it is used in subsequent experiments. finally, we\n",
      "empirically explore a few promising combinations suitable for different application scenarios where\n",
      "efficiency might be prioritized over performance, or vice versa. based on these findings, we suggest\n",
      "several strategies for deploying rag that balance both performance and efficiency.\n",
      "the contributions of this study are three-fold:\n",
      "• through extensive experimentation, we thoroughly investigated existing rag approaches and their\n",
      "combinations to identify and recommend optimal rag practices.\n",
      "2\n",
      "\n",
      "• we introduce a comprehensive framework of evaluation metrics and corresponding datasets to\n",
      "comprehensively assess the performance of retrieval-augmented generation models, covering\n",
      "general, specialized (or domain-specific), and rag-related capabilities.\n",
      "• we demonstrate that the integration of multimodal retrieval techniques can substantially improve\n",
      "question-answering capabilities on visual inputs and speed up the generation of multimodal content\n",
      "through a strategy of “retrieval as generation”.\n",
      "2\n",
      "related work\n",
      "ensuring the accuracy of responses generated by large language models (llms) such as chat-\n",
      "gpt [13] and llama [14] is essential. however, simply enlarging model size does not fundamentally\n",
      "address the issue of hallucinations [15, 16], especially in knowledge-intensive tasks and specialized\n",
      "domains. retrieval-augmented generation (rag) addresses these challenges by retrieving relevant\n",
      "documents from external knowledge bases, providing accurate, real-time, domain-specific context to\n",
      "llms [6]. previous works have optimized the rag pipeline through query and retrieval transfor-\n",
      "mations, enhancing retriever performance, and fine-tuning both the retriever and generator. these\n",
      "optimizations improve the interaction between input queries, retrieval mechanisms, and generation\n",
      "processes, ensuring the accuracy and relevance of responses.\n",
      "2.1\n",
      "query and retrieval transformation\n",
      "effective retrieval requires queries accurate, clear, and detailed. even when converted into embed-\n",
      "dings, semantic differences between queries and relevant documents can persist. previous works have\n",
      "explored methods to enhance query information through query transformation, thereby improving\n",
      "retrieval performance. for instance, query2doc [17] and hyde [10] generate pseudo-documents\n",
      "from original queries to enhance retrieval, while toc [18] decomposes queries into subqueries,\n",
      "aggregating the retrieved content for final results.\n",
      "other studies have focused on transforming retrieval source documents. llamaindex [19] provides an\n",
      "interface to generate pseudo-queries for retrieval documents, improving matching with real queries.\n",
      "some works employ contrastive learning to bring query and document embeddings closer in semantic\n",
      "space [12, 20, 21]. post-processing retrieved documents is another method to enhance generator\n",
      "output, with techniques like hierarchical prompt summarization [22] and using abstractive and\n",
      "extractive compressors [23] to reduce context length and remove redundancy [24].\n",
      "2.2\n",
      "retriever enhancement strategy\n",
      "document chunking and embedding methods significantly impact retrieval performance. common\n",
      "chunking strategies divide documents into chunks, but determining optimal chunk length can be\n",
      "challenging. small chunks may fragment sentences, while large chunks might include irrelevant\n",
      "context. llamaindex [19] optimizes the chunking method like small2big and sliding window.\n",
      "retrieved chunks can be irrelevant and numbers can be large, so reranking is necessary to filter\n",
      "irrelevant documents. a common reranking approach employs deep language models such as\n",
      "bert [25], t5 [26], or llama [27], which requires slow inference steps during reranking but grants\n",
      "better performance. tilde [28, 29] achieves efficiency by precomputing and storing the likelihood\n",
      "of query terms, ranking documents based on their sum.\n",
      "2.3\n",
      "retriever and generator fine-tuning\n",
      "fine-tuning within the rag framework is crucial for optimizing both retrievers and generators. some\n",
      "research focuses on fine-tuning the generator to better utilize retriever context [30–32], ensuring\n",
      "faithful and robust generated content. others fine-tune the retriever to learn to retrieve beneficial\n",
      "passages for the generator [33–35]. holistic approaches treat rag as an integrated system, fine-tuning\n",
      "both retriever and generator together to enhance overall performance [36–38], despite increased\n",
      "complexity and integration challenges.\n",
      "several surveys have extensively discussed current rag systems, covering aspects like text genera-\n",
      "tion [7, 8], integration with llms [6, 39], multimodal [40], and ai-generated content [41]. while\n",
      "these surveys provide comprehensive overviews of existing rag methodologies, selecting the appro-\n",
      "3\n",
      "\n",
      "which city will the next world cup be held?                                         \n",
      " \n",
      " \n",
      "           < search >\n",
      "\"french.washington played a \n",
      "crucial role in the american \n",
      "revolutionary war, leading the \n",
      "continental army against the \n",
      "british. \"\n",
      "please continue writing the \n",
      "above paragraph.      \n",
      "              < continuation writing >\n",
      "background knowledge\n",
      "\"to be, or not to be, that is the \n",
      "question.\" \n",
      "please translate this sentence into \n",
      "french. \n",
      "      \n",
      "< translation >\n",
      "insufficient information\n",
      "sufficient information\n",
      "please give me a plan for holding a graduation party.              \n",
      " \n",
      " \n",
      "       < planning >\n",
      "if you're currently a computer science student and your \n",
      "computer system encounters a malfunction, what should \n",
      "you do?               \n",
      " \n",
      "      < role-play >\n",
      "write an article about the geography of europe, focusing \n",
      "on the changes in rainfall in the western part of the \n",
      "country.                                    \n",
      "          < writing >\n",
      "no retrieval needed\n",
      "need to retrieval\n",
      "please find a novel that is as \n",
      "famous as \"one hundred years \n",
      "of solitude\".               < search >\n",
      "\"dave is attending his aunt's \n",
      "brother funeral today.\"\n",
      "paraphrase the given information \n",
      "effectively. \n",
      "              < rewriting >\n",
      "\"the renaissance was a \n",
      "cultural transformation in \n",
      "european history, marking the \n",
      "revival of arts, sciences, and \n",
      "humanistic thought. the \n",
      "fervor of artists and scholars \n",
      "propelled prosperity and \n",
      "innovation in arts, literature, \n",
      "and science.\" give me a \n",
      "summary.\n",
      "                    < summarization >\n",
      "identify who is football players: \n",
      "messi, jordan, kobe. \n",
      "          \n",
      "          < closed qa >\n",
      "tom has three sisters, and each \n",
      "sister has a brother. how many \n",
      "siblings are there in total?  \n",
      "          \n",
      "< reasonning >\n",
      "q: 3,1 a: 3   q: 2,5 a: 5   \n",
      "q: 5,7 a: ?\n",
      "   < in-context learning >                              \n",
      "\"chatgpt is a product of \n",
      "openai.\" \n",
      "please provide the ownership \n",
      "relationship. \n",
      "       < information extraction >\n",
      "no background knowledge\n",
      "if i want to travel from los angeles to new york and i \n",
      "want to choose the cheapest mode of transportation, \n",
      "should i drive or take a plane?           < decision making >\n",
      "i had a quarrel with my parents because they oppose my \n",
      "relationship with my boyfriend, but we genuinely love \n",
      "each other. how should i persuade my parents to accept \n",
      "our relationship?            \n",
      " \n",
      "   < suggestion >\n",
      "figure 2: classification of retrieval requirements for different tasks. in cases where information is\n",
      "not provided, we differentiate tasks based on the functions of the model.\n",
      "priate algorithm for practical implementation remains challenging. in this paper, we focus on best\n",
      "practices for applying rag methods, advancing the understanding and application of rag in llms.\n",
      "3\n",
      "rag workflow\n",
      "in this section, we detail the components of the rag workflow. for each module, we review\n",
      "commonly used approaches and select the default and alternative methods for our final pipeline.\n",
      "section 4 will discuss best practices. figure 1 presents the workflow and methods for each module.\n",
      "detailed experimental setups, including datasets, hyperparameters, and results are provided in\n",
      "appendix a.\n",
      "3.1\n",
      "query classification\n",
      "not all queries require retrieval-augmented due to the inherent capabilities of llms. while rag can\n",
      "enhance information accuracy and reduce hallucinations, frequent retrieval can increase response\n",
      "time. therefore, we begin by classifying queries to determine the necessity of retrieval. queries\n",
      "requiring retrieval proceed through the rag modules; others are handled directly by llms.\n",
      "retrieval is generally recommended when knowledge beyond the model’s parameters is needed.\n",
      "however, the necessity of retrieval varies by task. for instance, an llm trained up to 2023 can\n",
      "handle a translation request for “sora was developed by openai” without retrieval. conversely, an\n",
      "introduction request for the same topic would require retrieval to provide relevant information.\n",
      "therefore, we propose classifying tasks by type to determine if a query needs retrieval. we categorize\n",
      "model\n",
      "metrics\n",
      "acc prec rec\n",
      "f1\n",
      "bert-base-multilingual 0.95 0.96 0.94 0.95\n",
      "table 1: results of the query classifier.\n",
      "15 tasks based on whether they provide suffi-\n",
      "cient information, with specific tasks and exam-\n",
      "ples illustrated in figure 2. for tasks entirely\n",
      "based on user-given information, we denote as\n",
      "“sufficient”, which need not retrieval; otherwise,\n",
      "we denote as “insufficient”, and retrieval may\n",
      "be necessary. we train a classifier to automate\n",
      "this decision-making process. experimental de-\n",
      "tails are presented in appendix a.1. section 4\n",
      "explores the impact of query classification on the workflow, comparing scenarios with and without\n",
      "classification.\n",
      "4\n",
      "\n",
      "embedding model\n",
      "namespace-pt/msmarco\n",
      "mrr@1 mrr@10 mrr@100\n",
      "r@1\n",
      "r@10 r@100\n",
      "baai/llm-embedder [20]\n",
      "24.79\n",
      "37.58\n",
      "38.62\n",
      "24.07\n",
      "66.45\n",
      "90.75\n",
      "baai/bge-base-en-v1.5 [12]\n",
      "23.34\n",
      "35.80\n",
      "36.94\n",
      "22.63\n",
      "64.12\n",
      "90.13\n",
      "baai/bge-small-en-v1.5 [12]\n",
      "23.27\n",
      "35.78\n",
      "36.89\n",
      "22.65\n",
      "63.92\n",
      "89.80\n",
      "baai/bge-large-en-v1.5 [12]\n",
      "24.63\n",
      "37.48\n",
      "38.59\n",
      "23.91\n",
      "65.57\n",
      "90.60\n",
      "baai/bge-large-en [12]\n",
      "24.84\n",
      "37.66\n",
      "38.73\n",
      "24.13\n",
      "66.09\n",
      "90.64\n",
      "baai/bge-small-en [12]\n",
      "23.28\n",
      "35.79\n",
      "36.91\n",
      "22.62\n",
      "63.96\n",
      "89.67\n",
      "baai/bge-base-en [12]\n",
      "23.47\n",
      "35.94\n",
      "37.07\n",
      "22.73\n",
      "64.17\n",
      "90.14\n",
      "alibaba-nlp/gte-large-en-v1.5 [21]\n",
      "8.93\n",
      "15.60\n",
      "16.71\n",
      "8.67\n",
      "32.28\n",
      "60.36\n",
      "thenlper/gte-base [21]\n",
      "7.42\n",
      "13.23\n",
      "14.30\n",
      "7.21\n",
      "28.27\n",
      "56.20\n",
      "thenlper/gte-small [21]\n",
      "7.97\n",
      "14.81\n",
      "15.95\n",
      "7.71\n",
      "32.07\n",
      "61.08\n",
      "jinaai/jina-embeddings-v2-small-en [42]\n",
      "8.07\n",
      "15.02\n",
      "16.12\n",
      "7.87\n",
      "32.55\n",
      "60.36\n",
      "intfloat/e5-small-v2 [11]\n",
      "10.04\n",
      "18.23\n",
      "19.41\n",
      "9.74\n",
      "38.92\n",
      "68.42\n",
      "intfloat/e5-large-v2 [11]\n",
      "9.58\n",
      "17.94\n",
      "19.03\n",
      "9.35\n",
      "39.00\n",
      "66.11\n",
      "sentence-transformers/all-mpnet-base-v2\n",
      "5.80\n",
      "11.26\n",
      "12.26\n",
      "5.66\n",
      "25.57\n",
      "50.94\n",
      "table 2: results for different embedding models on namespace-pt/msmarco.\n",
      "3.2\n",
      "chunking\n",
      "chunking documents into smaller segments is crucial for enhancing retrieval precision and avoiding\n",
      "length issues in llms. this process can be applied at various levels of granularity, such as token,\n",
      "sentence, and semantic levels.\n",
      "• token-level chunking is straightforward but may split sentences, affecting retrieval quality.\n",
      "• semantic-level chunking uses llms to determine breakpoints, context-preserving but time-\n",
      "consuming.\n",
      "• sentence-level chunking balances preserving text semantics with simplicity and efficiency.\n",
      "in this study, we use sentence-level chunking, balancing simplicity and semantic preservation. we\n",
      "examine chunking from four dimensions.\n",
      "3.2.1\n",
      "chunk size\n",
      "chunk size significantly impacts performance. larger chunks provide more context, enhancing\n",
      "comprehension but increasing process time. smaller chunks improve retrieval recall and reduce time\n",
      "but may lack sufficient context.\n",
      "finding the optimal chunk size involves a balance between some metrics such as faithfulness, and\n",
      "relevancy. faithfulness measures whether the response is hallucinated or matches the retrieved texts.\n",
      "chunk size\n",
      "lyft_2021\n",
      "average\n",
      "faithfulness\n",
      "average\n",
      "relevancy\n",
      "2048\n",
      "80.37\n",
      "91.11\n",
      "1024\n",
      "94.26\n",
      "95.56\n",
      "512\n",
      "97.59\n",
      "97.41\n",
      "256\n",
      "97.22\n",
      "97.78\n",
      "128\n",
      "95.74\n",
      "97.22\n",
      "table 3: comparison of different chunk sizes.\n",
      "relevancy measures whether the retrieved texts\n",
      "and responses match queries.\n",
      "we use the\n",
      "evaluation module of llamaindex [43] to cal-\n",
      "culate the metrics above.\n",
      "for embedding,\n",
      "we use the text-embedding-ada-0022 model,\n",
      "which supports long input length. we choose\n",
      "zephyr-7b-alpha3 and gpt-3.5-turbo4 as\n",
      "generation model and evaluation model respec-\n",
      "tively. the size of the chunk overlap is 20 tokens.\n",
      "first sixty pages of the document lyft_20215\n",
      "are used as corpus, then prompting llms to\n",
      "generate about one hundred and seventy queries\n",
      "according to chosen corpus. the impact of different chunk sizes is shown in table 3.\n",
      "2https://platform.openai.com/docs/guides/embeddings/embedding-models\n",
      "3https://huggingface.co/huggingfaceh4/zephyr-7b-alpha\n",
      "4https://www.openai.com/\n",
      "5https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/\n",
      "data/10k/lyft_2021.pdf\n",
      "5\n",
      "\n",
      "3.2.2\n",
      "chunking techniques\n",
      "advanced techniques such as small-to-big and sliding window improve retrieval quality by organizing\n",
      "chunk block relationships. small-sized blocks are used to match queries, and larger blocks that\n",
      "include the small ones along with contextual information are returned.\n",
      "to demonstrate the effectiveness of advanced chunking techniques, we use the llm-embedder [20]\n",
      "model as an embedding model. the smaller chunk size is 175 tokens, the larger chunk size is 512\n",
      "tokens and the chunk overlap is 20 tokens. techniques like small-to-big and sliding window improve\n",
      "retrieval quality by maintaining context and ensuring relevant information is retrieved. detailed\n",
      "results are shown in table 4.\n",
      "3.2.3\n",
      "embedding model selection\n",
      "choosing the right embedding model is crucial for effective semantic matching of queries\n",
      "and chunk blocks. we use the evaluation module of flagembedding6 which uses the dataset\n",
      "chunk skill\n",
      "lyft_2021\n",
      "average\n",
      "faithfulness\n",
      "average\n",
      "relevancy\n",
      "original\n",
      "95.74\n",
      "95.37\n",
      "small2big\n",
      "96.67\n",
      "95.37\n",
      "sliding window\n",
      "97.41\n",
      "96.85\n",
      "table 4: comparison of different chunk skills.\n",
      "namespace-pt/msmarco7\n",
      "as\n",
      "queries\n",
      "and\n",
      "dataset namespace-pt/msmarco-corpus8 as\n",
      "corpus to choose the appropriate open source\n",
      "embedding model.\n",
      "as shown in table 2,\n",
      "llm-embedder [20] achieves comparable\n",
      "results with baai/bge-large-en [12], however,\n",
      "the size of the former is three times smaller\n",
      "than that of the latter.\n",
      "thus, we select the\n",
      "llm-embedder [20] for its balance of\n",
      "performance and size.\n",
      "3.2.4\n",
      "metadata addition\n",
      "enhancing chunk blocks with metadata like titles, keywords, and hypothetical questions can improve\n",
      "retrieval, provide more ways to post-process retrieved texts, and help llms better understand\n",
      "retrieved information. a detailed study on metadata inclusion will be addressed in future work.\n",
      "3.3\n",
      "vector databases\n",
      "vector databases store embedding vectors with their metadata, enabling efficient retrieval of doc-\n",
      "uments relevant to queries through various indexing and approximate nearest neighbor (ann)\n",
      "methods.\n",
      "to select an appropriate vector database for our research, we evaluated several options based on\n",
      "four key criteria: multiple index types, billion-scale vector support, hybrid search, and cloud-native\n",
      "database\n",
      "multiple\n",
      "index type\n",
      "billion-\n",
      "scale\n",
      "hybrid\n",
      "search\n",
      "cloud-\n",
      "native\n",
      "weaviate\n",
      "✗\n",
      "✗\n",
      "✓\n",
      "✓\n",
      "faiss\n",
      "✓\n",
      "✗\n",
      "✗\n",
      "✗\n",
      "chroma\n",
      "✗\n",
      "✗\n",
      "✓\n",
      "✓\n",
      "qdrant\n",
      "✗\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "milvus\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "table 5: comparison of various vector databases\n",
      "capabilities. these criteria were chosen for their\n",
      "impact on flexibility, scalability, and ease of\n",
      "deployment in modern, cloud-based infrastruc-\n",
      "tures. multiple index types provide the flexibil-\n",
      "ity to optimize searches based on different data\n",
      "characteristics and use cases. billion-scale vec-\n",
      "tor support is crucial for handling large datasets\n",
      "in llm applications. hybrid search combines\n",
      "vector search with traditional keyword search,\n",
      "enhancing retrieval accuracy. finally, cloud-\n",
      "native capabilities ensure seamless integration, scalability, and management in cloud environments.\n",
      "table 5 presents a detailed comparison of five open-source vector databases: weaviate, faiss,\n",
      "chroma, qdrant, and milvus.\n",
      "our evaluation indicates that milvus stands out as the most comprehensive solution among the\n",
      "databases evaluated, meeting all the essential criteria and outperforming other open-source options.\n",
      "6https://github.com/flagopen/flagembedding\n",
      "7https://huggingface.co/datasets/namespace-pt/msmarco\n",
      "8https://huggingface.co/datasets/namespace-pt/msmarco-corpus\n",
      "6\n",
      "\n",
      "method\n",
      "trec dl19\n",
      "trec dl20\n",
      "map\n",
      "ndcg@10\n",
      "r@50\n",
      "r@1k\n",
      "latency\n",
      "map\n",
      "ndcg@10\n",
      "r@50\n",
      "r@1k\n",
      "latency\n",
      "unsupervised\n",
      "bm25\n",
      "30.13\n",
      "50.58\n",
      "38.32\n",
      "75.01\n",
      "0.07\n",
      "28.56\n",
      "47.96\n",
      "46.18\n",
      "78.63\n",
      "0.29\n",
      "contriever\n",
      "23.99\n",
      "44.54\n",
      "37.54\n",
      "74.59\n",
      "3.06\n",
      "23.98\n",
      "42.13\n",
      "43.81\n",
      "75.39\n",
      "0.98\n",
      "supervised\n",
      "llm-embedder\n",
      "44.66\n",
      "70.20\n",
      "49.06\n",
      "84.48\n",
      "2.61\n",
      "45.60\n",
      "68.76\n",
      "61.36\n",
      "84.41\n",
      "0.71\n",
      "+ query rewriting\n",
      "44.56\n",
      "67.89\n",
      "51.45\n",
      "85.35\n",
      "7.80\n",
      "45.16\n",
      "65.62\n",
      "59.63\n",
      "83.45\n",
      "2.06\n",
      "+ query decomposition\n",
      "41.93\n",
      "66.10\n",
      "48.66\n",
      "82.62\n",
      "14.98\n",
      "43.30\n",
      "64.95\n",
      "57.74\n",
      "84.18\n",
      "2.01\n",
      "+ hyde\n",
      "50.87\n",
      "75.44\n",
      "54.93\n",
      "88.76\n",
      "7.21\n",
      "50.94\n",
      "73.94\n",
      "63.80\n",
      "88.03\n",
      "2.14\n",
      "+ hybrid search\n",
      "47.14\n",
      "72.50\n",
      "51.13\n",
      "89.08\n",
      "3.20\n",
      "47.72\n",
      "69.80\n",
      "64.32\n",
      "88.04\n",
      "0.77\n",
      "+ hyde + hybrid search\n",
      "52.13\n",
      "73.34\n",
      "55.38\n",
      "90.42\n",
      "11.16\n",
      "53.13\n",
      "72.72\n",
      "66.14\n",
      "90.67\n",
      "2.95\n",
      "table 6: results for different retrieval methods on trec dl19/20. the best result for each method\n",
      "is made bold and the second is underlined.\n",
      "configuration\n",
      "trec dl19\n",
      "trec dl20\n",
      "map\n",
      "ndcg@10\n",
      "r@50\n",
      "r@1k\n",
      "latency\n",
      "map\n",
      "ndcg@10\n",
      "r@50\n",
      "r@1k\n",
      "latency\n",
      "hyde\n",
      "w/ 1 pseudo-doc\n",
      "48.77\n",
      "72.49\n",
      "53.20\n",
      "87.73\n",
      "8.08\n",
      "51.31\n",
      "70.37\n",
      "63.28\n",
      "87.81\n",
      "2.09\n",
      "w/ 1 pseudo-doc + query\n",
      "50.87\n",
      "75.44\n",
      "54.93\n",
      "88.76\n",
      "7.21\n",
      "50.94\n",
      "73.94\n",
      "63.80\n",
      "88.03\n",
      "2.14\n",
      "w/ 8 pseudo-doc + query\n",
      "51.64\n",
      "75.12\n",
      "54.51\n",
      "89.17\n",
      "14.15\n",
      "53.14\n",
      "73.65\n",
      "65.79\n",
      "88.67\n",
      "3.44\n",
      "table 7: hyde with different concatenation of hypothetical documents and queries.\n",
      "3.4\n",
      "retrieval methods\n",
      "given a user query, the retrieval module selects the top-k relevant documents from a pre-built corpus\n",
      "based on the similarity between the query and the documents. the generation model then uses\n",
      "these documents to formulate an appropriate response to the query. however, original queries often\n",
      "underperform due to poor expression and lack of semantic information [6], negatively impacting the\n",
      "retrieval process. to address these issues, we evaluated three query transformation methods using the\n",
      "llm-embedder recommended in section 3.2 as the query and document encoder:\n",
      "• query rewriting: query rewriting refines queries to better match relevant documents. inspired\n",
      "by the rewrite-retrieve-read framework [9], we prompt an llm to rewrite queries to enhance\n",
      "performance.\n",
      "• query decomposition: this approach involves retrieving documents based on sub-questions\n",
      "derived from the original query, which is more complex to comprehend and handle.\n",
      "• pseudo-documents generation: this approach generates a hypothetical document based on the\n",
      "user query and uses the embedding of hypothetical answers to retrieve similar documents. one\n",
      "notable implement is hyde [10],\n",
      "recent studies, such as [44], indicate that combining lexical-based search with vector search signifi-\n",
      "cantly enhances performance. in this study, we use bm25 for sparse retrieval and contriever [45], an\n",
      "unsupervised contrastive encoder, for dense retrieval, serving as two robust baselines based on thakur\n",
      "et al. [46].\n",
      "3.4.1\n",
      "results for different retrieval methods\n",
      "we evaluated the performance of different search methods on the trec dl 2019 and 2020 passage\n",
      "ranking datasets. the results presented in table 6 show that supervised methods significantly\n",
      "outperformed unsupervised methods. combining with hyde and hybrid search, llm-embedder\n",
      "achieves the highest scores. however, query rewriting and query decomposition did not enhance\n",
      "retrieval performance as effectively. considering the best performance and tolerated latency, we\n",
      "recommend hybrid search with hyde as the default retrieval method. taking efficiency into\n",
      "consideration, hybrid search combines sparse retrieval (bm25) and dense retrieval (original\n",
      "embedding) and achieves notable performance with relatively low latency.\n",
      "3.4.2\n",
      "hyde with different concatenation of documents and query\n",
      "table 7 shows the impact of different concatenation strategies for hypothetical documents and queries\n",
      "using hyde. concatenating multiple pseudo-documents with the original query can significantly\n",
      "7\n",
      "\n",
      "hyperparameter\n",
      "trec dl19\n",
      "trec dl20\n",
      "map\n",
      "ndcg@10\n",
      "r@50\n",
      "r@1k\n",
      "latency\n",
      "map\n",
      "ndcg@10\n",
      "r@50\n",
      "r@1k\n",
      "latency\n",
      "hybrid search\n",
      "α = 0.1\n",
      "46.00\n",
      "70.87\n",
      "49.24\n",
      "88.89\n",
      "2.98\n",
      "46.54\n",
      "69.05\n",
      "63.36\n",
      "87.32\n",
      "0.90\n",
      "α = 0.3\n",
      "47.14\n",
      "72.50\n",
      "51.13\n",
      "89.08\n",
      "3.20\n",
      "47.72\n",
      "69.80\n",
      "64.32\n",
      "88.04\n",
      "0.77\n",
      "α = 0.5\n",
      "47.36\n",
      "72.24\n",
      "52.71\n",
      "88.09\n",
      "3.02\n",
      "47.19\n",
      "68.12\n",
      "64.90\n",
      "87.86\n",
      "0.87\n",
      "α = 0.7\n",
      "47.21\n",
      "71.89\n",
      "52.40\n",
      "88.01\n",
      "3.15\n",
      "45.82\n",
      "67.30\n",
      "64.23\n",
      "87.92\n",
      "1.02\n",
      "α = 0.9\n",
      "46.35\n",
      "70.67\n",
      "52.64\n",
      "88.22\n",
      "2.74\n",
      "44.02\n",
      "65.55\n",
      "63.22\n",
      "87.76\n",
      "1.20\n",
      "table 8: results of hybrid search with different alpha values.\n",
      "method\n",
      "ms marco passage ranking\n",
      "base model\n",
      "# params\n",
      "mrr@1\n",
      "mrr@10\n",
      "mrr@1k\n",
      "hit rate@10\n",
      "latency\n",
      "w/o reranking\n",
      "random ordering\n",
      "-\n",
      "-\n",
      "0.011\n",
      "0.027\n",
      "0.068\n",
      "0.092\n",
      "-\n",
      "bm25\n",
      "-\n",
      "-\n",
      "6.52\n",
      "11.65\n",
      "12.59\n",
      "24.63\n",
      "-\n",
      "dlm reranking\n",
      "monot5\n",
      "t5-base\n",
      "220m\n",
      "21.62\n",
      "31.78\n",
      "32.40\n",
      "54.07\n",
      "4.5\n",
      "monobert\n",
      "bert-large\n",
      "340m\n",
      "21.65\n",
      "31.69\n",
      "32.35\n",
      "53.38\n",
      "15.8\n",
      "rankllama\n",
      "llama-2-7b\n",
      "7b\n",
      "22.08\n",
      "32.35\n",
      "32.97\n",
      "54.53\n",
      "82.4\n",
      "tilde reranking\n",
      "tildev2\n",
      "bert-base\n",
      "110m\n",
      "18.57\n",
      "27.83\n",
      "28.60\n",
      "49.07\n",
      "0.02\n",
      "table 9: results of different reranking methods on the dev set of the ms marco passage ranking\n",
      "dataset. for each query, the top-1000 candidate passages retrieved by bm25 are reranked. latency is\n",
      "measured in seconds per query.\n",
      "enhance retrieval performance, though at the cost of increased latency, suggesting a trade-off between\n",
      "retrieval effectiveness and efficiency. however, indiscriminately increasing the number of hypothetical\n",
      "documents does not yield significant benefits and substantially raises latency, indicating that using a\n",
      "single hypothetical document is sufficient.\n",
      "3.4.3\n",
      "hybrid search with different weight on sparse retrieval\n",
      "table 8 presents the impact of different α values in hybrid search, where α controls the weighting\n",
      "between sparse retrieval and dense retrieval components. the relevance score is calculated as follows:\n",
      "sh = α · ss + sd\n",
      "(1)\n",
      "where ss, sd are the normalized relevance scores from sparse retrieval and dense retrieval respectively,\n",
      "and sh is the total retrieval score.\n",
      "we evaluated five different α values to determine their influence on performance. the results indicate\n",
      "that an α value of 0.3 yields the best performance, demonstrating that appropriate adjustment of\n",
      "α can enhance retrieval effectiveness to a certain extent. therefore, we selected α = 0.3 for our\n",
      "retrieval and main experiments. additional implementation details are presented in appendix a.2.\n",
      "3.5\n",
      "reranking methods\n",
      "after the initial retrieval, a reranking phase is employed to enhance the relevance of the retrieved\n",
      "documents, ensuring that the most pertinent information appears at the top of the list. this phase uses\n",
      "more precise and time-intensive methods to reorder documents effectively, increasing the similarity\n",
      "between the query and the top-ranked documents.\n",
      "we consider two approaches in our reranking module: dlm reranking, which utilizes classifi-\n",
      "cation, and tilde reranking, which focuses on query likelihoods. these approaches prioritize\n",
      "performance and efficiency, respectively.\n",
      "• dlm reranking:\n",
      "this method leverages deep language models (dlms) [25–27] for reranking.\n",
      "these models are fine-tuned to classify document relevancy to a query as “true” or “false”. during\n",
      "fine-tuning, the model is trained with concatenated query and document inputs, labeled by relevancy.\n",
      "at inference, documents are ranked based on the probability of the “true” token.\n",
      "• tilde reranking:\n",
      "tilde [28, 29] calculates the likelihood of each query term independently\n",
      "by predicting token probabilities across the model’s vocabulary. documents are scored by summing\n",
      "8\n",
      "\n",
      "method\n",
      "nq\n",
      "tqa\n",
      "hotpotqa\n",
      "avg.\n",
      "avg. token\n",
      "f1\n",
      "#token\n",
      "f1\n",
      "#token\n",
      "f1\n",
      "#token\n",
      "w/o summarization\n",
      "origin prompt\n",
      "27.07\n",
      "124\n",
      "33.61\n",
      "152\n",
      "33.92\n",
      "141\n",
      "31.53\n",
      "139\n",
      "extractive method\n",
      "bm25\n",
      "27.97\n",
      "40\n",
      "32.44\n",
      "59\n",
      "28.00\n",
      "63\n",
      "29.47\n",
      "54\n",
      "contriever\n",
      "23.62\n",
      "42\n",
      "33.79\n",
      "65\n",
      "23.64\n",
      "60\n",
      "27.02\n",
      "56\n",
      "recomp (extractive)\n",
      "27.84\n",
      "34\n",
      "35.32\n",
      "60\n",
      "29.46\n",
      "58\n",
      "30.87\n",
      "51\n",
      "abstractive method\n",
      "selectivecontext\n",
      "25.05\n",
      "65\n",
      "34.25\n",
      "70\n",
      "34.43\n",
      "66\n",
      "31.24\n",
      "67\n",
      "longllmlingua\n",
      "21.32\n",
      "51\n",
      "32.81\n",
      "56\n",
      "30.79\n",
      "57\n",
      "28.29\n",
      "55\n",
      "recomp (abstractive)\n",
      "33.68\n",
      "59\n",
      "35.87\n",
      "61\n",
      "29.01\n",
      "57\n",
      "32.85\n",
      "59\n",
      "table 10: comparison between different summarization methods.\n",
      "the pre-calculated log probabilities of query tokens, allowing for rapid reranking at inference.\n",
      "tildev2 improves this by indexing only document-present tokens, using nce loss, and expanding\n",
      "documents, thus enhancing efficiency and reducing index size.\n",
      "our experiments were conducted on the ms marco passage ranking dataset [47], a large-scale\n",
      "dataset for machine reading comprehension. we follow and make modifications to the implementation\n",
      "provided by pygaggle [26] and tilde [28], using the models monot5, monobert, rankllama\n",
      "and tildev2. reranking results are shown in table 9. we recommend monot5 as a comprehensive\n",
      "method balancing performance and efficiency. rankllama is suitable for achieving the best\n",
      "performance, while tildev2 is ideal for the quickest experience on a fixed collection. details on\n",
      "the experimental setup and results are presented in appendix a.3.\n",
      "3.6\n",
      "document repacking\n",
      "the performance of subsequent processes, such as llm response generation, may be affected by the\n",
      "order documents are provided. to address this issue, we incorporate a compact repacking module into\n",
      "the workflow after reranking, featuring three repacking methods: “forward”, “reverse” and “sides”.\n",
      "the “forward” method repacks documents by descending relevancy scores from the reranking phase,\n",
      "whereas the “reverse” arranges them in ascending order. inspired by liu et al. [48], concluding that\n",
      "optimal performance is achieved when relevant information is placed at the head or tail of the input,\n",
      "we also include a “sides” option.\n",
      "since the repacking method primarily affects subsequent modules, we select the best repacking\n",
      "method in section 4 by testing it in combination with other modules. in this section, we choose the\n",
      "“sides” method as the default repacking method.\n",
      "3.7\n",
      "summarization\n",
      "retrieval results may contain redundant or unnecessary information, potentially preventing llms\n",
      "from generating accurate responses. additionally, long prompts can slow down the inference process.\n",
      "therefore, efficient methods to summarize retrieved documents are crucial in the rag pipeline.\n",
      "summarization tasks can be extractive or abstractive. extractive methods segment text into sen-\n",
      "tences, then score and rank them based on importance. abstractive compressors synthesize infor-\n",
      "mation from multiple documents to rephrase and generate a cohesive summary. these tasks can be\n",
      "query-based or non-query-based. in this paper, as rag retrieves information relevant to queries, we\n",
      "focus exclusively on query-based methods.\n",
      "• recomp:\n",
      "recomp [23] has extractive and abstractive compressors. the extractive compressor\n",
      "selects useful sentences, while the abstractive compressor synthesizes information from multiple\n",
      "documents.\n",
      "• longllmlingua:\n",
      "longllmlingua [49] improves llmlingua by focusing on key informa-\n",
      "tion related to the query.\n",
      "• selective context\n",
      "selective context enhances llm efficiency by identifying and removing\n",
      "redundant information in the input context. it evaluates the informativeness of lexical units using\n",
      "9\n",
      "\n",
      "self-information computed by a base causal language model. this method is non-query-based,\n",
      "allowing a comparison between query-based and non-query-based approaches.\n",
      "we evaluate these methods on three benchmark datasets: nq, triviaqa, and hotpotqa. comparative\n",
      "results of different summarization methods are shown in table 10. we recommend recomp for\n",
      "its outstanding performance. longllmlingua does not perform well but demonstrates better\n",
      "generalization capabilities as it was not trained on these experimental datasets. therefore, we consider\n",
      "it as an alternative method. additional implementation details and discussions on non-query-based\n",
      "methods are provided in appendix a.4.\n",
      "3.8\n",
      "generator fine-tuning\n",
      "in this section, we focus on fine-tuning the generator while leaving retriever fine-tuning for future\n",
      "exploration. we aim to investigate the impact of fine-tuning, particularly the influence of relevant or\n",
      "irrelevant contexts on the generator’s performance.\n",
      "formally, we denote x as the query fed into the rag system, and d as the contexts for this input.\n",
      "the fine-tuning loss of the generator is the negative log-likelihood of the ground-truth output y.\n",
      "to explore the impact of fine-tuning, especially relevant and irrelevant contexts, we define dgold as a\n",
      "context relevant to the query, and drandom as a randomly retrieved context. we train the model by\n",
      "varying the composition of d as follows:\n",
      "• dg: the augmented context consists of query-relevant documents, denoted as dg = {dgold}.\n",
      "• dr: the context contains one randomly sampled document, denoted as dr = {drandom}.\n",
      "• dgr: the augmented context comprises a relevant document and a randomly-selected one, denoted\n",
      "as dgr = {dgold, drandom}.\n",
      "• dgg: the augmented context consists of two copies of a query-relevant document, denoted as\n",
      "dgg = {dgold, dgold}.\n",
      "we denote the base lm generator not fine-tuned as mb , and the model fine-tuned under the\n",
      "corresponding d as mg, mr, mgr, mgg. we fine-tuned our model on several qa and reading\n",
      "d\n",
      "dg\n",
      "dr\n",
      "dgr\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "coverage score\n",
      "models trained with different method\n",
      "mb\n",
      "mg\n",
      "mr\n",
      "mgr\n",
      "mgg\n",
      "figure 3: results of generator fine-tuning.\n",
      "comprehension datasets. ground-truth coverage\n",
      "is used as our evaluation metric since qa task\n",
      "answers are relatively short. we select llama-2-\n",
      "7b [50] as the base model. similar to training,\n",
      "we evaluate all trained models on validation sets\n",
      "with dg, dr, dgr, and d∅, where d∅indicates\n",
      "inference without retrieval. figure 3 presents\n",
      "our main results. models trained with a mix of\n",
      "relevant and random documents (mgr) perform\n",
      "best when provided with either gold or mixed\n",
      "contexts. this suggests that mixing relevant and\n",
      "random contexts during training can enhance the\n",
      "generator’s robustness to irrelevant information\n",
      "while ensuring effective utilization of relevant\n",
      "contexts. therefore, we identify the practice of\n",
      "augmenting with a few relevant and randomly-selected documents during training as the best\n",
      "approach. detailed dataset information, hyperparameters and experimental results can be found in\n",
      "appendix a.5.\n",
      "4\n",
      "searching for best rag practices\n",
      "in the following section, we investigate the optimal practices for implementing rag. to begin\n",
      "with, we used the default practice identified in section 3 for each module. following the workflow\n",
      "depicted in figure 1, we sequentially optimized individual modules and selected the most effective\n",
      "option among alternatives. this iterative process continued until we determined the best method for\n",
      "implementing the final summarization module. based on section 3.8, we used the llama2-7b-chat\n",
      "model fine-tuned where each query was augmented by a few random-selected and relevant documents\n",
      "10\n",
      "\n",
      "method\n",
      "commonsense\n",
      "fact check\n",
      "odqa\n",
      "multihop\n",
      "medical\n",
      "rag\n",
      "avg.\n",
      "acc\n",
      "acc\n",
      "em\n",
      "f1\n",
      "em\n",
      "f1\n",
      "acc\n",
      "score\n",
      "score\n",
      "f1\n",
      "latency\n",
      "classification module , hybrid with hyde, monot5, sides, recomp\n",
      "w/o classification\n",
      "0.719\n",
      "0.505\n",
      "0.391\n",
      "0.450\n",
      "0.212\n",
      "0.255\n",
      "0.528\n",
      "0.540\n",
      "0.465\n",
      "0.353\n",
      "16.58\n",
      "+ classification\n",
      "0.727\n",
      "0.595\n",
      "0.393\n",
      "0.450\n",
      "0.207\n",
      "0.257\n",
      "0.460\n",
      "0.580\n",
      "0.478\n",
      "0.353\n",
      "11.71\n",
      "with classification,\n",
      "retrieval module , monot5, sides, recomp\n",
      "+ hyde\n",
      "0.718\n",
      "0.595\n",
      "0.320\n",
      "0.373\n",
      "0.170\n",
      "0.213\n",
      "0.400\n",
      "0.545\n",
      "0.443\n",
      "0.293\n",
      "11.58\n",
      "+ original\n",
      "0.721\n",
      "0.585\n",
      "0.300\n",
      "0.350\n",
      "0.153\n",
      "0.197\n",
      "0.390\n",
      "0.486\n",
      "0.428\n",
      "0.273\n",
      "1.44\n",
      "+ hybrid\n",
      "0.718\n",
      "0.595\n",
      "0.347\n",
      "0.397\n",
      "0.190\n",
      "0.240\n",
      "0.750\n",
      "0.498\n",
      "0.477\n",
      "0.318\n",
      "1.45\n",
      "+ hybrid with hyde\n",
      "0.727\n",
      "0.595\n",
      "0.393\n",
      "0.450\n",
      "0.207\n",
      "0.257\n",
      "0.460\n",
      "0.580\n",
      "0.478\n",
      "0.353\n",
      "11.71\n",
      "with classification, hybrid with hyde, reranking module , sides, recomp\n",
      "w/o reranking\n",
      "0.720\n",
      "0.591\n",
      "0.365\n",
      "0.429\n",
      "0.211\n",
      "0.260\n",
      "0.512\n",
      "0.530\n",
      "0.470\n",
      "0.334\n",
      "10.31\n",
      "+ monot5\n",
      "0.727\n",
      "0.595\n",
      "0.393\n",
      "0.450\n",
      "0.207\n",
      "0.257\n",
      "0.460\n",
      "0.580\n",
      "0.478\n",
      "0.353\n",
      "11.71\n",
      "+ monobert\n",
      "0.723\n",
      "0.593\n",
      "0.383\n",
      "0.443\n",
      "0.217\n",
      "0.259\n",
      "0.482\n",
      "0.551\n",
      "0.475\n",
      "0.351\n",
      "11.65\n",
      "+ rankllama\n",
      "0.723\n",
      "0.597\n",
      "0.382\n",
      "0.443\n",
      "0.197\n",
      "0.240\n",
      "0.454\n",
      "0.558\n",
      "0.470\n",
      "0.342\n",
      "13.51\n",
      "+ tildev2\n",
      "0.725\n",
      "0.588\n",
      "0.394\n",
      "0.456\n",
      "0.209\n",
      "0.255\n",
      "0.486\n",
      "0.536\n",
      "0.476\n",
      "0.355\n",
      "11.26\n",
      "with classification, hybrid with hyde, monot5, repacking module , recomp\n",
      "+ sides\n",
      "0.727\n",
      "0.595\n",
      "0.393\n",
      "0.450\n",
      "0.207\n",
      "0.257\n",
      "0.460\n",
      "0.580\n",
      "0.478\n",
      "0.353\n",
      "11.71\n",
      "+ forward\n",
      "0.722\n",
      "0.599\n",
      "0.379\n",
      "0.437\n",
      "0.215\n",
      "0.260\n",
      "0.472\n",
      "0.542\n",
      "0.474\n",
      "0.349\n",
      "11.68\n",
      "+ reverse\n",
      "0.728\n",
      "0.592\n",
      "0.387\n",
      "0.445\n",
      "0.219\n",
      "0.263\n",
      "0.532\n",
      "0.560\n",
      "0.483\n",
      "0.354\n",
      "11.70\n",
      "with classification, hybrid with hyde, monot5, reverse, summarization module\n",
      "w/o summarization\n",
      "0.729\n",
      "0.591\n",
      "0.402\n",
      "0.457\n",
      "0.205\n",
      "0.252\n",
      "0.528\n",
      "0.533\n",
      "0.480\n",
      "0.355\n",
      "10.97\n",
      "+ recomp\n",
      "0.728\n",
      "0.592\n",
      "0.387\n",
      "0.445\n",
      "0.219\n",
      "0.263\n",
      "0.532\n",
      "0.560\n",
      "0.483\n",
      "0.354\n",
      "11.70\n",
      "+ longllmlingua\n",
      "0.713\n",
      "0.581\n",
      "0.362\n",
      "0.423\n",
      "0.199\n",
      "0.245\n",
      "0.530\n",
      "0.539\n",
      "0.466\n",
      "0.334\n",
      "16.17\n",
      "table 11: results of the search for optimal rag practices. modules enclosed in a boxed module\n",
      "are under investigation to determine the best method. the underlined method represents the selected\n",
      "implementation. the “avg” (average score) is calculated based on the acc, em, and rag scores for\n",
      "all tasks, while the average latency is measured in seconds per query. the best scores are highlighted\n",
      "in bold.\n",
      "as the generator. we used milvus to build a vector database that includes 10 million text of english\n",
      "wikipedia and 4 million text of medical data. we also investigated the impact of removing the query\n",
      "classification, reranking, and summarization modules to assess their contributions.\n",
      "4.1\n",
      "comprehensive evaluation\n",
      "we conducted extensive experiments across various nlp tasks and datasets to assess the perfor-\n",
      "mance of rag systems. specifically: (i) commonsense reasoning; (ii) fact checking; (iii)\n",
      "open-domain qa; (iv) multihop qa; (v) medical qa. for further details on the tasks and\n",
      "their corresponding datasets, please refer to appendix a.6. furthermore, we evaluated the rag\n",
      "capabilities on subsets extracted from these datasets, employing the metrics recommended in ra-\n",
      "gas [51], including faithfulness, context relevancy, answer relevancy, and answer correctness.\n",
      "additionally, we measured retrieval similarity by computing the cosine similarity between retrieved\n",
      "documents and gold documents.\n",
      "we used accuracy as the evaluation metric for the tasks of commonsense reasoning, fact checking,\n",
      "and medical qa. for open-domain qa and multihop qa, we employed token-level f1 score and\n",
      "exact match (em) score. the final rag score was calculated by averaging the aforementioned five\n",
      "rag capabilities. we followed trivedi et al. [52] and sub-sampled up to 500 examples from each\n",
      "dataset.\n",
      "4.2\n",
      "results and analysis\n",
      "based on the experimental results presented in table 11, the following key insights emerge:\n",
      "• query classification module: this module is referenced and contributes to both effectiveness\n",
      "and efficiency, leading to an average improvement in the overall score from 0.428 to 0.443 and a\n",
      "reduction in latency time from 16.41 to 11.58 seconds per query.\n",
      "11\n",
      "\n",
      "• retrieval module: while the “hybrid with hyde” method attained the highest rag score of\n",
      "0.58, it does so at a considerable computational cost with 11.71 second per query. consequently,\n",
      "the “hybrid” or “original” methods are recommended, as they reduce latency while maintaining\n",
      "comparable performance.\n",
      "• reranking module: the absence of a reranking module led to a noticeable drop in performance,\n",
      "highlighting its necessity. monot5 achieved the highest average score, affirming its efficacy in\n",
      "augmenting the relevance of retrieved documents. this indicates the critical role of reranking in\n",
      "enhancing the quality of generated responses.\n",
      "• repacking module: the reverse configuration exhibited superior performance, achieving an\n",
      "rag score of 0.560. this indicates that positioning more relevant context closer to the query leads\n",
      "to optimal outcomes.\n",
      "• summarization module: recomp demonstrated superior performance, although achieving compa-\n",
      "rable results with lower latency was possible by removing the summarization module. nevertheless,\n",
      "recomp remains the preferred choice due to its capability to address the generator’s maximum\n",
      "length constraints. in time-sensitive applications, removing summarization could effectively reduce\n",
      "response time.\n",
      "the experimental results demonstrate that each module contributes uniquely to the overall perfor-\n",
      "mance of the rag system. the query classification module enhances accuracy and reduces latency,\n",
      "while the retrieval and reranking modules significantly improve the system’s ability to handle diverse\n",
      "queries. the repacking and summarization modules further refine the system’s output, ensuring\n",
      "high-quality responses across different tasks.\n",
      "5\n",
      "discussion\n",
      "5.1\n",
      "best practices for implementing rag\n",
      "according to our experimental findings, we suggest two distinct recipes or practices for implementing\n",
      "rag systems, each customized to address specific requirements: one focusing on maximizing\n",
      "performance, and the other on striking a balance between efficiency and efficacy.\n",
      "best performance practice: to achieve the highest performance, it is recommended to incorporate\n",
      "query classification module, use the “hybrid with hyde” method for retrieval, employ monot5 for\n",
      "reranking, opt for reverse for repacking, and leverage recomp for summarization. this configuration\n",
      "yielded the highest average score of 0.483, albeit with a computationally-intensive process.\n",
      "balanced efficiency practice: in order to achieve a balance between performance and efficiency,\n",
      "it is recommended to incorporate the query classification module, implement the hybrid method\n",
      "for retrieval, use tildev2 for reranking, opt for reverse for repacking, and employ recomp for\n",
      "summarization. given that the retrieval module accounts for the majority of processing time in the\n",
      "system, transitioning to the hybrid method while keeping other modules unchanged can substantially\n",
      "reduce latency while preserving a comparable performance.\n",
      "5.2\n",
      "multimodal extension\n",
      "we have extended rag to multimodal applications. specifically, we have incorporated text2image\n",
      "and image2text retrieval capabilities into the system with a substantial collection of paired image and\n",
      "textual descriptions as a retrieval source. as depicted in figure 4, the text2image capability speeds\n",
      "up the image generation process when a user query aligns well with the textual descriptions of stored\n",
      "images (i.e., “retrieval as generation” strategy), while the image2text functionality comes into play\n",
      "when a user provides an image and engages in conversation about the input image. these multimodal\n",
      "rag capabilities offer the following advantages:\n",
      "• groundedness: retrieval methods provide information from verified multimodal materials, thereby\n",
      "ensuring authenticity and specificity. in contrast, on-the-fly generation relies on models to generate\n",
      "new content, which can occasionally result in factual errors or inaccuracies.\n",
      "• efficiency: retrieval methods are typically more efficient, especially when the answer already\n",
      "exists in stored materials. conversely, generation methods may require more computational\n",
      "resources to produce new content, particularly for images or lengthy texts.\n",
      "12\n",
      "\n",
      "a dog is sleeping\n",
      "retrieval\n",
      "retrieval\n",
      "retrieval\n",
      "a dog is sleeping.\n",
      "a dog is drinking water\n",
      "a dog is sleeping\n",
      "retrieval\n",
      "a dog is sleeping\n",
      "image2text retrieval\n",
      "text2image retrieval\n",
      "high similarity\n",
      "low similarity\n",
      "user query\n",
      "image caption model\n",
      "image generation model\n",
      "figure 4: workflow of multimodal retrieval. the upper section illustrates the text-to-image retrieval\n",
      "process. initially, a text query is used to find images in the database with the highest similarity. if a\n",
      "high similarity is found, the image is returned directly. if not, an image generation model is employed\n",
      "to create and return an appropriate image. the lower section demonstrates the image-to-text retrieval\n",
      "process. here, a user-provided image is matched with images in the database to find the highest\n",
      "similarity. if a high similarity is identified, the pre-stored caption of the matching image is returned.\n",
      "otherwise, an image captioning model generates and returns a new caption.\n",
      "• maintainability: generation models often necessitate careful fine-tuning to tailor them for new\n",
      "applications. in contrast, retrieval-based methods can be improved to address new demands by\n",
      "simply enlarging the size and enhancing the quality of retrieval sources.\n",
      "we plan to broaden the application of this strategy to include other modalities, such as video and\n",
      "speech, while also exploring efficient and effective cross-modal retrieval techniques.\n",
      "6\n",
      "conclusion\n",
      "in this study, we aim to identify optimal practices for implementing retrieval-augmented generation\n",
      "in order to improve the quality and reliability of content produced by large language models. we\n",
      "systematically assessed a range of potential solutions for each module within the rag framework\n",
      "and recommended the most effective approach for each module. furthermore, we introduced a\n",
      "comprehensive evaluation benchmark for rag systems and conducted extensive experiments to\n",
      "determine the best practices among various alternatives. our findings not only contribute to a deeper\n",
      "understanding of retrieval-augmented generation systems but also establish a foundation for future\n",
      "research.\n",
      "limitations\n",
      "we have evaluated the impact of various methods for fine-tuning llm generators. previous studies\n",
      "have demonstrated the feasibility of training both the retriever and generator jointly. we would\n",
      "like to explore this possibility in the future. in this study, we embraced the principle of modular\n",
      "13\n",
      "\n",
      "design to simplify the search for optimal rag implementations, thereby reducing complexity. due\n",
      "to the daunting costs associated with constructing vector databases and conducting experiments, our\n",
      "evaluation was limited to investigating the effectiveness and influence of representative chunking\n",
      "techniques within the chunking module. it would be intriguing to further explore the impact of\n",
      "different chunking techniques on the entire rag systems. while we have discussed the application of\n",
      "rag in the domain of nlp and extended its scope to image generation, an enticing avenue for future\n",
      "exploration would involve expanding this research to other modalities such as speech and video.\n",
      "acknowledgments\n",
      "the authors would like to thank the anonymous reviewers for their valuable comments. this work\n",
      "was supported by national natural science foundation of china (no. 62076068).\n",
      "references\n",
      "[1] long ouyang, jeff wu, xu jiang, diogo almeida, carroll l. wainwright, pamela mishkin,\n",
      "chong zhang, sandhini agarwal, katarina slama, alex ray, john schulman, jacob hilton,\n",
      "fraser kelton, luke miller, maddie simens, amanda askell, peter welinder, paul christiano,\n",
      "jan leike, and ryan lowe. training language models to follow instructions with human\n",
      "feedback. in proceedings of the conference on neural information processing systems (neurips\n",
      "2022), 2022.\n",
      "[2] rafael rafailov, archit sharma, eric mitchell, stefano ermon, christopher d manning, and\n",
      "chelsea finn. direct preference optimization: your language model is secretly a reward model.\n",
      "arxiv preprint arxiv:2305.18290, 2023.\n",
      "[3] yao zhao, rishabh joshi, tianqi liu, misha khalman, mohammad saleh, and peter j liu. slic-\n",
      "hf: sequence likelihood calibration with human feedback. arxiv preprint arxiv:2305.10425,\n",
      "2023.\n",
      "[4] zheng yuan, hongyi yuan, chuanqi tan, wei wang, songfang huang, and fei huang. rrhf:\n",
      "rank responses to align language models with human feedback without tears. arxiv preprint\n",
      "arxiv:2304.05302, 2023.\n",
      "[5] wenhao liu, xiaohua wang, muling wu, tianlong li, changze lv, zixuan ling, jianhao zhu,\n",
      "cenyuan zhang, xiaoqing zheng, and xuanjing huang. aligning large language models with\n",
      "human preferences through representation engineering. arxiv preprint arxiv:2312.15997, 2023.\n",
      "[6] yunfan gao, yun xiong, xinyu gao, kangxiang jia, jinliu pan, yuxi bi, yi dai, jiawei sun,\n",
      "and haofen wang. retrieval-augmented generation for large language models: a survey. arxiv\n",
      "preprint arxiv:2312.10997, 2023.\n",
      "[7] huayang li, yixuan su, deng cai, yan wang, and lemao liu. a survey on retrieval-augmented\n",
      "text generation. arxiv preprint arxiv:2202.01110, 2022.\n",
      "[8] deng cai, yan wang, lemao liu, and shuming shi. recent advances in retrieval-augmented\n",
      "text generation. in proceedings of the 45th international acm sigir conference on research\n",
      "and development in information retrieval, pages 3417–3419, 2022.\n",
      "[9] xinbei ma, yeyun gong, pengcheng he, hai zhao, and nan duan. query rewriting for\n",
      "retrieval-augmented large language models. arxiv preprint arxiv:2305.14283, 2023.\n",
      "[10] luyu gao, xueguang ma, jimmy lin, and jamie callan. precise zero-shot dense retrieval\n",
      "without relevance labels. arxiv preprint arxiv:2212.10496, 2022.\n",
      "[11] liang wang, nan yang, xiaolong huang, binxing jiao, linjun yang, daxin jiang, rangan\n",
      "majumder, and furu wei. text embeddings by weakly-supervised contrastive pre-training.\n",
      "arxiv preprint arxiv:2212.03533, 2022.\n",
      "[12] shitao xiao, zheng liu, peitian zhang, and niklas muennighoff. c-pack: packaged resources\n",
      "to advance general chinese embedding, 2023.\n",
      "14\n",
      "\n",
      "[13] openai. gpt-4 technical report. corr, abs/2303.08774, 2023. doi: 10.48550/arxiv.2303.\n",
      "08774. url https://doi.org/10.48550/arxiv.2303.08774.\n",
      "[14] hugo touvron, thibaut lavril, gautier izacard, xavier martinet, marie-anne lachaux, timo-\n",
      "thée lacroix, baptiste rozière, naman goyal, eric hambro, faisal azhar, et al. llama: open\n",
      "and efficient foundation language models. arxiv preprint arxiv:2302.13971, 2023.\n",
      "[15] yue zhang, yafu li, leyang cui, deng cai, lemao liu, tingchen fu, xinting huang, enbo\n",
      "zhao, yu zhang, yulong chen, et al. siren’s song in the ai ocean: a survey on hallucination in\n",
      "large language models. arxiv preprint arxiv:2309.01219, 2023.\n",
      "[16] xiaohua wang, yuliang yan, longtao huang, xiaoqing zheng, and xuan-jing huang. halluci-\n",
      "nation detection for generative large language models by bayesian sequential estimation. in\n",
      "proceedings of the 2023 conference on empirical methods in natural language processing,\n",
      "pages 15361–15371, 2023.\n",
      "[17] liang wang, nan yang, and furu wei. query2doc: query expansion with large language\n",
      "models. arxiv preprint arxiv:2303.07678, 2023.\n",
      "[18] gangwoo kim, sungdong kim, byeongguk jeon, joonsuk park, and jaewoo kang. tree of\n",
      "clarifications: answering ambiguous questions with retrieval-augmented large language models.\n",
      "arxiv preprint arxiv:2310.14696, 2023.\n",
      "[19] jerry liu. llamaindex, 11 2022. url https://github.com/jerryjliu/llama_index.\n",
      "[20] peitian zhang, shitao xiao, zheng liu, zhicheng dou, and jian-yun nie. retrieve anything to\n",
      "augment large language models. arxiv preprint arxiv:2310.07554, 2023.\n",
      "[21] zehan li, xin zhang, yanzhao zhang, dingkun long, pengjun xie, and meishan zhang.\n",
      "towards general text embeddings with multi-stage contrastive learning.\n",
      "arxiv preprint\n",
      "arxiv:2308.03281, 2023.\n",
      "[22] huiqiang jiang, qianhui wu, chin-yew lin, yuqing yang, and lili qiu.\n",
      "llmlingua:\n",
      "compressing prompts for accelerated inference of large language models. arxiv preprint\n",
      "arxiv:2310.05736, 2023.\n",
      "[23] fangyuan xu, weijia shi, and eunsol choi. recomp: improving retrieval-augmented lms with\n",
      "compression and selective augmentation. arxiv preprint arxiv:2310.04408, 2023.\n",
      "[24] zhiruo wang, jun araki, zhengbao jiang, md rizwan parvez, and graham neubig. learning\n",
      "to filter context for retrieval-augmented generation. arxiv preprint arxiv:2311.08377, 2023.\n",
      "[25] rodrigo nogueira, wei yang, kyunghyun cho, and jimmy lin. multi-stage document ranking\n",
      "with bert. arxiv preprint arxiv:1910.14424, 2019.\n",
      "[26] rodrigo nogueira, zhiying jiang, and jimmy lin.\n",
      "document ranking with a pretrained\n",
      "sequence-to-sequence model. arxiv preprint arxiv:2003.06713, 2020.\n",
      "[27] xueguang ma, liang wang, nan yang, furu wei, and jimmy lin. fine-tuning llama for\n",
      "multi-stage text retrieval. arxiv preprint arxiv:2310.08319, 2023.\n",
      "[28] shengyao zhuang and guido zuccon. tilde: term independent likelihood model for passage\n",
      "re-ranking. in proceedings of the 44th international acm sigir conference on research and\n",
      "development in information retrieval, pages 1483–1492, 2021.\n",
      "[29] shengyao zhuang and guido zuccon. fast passage re-ranking with contextualized exact term\n",
      "matching and efficient passage expansion. arxiv preprint arxiv:2108.08513, 2021.\n",
      "[30] hongyin luo, yung-sung chuang, yuan gong, tianhua zhang, yoon kim, xixin wu, danny\n",
      "fox, helen m. meng, and james r. glass. sail: search-augmented instruction learning.\n",
      "in conference on empirical methods in natural language processing, 2023. url https:\n",
      "//api.semanticscholar.org/corpusid:258865283.\n",
      "15\n",
      "\n",
      "[31] tianjun zhang, shishir g. patil, naman jain, sheng shen, matei a. zaharia, ion stoica,\n",
      "and joseph e. gonzalez. raft: adapting language model to domain specific rag. arxiv,\n",
      "abs/2403.10131, 2024.\n",
      "[32] zihan liu, wei ping, rajarshi roy, peng xu, chankyu lee, mohammad shoeybi, and bryan\n",
      "catanzaro. chatqa: surpassing gpt-4 on conversational qa and rag. 2024. url https:\n",
      "//api.semanticscholar.org/corpusid:267035133.\n",
      "[33] gautier izacard, patrick lewis, maria lomeli, lucas hosseini, fabio petroni, timo schick,\n",
      "jane a. yu, armand joulin, sebastian riedel, and edouard grave. few-shot learning with\n",
      "retrieval augmented language models. arxiv, abs/2208.03299, 2022.\n",
      "[34] lingxi zhang, yue yu, kuan wang, and chao zhang. arl2: aligning retrievers for black-box\n",
      "large language models via self-guided adaptive relevance labeling. arxiv, abs/2402.13542,\n",
      "2024.\n",
      "[35] weijia shi, sewon min, michihiro yasunaga, minjoon seo, rich james, mike lewis, luke\n",
      "zettlemoyer, and wen-tau yih. replug: retrieval-augmented black-box language models. arxiv\n",
      "preprint arxiv:2301.12652, 2023.\n",
      "[36] kelvin guu, kenton lee, zora tung, panupong pasupat, and ming-wei chang.\n",
      "realm:\n",
      "retrieval-augmented language model pre-training. arxiv, abs/2002.08909, 2020.\n",
      "[37] xi victoria lin, xilun chen, mingda chen, weijia shi, maria lomeli, rich james, pedro\n",
      "rodriguez, jacob kahn, gergely szilvasy, mike lewis, luke zettlemoyer, and scott yih.\n",
      "ra-dit: retrieval-augmented dual instruction tuning. arxiv, abs/2310.01352, 2023.\n",
      "[38] hamed zamani and michael bendersky. stochastic rag: end-to-end retrieval-augmented gen-\n",
      "eration through expected utility maximization. 2024. url https://api.semanticscholar.\n",
      "org/corpusid:269605438.\n",
      "[39] yizheng huang and jimmy huang. a survey on retrieval-augmented text generation for large\n",
      "language models. arxiv preprint arxiv:2404.10981, 2024.\n",
      "[40] ruochen zhao, hailin chen, weishi wang, fangkai jiao, xuan long do, chengwei qin,\n",
      "bosheng ding, xiaobao guo, minzhi li, xingxuan li, et al. retrieving multimodal information\n",
      "for augmented generation: a survey. arxiv preprint arxiv:2303.10868, 2023.\n",
      "[41] penghao zhao, hailin zhang, qinhan yu, zhengren wang, yunteng geng, fangcheng fu, ling\n",
      "yang, wentao zhang, and bin cui. retrieval-augmented generation for ai-generated content: a\n",
      "survey. arxiv preprint arxiv:2402.19473, 2024.\n",
      "[42] michael günther, jackmin ong, isabelle mohr, alaeddine abdessalem, tanguy abel, moham-\n",
      "mad kalim akram, susana guzman, georgios mastrapas, saba sturua, bo wang, et al. jina\n",
      "embeddings 2: 8192-token general-purpose text embeddings for long documents. arxiv preprint\n",
      "arxiv:2310.19923, 2023.\n",
      "[43] llamaindex. llamaindex website. https://www.llamaindex.com. accessed: 2024-06-08.\n",
      "[44] kunal sawarkar, abhilasha mangal, and shivam raj solanki. blended rag: improving rag\n",
      "(retriever-augmented generation) accuracy with semantic search and hybrid query-based retriev-\n",
      "ers. arxiv preprint arxiv:2404.07220, 2024.\n",
      "[45] gautier izacard, mathilde caron, lucas hosseini, sebastian riedel, piotr bojanowski, armand\n",
      "joulin, and edouard grave. unsupervised dense information retrieval with contrastive learning.\n",
      "arxiv preprint arxiv:2112.09118, 2021.\n",
      "[46] nandan thakur, nils reimers, andreas rücklé, abhishek srivastava, and iryna gurevych. beir:\n",
      "a heterogenous benchmark for zero-shot evaluation of information retrieval models. arxiv\n",
      "preprint arxiv:2104.08663, 2021.\n",
      "[47] payal bajaj, daniel campos, nick craswell, li deng, jianfeng gao, xiaodong liu, rangan\n",
      "majumder, andrew mcnamara, bhaskar mitra, tri nguyen, et al. ms marco: a human\n",
      "generated machine reading comprehension dataset. arxiv preprint arxiv:1611.09268, 2016.\n",
      "16\n",
      "\n",
      "[48] nelson f liu, kevin lin, john hewitt, ashwin paranjape, michele bevilacqua, fabio petroni,\n",
      "and percy liang. lost in the middle: how language models use long contexts. transactions of\n",
      "the association for computational linguistics, 12:157–173, 2024.\n",
      "[49] huiqiang jiang, qianhui wu, xufang luo, dongsheng li, chin-yew lin, yuqing yang, and\n",
      "lili qiu. longllmlingua: accelerating and enhancing llms in long context scenarios via prompt\n",
      "compression. arxiv preprint arxiv:2310.06839, 2023.\n",
      "[50] hugo touvron, louis martin, kevin r. stone, peter albert, amjad almahairi, yasmine babaei,\n",
      "nikolay bashlykov, soumya batra, prajjwal bhargava, shruti bhosale, daniel m. bikel, lukas\n",
      "blecher, cristian cantón ferrer, moya chen, guillem cucurull, david esiobu, jude fernandes,\n",
      "jeremy fu, wenyin fu, brian fuller, cynthia gao, vedanuj goswami, naman goyal, anthony s.\n",
      "hartshorn, saghar hosseini, rui hou, hakan inan, marcin kardas, viktor kerkez, madian\n",
      "khabsa, isabel m. kloumann, a. v. korenev, punit singh koura, marie-anne lachaux, thibaut\n",
      "lavril, jenya lee, diana liskovich, yinghai lu, yuning mao, xavier martinet, todor mihaylov,\n",
      "pushkar mishra, igor molybog, yixin nie, andrew poulton, jeremy reizenstein, rashi rungta,\n",
      "kalyan saladi, alan schelten, ruan silva, eric michael smith, r. subramanian, xia tan, binh\n",
      "tang, ross taylor, adina williams, jian xiang kuan, puxin xu, zhengxu yan, iliyan zarov,\n",
      "yuchen zhang, angela fan, melanie kambadur, sharan narang, aurelien rodriguez, robert\n",
      "stojnic, sergey edunov, and thomas scialom. llama 2: open foundation and fine-tuned chat\n",
      "models. arxiv, abs/2307.09288, 2023.\n",
      "[51] es shahul, jithin james, luis espinosa anke, and steven schockaert. ragas: automated\n",
      "evaluation of retrieval augmented generation. in conference of the european chapter of the\n",
      "association for computational linguistics, 2023. url https://api.semanticscholar.org/\n",
      "corpusid:263152733.\n",
      "[52] harsh trivedi, niranjan balasubramanian, tushar khot, and ashish sabharwal. musique:\n",
      "multihop questions via single-hop question composition. transactions of the association\n",
      "for computational linguistics, page 539–554, may 2022. doi: 10.1162/tacl_a_00475. url\n",
      "http://dx.doi.org/10.1162/tacl_a_00475.\n",
      "[53] mike conover, matt hayes, ankit mathur, jianwei xie, jun wan, sam shah, ali ghodsi,\n",
      "patrick wendell, matei zaharia, and reynold xin. free dolly: introducing the world’s first truly\n",
      "open instruction-tuned llm, 2023. url https://www.databricks.com/blog/2023/04/12/\n",
      "dolly-first-open-commercially-viable-instruction-tuned-llm.\n",
      "[54] nick craswell, bhaskar mitra, emine yilmaz, daniel fernando campos, and ellen m. voorhees.\n",
      "overview of the trec 2019 deep learning track. arxiv, abs/2003.07820, 2020. url https:\n",
      "//api.semanticscholar.org/corpusid:253234683.\n",
      "[55] nick craswell, bhaskar mitra, emine yilmaz, daniel fernando campos, and ellen m. voorhees.\n",
      "overview of the trec 2020 deep learning track. arxiv, abs/2102.07662, 2021. url https:\n",
      "//api.semanticscholar.org/corpusid:212737158.\n",
      "[56] jimmy lin, xueguang ma, sheng-chieh lin, jheng-hong yang, ronak pradeep, and rodrigo\n",
      "nogueira. pyserini: a python toolkit for reproducible information retrieval research with sparse\n",
      "and dense representations. in proceedings of the 44th international acm sigir conference on\n",
      "research and development in information retrieval, pages 2356–2362, 2021.\n",
      "[57] tom kwiatkowski, jennimaria palomaki, olivia redfield, michael collins, ankur p. parikh,\n",
      "chris alberti, danielle epstein, illia polosukhin, jacob devlin, kenton lee, kristina toutanova,\n",
      "llion jones, matthew kelcey, ming-wei chang, andrew m. dai, jakob uszkoreit, quoc v. le,\n",
      "and slav petrov. natural questions: a benchmark for question answering research. transactions\n",
      "of the association for computational linguistics, 7:453–466, 2019.\n",
      "[58] mandar joshi, eunsol choi, daniel s. weld, and luke zettlemoyer. triviaqa: a large scale\n",
      "distantly supervised challenge dataset for reading comprehension. arxiv, abs/1705.03551,\n",
      "2017.\n",
      "[59] zhilin yang, peng qi, saizheng zhang, yoshua bengio, william w cohen, ruslan salakhut-\n",
      "dinov, and christopher d manning. hotpotqa: a dataset for diverse, explainable multi-hop\n",
      "question answering. arxiv preprint arxiv:1809.09600, 2018.\n",
      "17\n",
      "\n",
      "[60] ivan stelmakh, yi luan, bhuwan dhingra, and ming-wei chang. asqa: factoid questions meet\n",
      "long-form answers. arxiv, abs/2204.06092, 2022.\n",
      "[61] tomáš koˇ\n",
      "cisk`\n",
      "y, jonathan schwarz, phil blunsom, chris dyer, karl moritz hermann, gábor\n",
      "melis, and edward grefenstette. the narrativeqa reading comprehension challenge. transac-\n",
      "tions of the association for computational linguistics, 6:317–328, 2018.\n",
      "[62] pranav rajpurkar, jian zhang, konstantin lopyrev, and percy liang. squad: 100,000+ questions\n",
      "for machine comprehension of text. arxiv preprint arxiv:1606.05250, 2016.\n",
      "[63] stephanie lin, jacob hilton, and owain evans. truthfulqa: measuring how models mimic\n",
      "human falsehoods. arxiv preprint arxiv:2109.07958, 2021.\n",
      "[64] j. edward hu, yelong shen, phillip wallis, zeyuan allen-zhu, yuanzhi li, shean wang, and\n",
      "weizhu chen. lora: low-rank adaptation of large language models. arxiv, abs/2106.09685,\n",
      "2021.\n",
      "[65] dan hendrycks, collin burns, steven basart, andy zou, mantas mazeika, dawn song, and\n",
      "jacob steinhardt. measuring massive multitask language understanding. cornell university -\n",
      "arxiv,cornell university - arxiv, sep 2020.\n",
      "[66] peter clark, isaac cowhey, oren etzioni, tushar khot, ashish sabharwal, carissa schoenick,\n",
      "and oyvind tafjord. think you have solved question answering? try arc, the ai2 reasoning chal-\n",
      "lenge. arxiv, abs/1803.05457, 2018. url https://api.semanticscholar.org/corpusid:\n",
      "3922816.\n",
      "[67] todor mihaylov, peter clark, tushar khot, and ashish sabharwal.\n",
      "can a suit of armor\n",
      "conduct electricity? a new dataset for open book question answering. in proceedings of\n",
      "the 2018 conference on empirical methods in natural language processing, jan 2018. doi:\n",
      "10.18653/v1/d18-1260. url http://dx.doi.org/10.18653/v1/d18-1260.\n",
      "[68] james thorne, andreas vlachos, christos christodoulopoulos, and arpit mittal. fever: a\n",
      "large-scale dataset for fact extraction and verification. arxiv, abs/1803.05355, 2018. url\n",
      "https://api.semanticscholar.org/corpusid:4711425.\n",
      "[69] tianhua zhang, hongyin luo, yung-sung chuang, wei fang, luc gaitskell, thomas\n",
      "hartvigsen, xixin wu, danny fox, helen m. meng, and james r. glass. interpretable unified\n",
      "language checking. arxiv, abs/2304.03728, 2023. url https://api.semanticscholar.\n",
      "org/corpusid:258041307.\n",
      "[70] jonathan berant, andrew chou, roy frostig, and percy liang. semantic parsing on freebase\n",
      "from question-answer pairs. empirical methods in natural language processing,empirical\n",
      "methods in natural language processing, oct 2013.\n",
      "[71] xanh ho, a. nguyen, saku sugawara, and akiko aizawa. constructing a multi-hop qa\n",
      "dataset for comprehensive evaluation of reasoning steps. arxiv, abs/2011.01060, 2020. url\n",
      "https://api.semanticscholar.org/corpusid:226236740.\n",
      "[72] ofir press, muru zhang, sewon min, ludwig schmidt, noaha. smith, and mike lewis.\n",
      "measuring and narrowing the compositionality gap in language models. oct 2022.\n",
      "[73] qiao jin, bhuwan dhingra, zhengping liu, william w. cohen, and xinghua lu. pubmedqa: a\n",
      "dataset for biomedical research question answering. in conference on empirical methods in\n",
      "natural language processing, 2019. url https://api.semanticscholar.org/corpusid:\n",
      "202572622.\n",
      "[74] akari asai, zeqiu wu, yizhong wang, avirup sil, and hannaneh hajishirzi. self-rag: learning\n",
      "to retrieve, generate, and critique through self-reflection. arxiv preprint arxiv:2310.11511,\n",
      "2023.\n",
      "18\n",
      "\n",
      "a\n",
      "experimental details\n",
      "in this section, we provide detailed experimental settings for each module, covering dataset specifics,\n",
      "training parameters, and any additional experimental results.\n",
      "a.1\n",
      "query classification\n",
      "datasets\n",
      "we utilized a subset of the databricks-dolly-15k [53] and generated additional data\n",
      "using gpt-4.the prompt template for generating questions is shown in table 14.\n",
      "implementation details\n",
      "we choose bert-base-multilingual-cased as our classifier, with a batch\n",
      "size of 16 and a learning rate of 1e-5. the evaluation of results is showcased in table 1.\n",
      "a.2\n",
      "experimental details of retrieval methods\n",
      "implementation details of the comparative experiments of different retrieval methods are as below:\n",
      "datasets\n",
      "we use the trec dl 2019 [54] and 2020 [55] passage ranking datasets to evaluate the\n",
      "performance of different retrieval methods.\n",
      "metrics\n",
      "widely-used evaluation metrics for retrieval include map, ndcg@10, r@50 and r@1k.\n",
      "both map and ndcg@10 are order-aware metrics that take the ranking of search results into account.\n",
      "in contrast, r@k is an order-unaware metric. we also report the average latency incurred by each\n",
      "method per query.\n",
      "implementation details\n",
      "for sparse retrieval, we use the bm25 algorithm, which relies on the tf-\n",
      "idf algorithm. for dense retrieval, we employ contriever as our unsupervised contrastive text encoder.\n",
      "based on our evaluation of embedding models, we implement our supervised dense retrieval using\n",
      "llm-embedder. we use the default implementation of bm25 and contriever from pyserini [56].\n",
      "the bm25 index is constructed using lucene on ms marco collections, while the dense vector\n",
      "index is generated with faiss employing flat configuration on the same dataset. for query rewriting,\n",
      "we prompt zephyr-7b-alpha9, a model trained to act as a helpful assistant, to rewrite the original\n",
      "query. for query decomposition, we employ gpt-3.5-turbo-0125 to break down the original query\n",
      "into multiple sub-queries. we closely follow the implementation from hyde [10], utilizing the more\n",
      "advanced instruction-following language model, gpt-3.5-turbo-instruct, to generate hypothetical\n",
      "answers. the model infers with a default temperature of 0.7, sampling up to a maximum of 512\n",
      "tokens. retrieval experiments and evaluation are conducted using the pyserini toolkit.\n",
      "a.3\n",
      "experimental details of reranking methods\n",
      "datasets\n",
      "our experiments utilize the ms marco passage ranking dataset, a substantial corpus\n",
      "designed for machine reading comprehension tasks. this dataset comprises over 8.8 million passages\n",
      "and 1 million queries. the training set contains approximately 398m tuples of queries paired with\n",
      "corresponding positive and negative passages, while the development set comprises 6,980 queries,\n",
      "paired with their bm25 retrieval results, and preserves the top-1000 ranked candidate passages for\n",
      "each query. we evaluate the effectiveness of the methods on the development set, as the test set is not\n",
      "publicly available.\n",
      "metrics\n",
      "the evaluation metrics mrr@1, mrr@10, mrr@1k and hit rate@10 are used.\n",
      "mrr@10 is the official metric proposed by ms marco.\n",
      "implementation details\n",
      "we follow and make modifications to the implementation provided by\n",
      "pygaggle [26] and tilde [28]. for dlm-based reranking, we use monot5 [26] based on t5-base,\n",
      "monobert [25] based on bert-large and rankllama [27] based on llama-2-7b. for tilde\n",
      "reranking, we use tildev2 [29] based on bert-base.\n",
      "typically, 50 documents are retrieved as input for the reranking module. the documents remaining\n",
      "after the reranking and repacking phase can be further concentrated by assigning a top-k value or a\n",
      "relevancy score threshold.\n",
      "result analysis\n",
      "reranking results are shown in table 9. we compare our results with a randomly\n",
      "shuffled ordering and the bm25 retrieval baseline. all reranking methods demonstrate a notable\n",
      "9https://huggingface.co/huggingfaceh4/zephyr-7b-alpha\n",
      "19\n",
      "\n",
      "context\n",
      "model\n",
      "nq\n",
      "triviaqa\n",
      "hotpotqa\n",
      "asqa\n",
      "avg.\n",
      "d∅\n",
      "mb\n",
      "29.78\n",
      "60.44\n",
      "23.73\n",
      "37.89\n",
      "37.96\n",
      "mg\n",
      "26.23\n",
      "58.26\n",
      "26.67\n",
      "32.30\n",
      "35.87\n",
      "mr\n",
      "31.10\n",
      "61.37\n",
      "28.40\n",
      "39.96\n",
      "40.21\n",
      "mgr\n",
      "25.92\n",
      "57.62\n",
      "26.43\n",
      "32.99\n",
      "35.70\n",
      "mgg\n",
      "26.69\n",
      "58.07\n",
      "27.04\n",
      "33.75\n",
      "36.39\n",
      "dg\n",
      "mb\n",
      "44.78\n",
      "79.90\n",
      "56.72\n",
      "71.64\n",
      "63.26\n",
      "mg\n",
      "85.72\n",
      "88.16\n",
      "79.82\n",
      "85.51\n",
      "84.80\n",
      "mr\n",
      "60.98\n",
      "80.20\n",
      "65.73\n",
      "67.49\n",
      "68.60\n",
      "mgr\n",
      "87.60\n",
      "87.94\n",
      "81.07\n",
      "87.58\n",
      "86.05\n",
      "mgg\n",
      "86.72\n",
      "88.35\n",
      "79.59\n",
      "83.44\n",
      "84.53\n",
      "dr\n",
      "mb\n",
      "16.49\n",
      "50.03\n",
      "21.57\n",
      "28.79\n",
      "29.22\n",
      "mg\n",
      "22.15\n",
      "46.98\n",
      "24.36\n",
      "29.40\n",
      "30.72\n",
      "mr\n",
      "36.92\n",
      "58.42\n",
      "29.64\n",
      "39.54\n",
      "41.13\n",
      "mgr\n",
      "23.63\n",
      "45.01\n",
      "24.17\n",
      "27.95\n",
      "30.19\n",
      "mgg\n",
      "21.08\n",
      "43.83\n",
      "23.23\n",
      "27.33\n",
      "28.87\n",
      "dgr\n",
      "mb\n",
      "34.65\n",
      "81.27\n",
      "52.75\n",
      "65.42\n",
      "58.52\n",
      "mg\n",
      "85.00\n",
      "87.33\n",
      "78.18\n",
      "83.02\n",
      "83.38\n",
      "mr\n",
      "60.28\n",
      "79.32\n",
      "63.82\n",
      "67.29\n",
      "67.68\n",
      "mgr\n",
      "87.63\n",
      "87.14\n",
      "79.95\n",
      "87.78\n",
      "85.63\n",
      "mgg\n",
      "86.31\n",
      "86.90\n",
      "78.10\n",
      "83.85\n",
      "83.79\n",
      "table 12: results of the model augmented with different contexts on various qa datasets.\n",
      "increase in performance across all metrics. approximately equal performance is achieved by monot5\n",
      "and monobert, and rankllama performs best, each ascending in latency. tildev2 is the fastest,\n",
      "taking approximately 10 to 20 milliseconds per query at the cost of performance. additionally,\n",
      "tildev2 requires that the passages reranked be identically included in the previously indexed\n",
      "collection. preprocessing must be redone at inference for new unseen passages, negating the efficiency\n",
      "advantages.\n",
      "a.4\n",
      "experimental details of summarization methods\n",
      "selective context\n",
      "selective context enhances llm efficiency by identifying and removing\n",
      "redundant information in the input context. it evaluates the informativeness of lexical units using\n",
      "self-information computed by a base causal language model. this method is non-query-based,\n",
      "allowing a comparison between query-based and non-query-based approaches.\n",
      "datasets\n",
      "we evaluated these methods on three datasets: natural questions (nq) [57], trivi-\n",
      "aqa [58], and hotpotqa [59].\n",
      "metrics\n",
      "evaluation metrics include the f1 score and the number of tokens changed after summa-\n",
      "rization to measure conciseness.\n",
      "implementation details\n",
      "for all methods, we use llama3-8b-instruct as the generator model\n",
      "and set a summarization ratio of 0.4. for extractive methods, importance scores determine the\n",
      "sentences retained. for abstractive methods, we control the maximum generation length using the\n",
      "summarization ratio to align with extractive methods. experiments are conducted on the nq test set,\n",
      "triviaqa test set, and hotpotqa development set.\n",
      "a.5\n",
      "experimental details of generator fine-tuning\n",
      "datasets\n",
      "we fine-tune our model on several question answering(qa) and reading comprehension\n",
      "datasets, including asqa [60], hotpotqa [59], narrativeqa [61], nq [57], squad [62], trivi-\n",
      "aqa [58], truthfulqa [63]. we use their train splits (for those containing significantly more data\n",
      "20\n",
      "\n",
      "[instruction]\n",
      "please generate ten descriptions for the continuation task.\n",
      "[context]\n",
      "for example:\n",
      "1.“french.washington played a crucial role in the american revolutionary war, leading\n",
      "the continental army against the british.” please continue writing the above paragraph.\n",
      "2.“the discovery of the double helix structure of dna by james watson and francis\n",
      "crick revolutionized the field of genetics, laying the foundation for modern molecular\n",
      "biology and biotechnology.” please continue by discussing recent developments in\n",
      "genetic research, such as crispr gene editing, and their potential ethical implications.\n",
      "table 14: template for generating task classification data.\n",
      "entries than others, we conducted a random sample). for evaluation, asqa [60], hotpotqa [59],\n",
      "nq [57], triviaqa [58] are used. we evaluate our model on their validation splits or manually split a\n",
      "dataset\n",
      "#train\n",
      "#eval\n",
      "asqa\n",
      "2, 090\n",
      "483\n",
      "hotpotqa\n",
      "15, 000\n",
      "7, 405\n",
      "triviaqa\n",
      "9, 000\n",
      "6, 368\n",
      "nq\n",
      "15, 000\n",
      "8, 006\n",
      "narrativeqa\n",
      "7, 000\n",
      "−−\n",
      "squad\n",
      "67, 00\n",
      "−−\n",
      "truthfulqa\n",
      "817\n",
      "−−\n",
      "table 13: number of examples in each dataset\n",
      "used in the fine-tuning experiments.\n",
      "subset from the training set to avoid overlapping.\n",
      "the exact number of entries in each train and\n",
      "test set is detailed in table 13.\n",
      "we use the dataset-provided documents as dgold\n",
      "for each data entry. to obtain drandom we sam-\n",
      "ple the context of different entries within the\n",
      "same dataset, to make sure the distributions of\n",
      "drandom and dgold are roughly similar.\n",
      "metrics\n",
      "we use the ground-truth coverage\n",
      "as our evaluation metric, considering that the\n",
      "answers of qa tasks are relatively short, while\n",
      "the generation length of the model is sometimes hard to limit.\n",
      "implementation details\n",
      "we select llama-2-7b [50] as the base model. for efficiency, we use\n",
      "lora [64] and int8 quantization during training. the prompt templates used for fine-tuning and\n",
      "evaluation mainly follow lin et al. [37]. we train our generator for 3 epochs and constrain the\n",
      "maximum length of the sequence to 1600, using a batch size of 4 and a learning rate of 5e-5. during\n",
      "testing, we use a zero-shot setting.\n",
      "detailed results\n",
      "table 12 shows our evaluation results on each dataset.\n",
      "a.6\n",
      "experimental details of comprehensive evaluation\n",
      "tasks and datasets\n",
      "we conducted extensive experiments across various nlp tasks and datasets to\n",
      "assess the performance of rag systems. specifically: (1) commonsense reasoning: we evaluated\n",
      "on mmlu [65], arc-challenge [66], and openbookqa [67] datasets. (2) fact checking: our\n",
      "evaluation encompassed the fever [68] and pubhealth [69] datasets. (3) open-domain qa:\n",
      "we assessed on nq [57], triviaqa [58], and webquestions [70] datasets. (4) multihop qa:\n",
      "our evaluation included the hotpotqa [59], 2wikimultihopqa [71], and musique [52] datasets.\n",
      "for musique, we followed the approach outlined in [72] and focused solely on answerable 2-hop\n",
      "questions. (5) medical qa: we also assessed on the pubmedqa [73] dataset. in each dataset, we\n",
      "randomly sub-sample 500 entries from the test set for our experiments. for datasets without test set,\n",
      "we use develop set instead.\n",
      "to assess rag capabilities, we evenly collect a total of 500 entries from nq, triviaqa, hotpotqa,\n",
      "2wikimultihopqa and musique. each entry is a “question, gold document, gold answer” triple.\n",
      "metrics\n",
      "we use token-level f1 score and em score for open-domain qa and multihop qa tasks,\n",
      "and accuracy for others. we use a more lenient em score, which evaluates performance based on\n",
      "whether the model generations include gold answers instead of strictly exact matching [74].\n",
      "towards rag capabilities evaluation, we adopt four metrics from ragas, including faithfulness,\n",
      "context relevancy, answer relevancy, and answer correctness. faithfulness measures how\n",
      "factually consistent the generated answer is with the retrieved context. an answer is considered\n",
      "faithful if all claims made can be directly inferred from the provided context. context relevancy\n",
      "evaluates how relevant the retrieved context is to the original query. answer relevancy assesses the\n",
      "21\n",
      "\n",
      "pertinence of the generated answer to the original query. answer correctness involves the accuracy\n",
      "of the generated answer when compared to the ground truth. for example, context relevancy is\n",
      "calculated from the proportion of sentences within the retrieved context that are relevant for answering\n",
      "the given question to all sentences:\n",
      "context relevancy =\n",
      "|s|\n",
      "|total|\n",
      "(2)\n",
      "where |s| denotes the number of relevant sentences, |total| denotes the total number of sentences\n",
      "retrieved. all these metrics are evaluated using the ragas framework, with gpt-4 serving as the\n",
      "judge.\n",
      "additionally, we compute the cosine similarity between the retrieved document and the gold document\n",
      "as retrieval similarity. the retrieved document and gold document are fed into an embedding\n",
      "model, then the resulting embeddings are used to compute the cosine similarity.\n",
      "implementation details\n",
      "for open-domain qa and multihop qa datasets, we set the generation\n",
      "model’s maximum new token number to 100 tokens. for other datasets, we set it to 50 tokens. to\n",
      "deal with excessively long retrieved documents, we truncated the documents to 2048 words when\n",
      "evaluating rankllama and longllmlingua.\n",
      "for all datasets, we use greedy decoding during generation. to better compare the capabilities of\n",
      "different rag modules, we adopt the 0-shot evaluation setting, i.e., no in-context examples are\n",
      "offered. in the multiple choice and fact checking tasks, answers generated by the model may take\n",
      "a variety of forms (e.g., “the answer is a” instead of “a”). therefore, we preprocess the responses\n",
      "generated by the model, applying regular expression templates to match them with gold labels.\n",
      "22\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in documents:\n",
    "    doc.page_content = doc.page_content.lower()\n",
    "\n",
    "for doc in documents:\n",
    "    print(doc.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,  \n",
    "    chunk_overlap=100 \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "searching for best practices in retrieval-augmented\n",
      "generation\n",
      "xiaohua wang, zhenghua wang, xuan gao, feiran zhang,\n",
      "yixin wu, zhibo xu, tianyuan shi, zhengyuan wang, shizheng li,\n",
      "qi qian, ruicheng yin, changze lv, xiaoqing zheng∗\n",
      ", xuanjing huang\n",
      "school of computer science, fudan university, shanghai, china\n",
      "shanghai key laboratory of intelligent information processing\n",
      "{xiaohuawang22,zhenghuawang23}@m.fudan.edu.cn\n",
      "{zhengxq,xjhuang}@fudan.edu.cn\n",
      "abstract\n",
      "{xiaohuawang22,zhenghuawang23}@m.fudan.edu.cn\n",
      "{zhengxq,xjhuang}@fudan.edu.cn\n",
      "abstract\n",
      "retrieval-augmented generation (rag) techniques have proven to be effective\n",
      "in integrating up-to-date information, mitigating hallucinations, and enhancing\n",
      "response quality, particularly in specialized domains. while many rag approaches\n",
      "have been proposed to enhance large language models through query-dependent\n",
      "retrievals, these approaches still suffer from their complex implementation and\n",
      "retrievals, these approaches still suffer from their complex implementation and\n",
      "prolonged response times. typically, a rag workflow involves multiple processing\n",
      "steps, each of which can be executed in various ways. here, we investigate\n",
      "existing rag approaches and their potential combinations to identify optimal\n",
      "rag practices. through extensive experiments, we suggest several strategies\n",
      "for deploying rag that balance both performance and efficiency. moreover,\n",
      "for deploying rag that balance both performance and efficiency. moreover,\n",
      "we demonstrate that multimodal retrieval techniques can significantly enhance\n",
      "question-answering capabilities about visual inputs and accelerate the generation\n",
      "of multimodal content using a “retrieval as generation” strategy. resources are\n",
      "available at https://github.com/fudandnn-nlp/rag.\n",
      "1\n",
      "introduction\n",
      "generative large language models are prone to producing outdated information or fabricating facts,\n",
      "generative large language models are prone to producing outdated information or fabricating facts,\n",
      "although they were aligned with human preferences by reinforcement learning [1] or lightweight\n",
      "alternatives [2–5]. retrieval-augmented generation (rag) techniques address these issues by com-\n",
      "bining the strengths of pretraining and retrieval-based models, thereby providing a robust framework\n",
      "for enhancing model performance [6]. furthermore, rag enables rapid deployment of applications\n",
      "for enhancing model performance [6]. furthermore, rag enables rapid deployment of applications\n",
      "for specific organizations and domains without necessitating updates to the model parameters, as\n",
      "long as query-related documents are provided.\n",
      "many rag approaches have been proposed to enhance large language models (llms) through\n",
      "query-dependent retrievals [6–8]. a typical rag workflow usually contains multiple intervening\n",
      "query-dependent retrievals [6–8]. a typical rag workflow usually contains multiple intervening\n",
      "processing steps: query classification (determining whether retrieval is necessary for a given input\n",
      "query), retrieval (efficiently obtaining relevant documents for the query), reranking (refining the\n",
      "order of retrieved documents based on their relevance to the query), repacking (organizing the\n",
      "retrieved documents into a structured one for better generation), summarization (extracting key\n",
      "retrieved documents into a structured one for better generation), summarization (extracting key\n",
      "information for response generation from the repacked document and eliminating redundancies)\n",
      "modules. implementing rag also requires decisions on the ways to properly split documents into\n",
      "chunks, the types of embeddings to use for semantically representing these chunks, the choice of\n",
      "∗corresponding author.\n",
      "preprint. under review.\n",
      "arxiv:2407.01219v1  [cs.cl]  1 jul 2024\n",
      "large language model\n",
      "query classification\n",
      "reranking\n",
      "dlm-based\n",
      "monot5\n",
      "monobert\n",
      "rankllama\n",
      "tilde\n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "retrieval\n",
      "original query\n",
      "bm25\n",
      "contriever\n",
      "llm-embedder\n",
      "query rewriting\n",
      "query decomposition\n",
      "hyde\n",
      "hybrid search\n",
      "hyde+hybrid search\n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      "   \n",
      "summarization\n",
      "extractive\n",
      "recomp\n",
      "bm25\n",
      "contriever\n",
      "abstractive\n",
      "longllmlingua\n",
      "selectivecontext\n",
      "recomp\n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "vector database\n",
      "milvus\n",
      "faiss\n",
      "weaviate\n",
      "qdrant\n",
      "chroma\n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "chunking\n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "vector database\n",
      "milvus\n",
      "faiss\n",
      "weaviate\n",
      "qdrant\n",
      "chroma\n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "chunking\n",
      "chunking size\n",
      "small2big\n",
      "sliding windows\n",
      "chunk metadata\n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      " \n",
      "embedding\n",
      "llm-embedder\n",
      "intfloat/e5\n",
      "baai/bge\n",
      "jina-embeddings-v2\n",
      "gte\n",
      "all-mpnet-base-v2\n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      "     \n",
      "repacking\n",
      "sides\n",
      "forward\n",
      "reverse\n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      "  \n",
      "evaluation\n",
      "general performance\n",
      "specific domains\n",
      "retrieval capability\n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      "  \n",
      "fine-tune\n",
      "disturb\n",
      "random\n",
      "normal\n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      "    \n",
      "retrieval source\n",
      "retrieval capability\n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      "  \n",
      "fine-tune\n",
      "disturb\n",
      "random\n",
      "normal\n",
      "•\n",
      " \n",
      "•\n",
      " \n",
      "•\n",
      "    \n",
      "retrieval source\n",
      "figure 1: retrieval-augmented generation workflow. this study investigates the contribution of\n",
      "each component and provides insights into optimal rag practices through extensive experimentation.\n",
      "the optional methods considered for each component are indicated in bold fonts, while the methods\n",
      "underlined indicate the default choice for individual modules. the methods indicated in blue font\n",
      "underlined indicate the default choice for individual modules. the methods indicated in blue font\n",
      "denote the best-performing selections identified empirically.\n",
      "vector databases to efficiently store feature representations, and the methods for effectively fine-tuning\n",
      "llms (see figure 1).\n",
      "what adds complexity and challenge is the variability in implementing each processing step. for\n",
      "example, in retrieving relevant documents for an input query, various methods can be employed.\n",
      "example, in retrieving relevant documents for an input query, various methods can be employed.\n",
      "one approach involves rewriting the query first and using the rewritten queries for retrieval [9].\n",
      "alternatively, pseudo-responses to the query can be generated first, and the similarity between\n",
      "these pseudo-responses and the backend documents can be compared for retrieval [10]. another\n",
      "option is to directly employ embedding models, typically trained in a contrastive manner using\n",
      "option is to directly employ embedding models, typically trained in a contrastive manner using\n",
      "positive and negative query-response pairs [11, 12]. the techniques chosen for each step and their\n",
      "combinations significantly impact both the effectiveness and efficiency of rag systems. to the best\n",
      "of our knowledge, there has been no systematic effort to pursue the optimal implementation of rag,\n",
      "particularly for the entire rag workflow.\n",
      "particularly for the entire rag workflow.\n",
      "in this study, we aim to identify the best practices for rag through extensive experimentation. given\n",
      "the infeasibility of testing all possible combinations of these methods, we adopt a three-step approach\n",
      "to identify optimal rag practices. first, we compare representative methods for each rag step (or\n",
      "module) and select up to three of the best-performing methods. next, we evaluate the impact of each\n",
      "module) and select up to three of the best-performing methods. next, we evaluate the impact of each\n",
      "method on the overall rag performance by testing one method at a time for an individual step, while\n",
      "keeping the other rag modules unchanged. this allows us to determine the most effective method\n",
      "for each step based on its contribution and interaction with other modules during response generation.\n",
      "once the best method is chosen for a module, it is used in subsequent experiments. finally, we\n",
      "once the best method is chosen for a module, it is used in subsequent experiments. finally, we\n",
      "empirically explore a few promising combinations suitable for different application scenarios where\n",
      "efficiency might be prioritized over performance, or vice versa. based on these findings, we suggest\n",
      "several strategies for deploying rag that balance both performance and efficiency.\n",
      "the contributions of this study are three-fold:\n",
      "the contributions of this study are three-fold:\n",
      "• through extensive experimentation, we thoroughly investigated existing rag approaches and their\n",
      "combinations to identify and recommend optimal rag practices.\n",
      "2\n",
      "• we introduce a comprehensive framework of evaluation metrics and corresponding datasets to\n",
      "comprehensively assess the performance of retrieval-augmented generation models, covering\n",
      "general, specialized (or domain-specific), and rag-related capabilities.\n",
      "• we demonstrate that the integration of multimodal retrieval techniques can substantially improve\n",
      "question-answering capabilities on visual inputs and speed up the generation of multimodal content\n",
      "question-answering capabilities on visual inputs and speed up the generation of multimodal content\n",
      "through a strategy of “retrieval as generation”.\n",
      "2\n",
      "related work\n",
      "ensuring the accuracy of responses generated by large language models (llms) such as chat-\n",
      "gpt [13] and llama [14] is essential. however, simply enlarging model size does not fundamentally\n",
      "address the issue of hallucinations [15, 16], especially in knowledge-intensive tasks and specialized\n",
      "domains. retrieval-augmented generation (rag) addresses these challenges by retrieving relevant\n",
      "documents from external knowledge bases, providing accurate, real-time, domain-specific context to\n",
      "llms [6]. previous works have optimized the rag pipeline through query and retrieval transfor-\n",
      "mations, enhancing retriever performance, and fine-tuning both the retriever and generator. these\n",
      "optimizations improve the interaction between input queries, retrieval mechanisms, and generation\n",
      "optimizations improve the interaction between input queries, retrieval mechanisms, and generation\n",
      "processes, ensuring the accuracy and relevance of responses.\n",
      "2.1\n",
      "query and retrieval transformation\n",
      "effective retrieval requires queries accurate, clear, and detailed. even when converted into embed-\n",
      "dings, semantic differences between queries and relevant documents can persist. previous works have\n",
      "explored methods to enhance query information through query transformation, thereby improving\n",
      "explored methods to enhance query information through query transformation, thereby improving\n",
      "retrieval performance. for instance, query2doc [17] and hyde [10] generate pseudo-documents\n",
      "from original queries to enhance retrieval, while toc [18] decomposes queries into subqueries,\n",
      "aggregating the retrieved content for final results.\n",
      "other studies have focused on transforming retrieval source documents. llamaindex [19] provides an\n",
      "other studies have focused on transforming retrieval source documents. llamaindex [19] provides an\n",
      "interface to generate pseudo-queries for retrieval documents, improving matching with real queries.\n",
      "some works employ contrastive learning to bring query and document embeddings closer in semantic\n",
      "space [12, 20, 21]. post-processing retrieved documents is another method to enhance generator\n",
      "output, with techniques like hierarchical prompt summarization [22] and using abstractive and\n",
      "output, with techniques like hierarchical prompt summarization [22] and using abstractive and\n",
      "extractive compressors [23] to reduce context length and remove redundancy [24].\n",
      "2.2\n",
      "retriever enhancement strategy\n",
      "document chunking and embedding methods significantly impact retrieval performance. common\n",
      "chunking strategies divide documents into chunks, but determining optimal chunk length can be\n",
      "challenging. small chunks may fragment sentences, while large chunks might include irrelevant\n",
      "challenging. small chunks may fragment sentences, while large chunks might include irrelevant\n",
      "context. llamaindex [19] optimizes the chunking method like small2big and sliding window.\n",
      "retrieved chunks can be irrelevant and numbers can be large, so reranking is necessary to filter\n",
      "irrelevant documents. a common reranking approach employs deep language models such as\n",
      "bert [25], t5 [26], or llama [27], which requires slow inference steps during reranking but grants\n",
      "bert [25], t5 [26], or llama [27], which requires slow inference steps during reranking but grants\n",
      "better performance. tilde [28, 29] achieves efficiency by precomputing and storing the likelihood\n",
      "of query terms, ranking documents based on their sum.\n",
      "2.3\n",
      "retriever and generator fine-tuning\n",
      "fine-tuning within the rag framework is crucial for optimizing both retrievers and generators. some\n",
      "research focuses on fine-tuning the generator to better utilize retriever context [30–32], ensuring\n",
      "research focuses on fine-tuning the generator to better utilize retriever context [30–32], ensuring\n",
      "faithful and robust generated content. others fine-tune the retriever to learn to retrieve beneficial\n",
      "passages for the generator [33–35]. holistic approaches treat rag as an integrated system, fine-tuning\n",
      "both retriever and generator together to enhance overall performance [36–38], despite increased\n",
      "complexity and integration challenges.\n",
      "complexity and integration challenges.\n",
      "several surveys have extensively discussed current rag systems, covering aspects like text genera-\n",
      "tion [7, 8], integration with llms [6, 39], multimodal [40], and ai-generated content [41]. while\n",
      "these surveys provide comprehensive overviews of existing rag methodologies, selecting the appro-\n",
      "3\n",
      "which city will the next world cup be held?                                         \n",
      " \n",
      " \n",
      "           < search >\n",
      "\"french.washington played a \n",
      "crucial role in the american \n",
      "revolutionary war, leading the \n",
      "continental army against the \n",
      "british. \"\n",
      "please continue writing the \n",
      "above paragraph.      \n",
      "              < continuation writing >\n",
      "background knowledge\n",
      "\"to be, or not to be, that is the \n",
      "question.\" \n",
      "please translate this sentence into \n",
      "french. \n",
      "      \n",
      "< translation >\n",
      "insufficient information\n",
      "please translate this sentence into \n",
      "french. \n",
      "      \n",
      "< translation >\n",
      "insufficient information\n",
      "sufficient information\n",
      "please give me a plan for holding a graduation party.              \n",
      " \n",
      " \n",
      "       < planning >\n",
      "if you're currently a computer science student and your \n",
      "computer system encounters a malfunction, what should \n",
      "you do?               \n",
      " \n",
      "      < role-play >\n",
      "write an article about the geography of europe, focusing \n",
      "on the changes in rainfall in the western part of the\n",
      "on the changes in rainfall in the western part of the \n",
      "country.                                    \n",
      "          < writing >\n",
      "no retrieval needed\n",
      "need to retrieval\n",
      "please find a novel that is as \n",
      "famous as \"one hundred years \n",
      "of solitude\".               < search >\n",
      "\"dave is attending his aunt's \n",
      "brother funeral today.\"\n",
      "paraphrase the given information \n",
      "effectively. \n",
      "              < rewriting >\n",
      "\"the renaissance was a \n",
      "cultural transformation in \n",
      "european history, marking the\n",
      "\"the renaissance was a \n",
      "cultural transformation in \n",
      "european history, marking the \n",
      "revival of arts, sciences, and \n",
      "humanistic thought. the \n",
      "fervor of artists and scholars \n",
      "propelled prosperity and \n",
      "innovation in arts, literature, \n",
      "and science.\" give me a \n",
      "summary.\n",
      "                    < summarization >\n",
      "identify who is football players: \n",
      "messi, jordan, kobe. \n",
      "          \n",
      "          < closed qa >\n",
      "tom has three sisters, and each \n",
      "sister has a brother. how many \n",
      "siblings are there in total?\n",
      "tom has three sisters, and each \n",
      "sister has a brother. how many \n",
      "siblings are there in total?  \n",
      "          \n",
      "< reasonning >\n",
      "q: 3,1 a: 3   q: 2,5 a: 5   \n",
      "q: 5,7 a: ?\n",
      "   < in-context learning >                              \n",
      "\"chatgpt is a product of \n",
      "openai.\" \n",
      "please provide the ownership \n",
      "relationship. \n",
      "       < information extraction >\n",
      "no background knowledge\n",
      "if i want to travel from los angeles to new york and i \n",
      "want to choose the cheapest mode of transportation,\n",
      "want to choose the cheapest mode of transportation, \n",
      "should i drive or take a plane?           < decision making >\n",
      "i had a quarrel with my parents because they oppose my \n",
      "relationship with my boyfriend, but we genuinely love \n",
      "each other. how should i persuade my parents to accept \n",
      "our relationship?            \n",
      " \n",
      "   < suggestion >\n",
      "figure 2: classification of retrieval requirements for different tasks. in cases where information is\n",
      "not provided, we differentiate tasks based on the functions of the model.\n",
      "priate algorithm for practical implementation remains challenging. in this paper, we focus on best\n",
      "practices for applying rag methods, advancing the understanding and application of rag in llms.\n",
      "3\n",
      "rag workflow\n",
      "in this section, we detail the components of the rag workflow. for each module, we review\n",
      "commonly used approaches and select the default and alternative methods for our final pipeline.\n",
      "commonly used approaches and select the default and alternative methods for our final pipeline.\n",
      "section 4 will discuss best practices. figure 1 presents the workflow and methods for each module.\n",
      "detailed experimental setups, including datasets, hyperparameters, and results are provided in\n",
      "appendix a.\n",
      "3.1\n",
      "query classification\n",
      "not all queries require retrieval-augmented due to the inherent capabilities of llms. while rag can\n",
      "not all queries require retrieval-augmented due to the inherent capabilities of llms. while rag can\n",
      "enhance information accuracy and reduce hallucinations, frequent retrieval can increase response\n",
      "time. therefore, we begin by classifying queries to determine the necessity of retrieval. queries\n",
      "requiring retrieval proceed through the rag modules; others are handled directly by llms.\n",
      "retrieval is generally recommended when knowledge beyond the model’s parameters is needed.\n",
      "retrieval is generally recommended when knowledge beyond the model’s parameters is needed.\n",
      "however, the necessity of retrieval varies by task. for instance, an llm trained up to 2023 can\n",
      "handle a translation request for “sora was developed by openai” without retrieval. conversely, an\n",
      "introduction request for the same topic would require retrieval to provide relevant information.\n",
      "therefore, we propose classifying tasks by type to determine if a query needs retrieval. we categorize\n",
      "model\n",
      "metrics\n",
      "model\n",
      "metrics\n",
      "acc prec rec\n",
      "f1\n",
      "bert-base-multilingual 0.95 0.96 0.94 0.95\n",
      "table 1: results of the query classifier.\n",
      "15 tasks based on whether they provide suffi-\n",
      "cient information, with specific tasks and exam-\n",
      "ples illustrated in figure 2. for tasks entirely\n",
      "based on user-given information, we denote as\n",
      "“sufficient”, which need not retrieval; otherwise,\n",
      "we denote as “insufficient”, and retrieval may\n",
      "be necessary. we train a classifier to automate\n",
      "this decision-making process. experimental de-\n",
      "be necessary. we train a classifier to automate\n",
      "this decision-making process. experimental de-\n",
      "tails are presented in appendix a.1. section 4\n",
      "explores the impact of query classification on the workflow, comparing scenarios with and without\n",
      "classification.\n",
      "4\n",
      "embedding model\n",
      "namespace-pt/msmarco\n",
      "mrr@1 mrr@10 mrr@100\n",
      "r@1\n",
      "r@10 r@100\n",
      "baai/llm-embedder [20]\n",
      "24.79\n",
      "37.58\n",
      "38.62\n",
      "24.07\n",
      "66.45\n",
      "90.75\n",
      "baai/bge-base-en-v1.5 [12]\n",
      "23.34\n",
      "35.80\n",
      "36.94\n",
      "22.63\n",
      "64.12\n",
      "90.13\n",
      "baai/bge-small-en-v1.5 [12]\n",
      "23.27\n",
      "35.78\n",
      "36.89\n",
      "22.65\n",
      "63.92\n",
      "89.80\n",
      "baai/bge-large-en-v1.5 [12]\n",
      "24.63\n",
      "37.48\n",
      "38.59\n",
      "23.91\n",
      "65.57\n",
      "90.60\n",
      "baai/bge-large-en [12]\n",
      "24.84\n",
      "37.66\n",
      "38.73\n",
      "24.13\n",
      "66.09\n",
      "90.64\n",
      "baai/bge-small-en [12]\n",
      "23.28\n",
      "35.79\n",
      "36.91\n",
      "22.62\n",
      "63.96\n",
      "89.67\n",
      "baai/bge-base-en [12]\n",
      "23.47\n",
      "35.94\n",
      "37.07\n",
      "22.73\n",
      "64.17\n",
      "90.14\n",
      "23.28\n",
      "35.79\n",
      "36.91\n",
      "22.62\n",
      "63.96\n",
      "89.67\n",
      "baai/bge-base-en [12]\n",
      "23.47\n",
      "35.94\n",
      "37.07\n",
      "22.73\n",
      "64.17\n",
      "90.14\n",
      "alibaba-nlp/gte-large-en-v1.5 [21]\n",
      "8.93\n",
      "15.60\n",
      "16.71\n",
      "8.67\n",
      "32.28\n",
      "60.36\n",
      "thenlper/gte-base [21]\n",
      "7.42\n",
      "13.23\n",
      "14.30\n",
      "7.21\n",
      "28.27\n",
      "56.20\n",
      "thenlper/gte-small [21]\n",
      "7.97\n",
      "14.81\n",
      "15.95\n",
      "7.71\n",
      "32.07\n",
      "61.08\n",
      "jinaai/jina-embeddings-v2-small-en [42]\n",
      "8.07\n",
      "15.02\n",
      "16.12\n",
      "7.87\n",
      "32.55\n",
      "60.36\n",
      "intfloat/e5-small-v2 [11]\n",
      "10.04\n",
      "18.23\n",
      "19.41\n",
      "9.74\n",
      "38.92\n",
      "68.42\n",
      "intfloat/e5-large-v2 [11]\n",
      "9.58\n",
      "17.94\n",
      "19.03\n",
      "9.35\n",
      "39.00\n",
      "66.11\n",
      "10.04\n",
      "18.23\n",
      "19.41\n",
      "9.74\n",
      "38.92\n",
      "68.42\n",
      "intfloat/e5-large-v2 [11]\n",
      "9.58\n",
      "17.94\n",
      "19.03\n",
      "9.35\n",
      "39.00\n",
      "66.11\n",
      "sentence-transformers/all-mpnet-base-v2\n",
      "5.80\n",
      "11.26\n",
      "12.26\n",
      "5.66\n",
      "25.57\n",
      "50.94\n",
      "table 2: results for different embedding models on namespace-pt/msmarco.\n",
      "3.2\n",
      "chunking\n",
      "chunking documents into smaller segments is crucial for enhancing retrieval precision and avoiding\n",
      "length issues in llms. this process can be applied at various levels of granularity, such as token,\n",
      "sentence, and semantic levels.\n",
      "sentence, and semantic levels.\n",
      "• token-level chunking is straightforward but may split sentences, affecting retrieval quality.\n",
      "• semantic-level chunking uses llms to determine breakpoints, context-preserving but time-\n",
      "consuming.\n",
      "• sentence-level chunking balances preserving text semantics with simplicity and efficiency.\n",
      "in this study, we use sentence-level chunking, balancing simplicity and semantic preservation. we\n",
      "examine chunking from four dimensions.\n",
      "3.2.1\n",
      "chunk size\n",
      "examine chunking from four dimensions.\n",
      "3.2.1\n",
      "chunk size\n",
      "chunk size significantly impacts performance. larger chunks provide more context, enhancing\n",
      "comprehension but increasing process time. smaller chunks improve retrieval recall and reduce time\n",
      "but may lack sufficient context.\n",
      "finding the optimal chunk size involves a balance between some metrics such as faithfulness, and\n",
      "relevancy. faithfulness measures whether the response is hallucinated or matches the retrieved texts.\n",
      "chunk size\n",
      "lyft_2021\n",
      "chunk size\n",
      "lyft_2021\n",
      "average\n",
      "faithfulness\n",
      "average\n",
      "relevancy\n",
      "2048\n",
      "80.37\n",
      "91.11\n",
      "1024\n",
      "94.26\n",
      "95.56\n",
      "512\n",
      "97.59\n",
      "97.41\n",
      "256\n",
      "97.22\n",
      "97.78\n",
      "128\n",
      "95.74\n",
      "97.22\n",
      "table 3: comparison of different chunk sizes.\n",
      "relevancy measures whether the retrieved texts\n",
      "and responses match queries.\n",
      "we use the\n",
      "evaluation module of llamaindex [43] to cal-\n",
      "culate the metrics above.\n",
      "for embedding,\n",
      "we use the text-embedding-ada-0022 model,\n",
      "which supports long input length. we choose\n",
      "zephyr-7b-alpha3 and gpt-3.5-turbo4 as\n",
      "which supports long input length. we choose\n",
      "zephyr-7b-alpha3 and gpt-3.5-turbo4 as\n",
      "generation model and evaluation model respec-\n",
      "tively. the size of the chunk overlap is 20 tokens.\n",
      "first sixty pages of the document lyft_20215\n",
      "are used as corpus, then prompting llms to\n",
      "generate about one hundred and seventy queries\n",
      "according to chosen corpus. the impact of different chunk sizes is shown in table 3.\n",
      "2https://platform.openai.com/docs/guides/embeddings/embedding-models\n",
      "2https://platform.openai.com/docs/guides/embeddings/embedding-models\n",
      "3https://huggingface.co/huggingfaceh4/zephyr-7b-alpha\n",
      "4https://www.openai.com/\n",
      "5https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/\n",
      "data/10k/lyft_2021.pdf\n",
      "5\n",
      "3.2.2\n",
      "chunking techniques\n",
      "advanced techniques such as small-to-big and sliding window improve retrieval quality by organizing\n",
      "chunk block relationships. small-sized blocks are used to match queries, and larger blocks that\n",
      "include the small ones along with contextual information are returned.\n",
      "to demonstrate the effectiveness of advanced chunking techniques, we use the llm-embedder [20]\n",
      "model as an embedding model. the smaller chunk size is 175 tokens, the larger chunk size is 512\n",
      "model as an embedding model. the smaller chunk size is 175 tokens, the larger chunk size is 512\n",
      "tokens and the chunk overlap is 20 tokens. techniques like small-to-big and sliding window improve\n",
      "retrieval quality by maintaining context and ensuring relevant information is retrieved. detailed\n",
      "results are shown in table 4.\n",
      "3.2.3\n",
      "embedding model selection\n",
      "choosing the right embedding model is crucial for effective semantic matching of queries\n",
      "choosing the right embedding model is crucial for effective semantic matching of queries\n",
      "and chunk blocks. we use the evaluation module of flagembedding6 which uses the dataset\n",
      "chunk skill\n",
      "lyft_2021\n",
      "average\n",
      "faithfulness\n",
      "average\n",
      "relevancy\n",
      "original\n",
      "95.74\n",
      "95.37\n",
      "small2big\n",
      "96.67\n",
      "95.37\n",
      "sliding window\n",
      "97.41\n",
      "96.85\n",
      "table 4: comparison of different chunk skills.\n",
      "namespace-pt/msmarco7\n",
      "as\n",
      "queries\n",
      "and\n",
      "dataset namespace-pt/msmarco-corpus8 as\n",
      "corpus to choose the appropriate open source\n",
      "embedding model.\n",
      "corpus to choose the appropriate open source\n",
      "embedding model.\n",
      "as shown in table 2,\n",
      "llm-embedder [20] achieves comparable\n",
      "results with baai/bge-large-en [12], however,\n",
      "the size of the former is three times smaller\n",
      "than that of the latter.\n",
      "thus, we select the\n",
      "llm-embedder [20] for its balance of\n",
      "performance and size.\n",
      "3.2.4\n",
      "metadata addition\n",
      "enhancing chunk blocks with metadata like titles, keywords, and hypothetical questions can improve\n",
      "enhancing chunk blocks with metadata like titles, keywords, and hypothetical questions can improve\n",
      "retrieval, provide more ways to post-process retrieved texts, and help llms better understand\n",
      "retrieved information. a detailed study on metadata inclusion will be addressed in future work.\n",
      "3.3\n",
      "vector databases\n",
      "vector databases store embedding vectors with their metadata, enabling efficient retrieval of doc-\n",
      "uments relevant to queries through various indexing and approximate nearest neighbor (ann)\n",
      "uments relevant to queries through various indexing and approximate nearest neighbor (ann)\n",
      "methods.\n",
      "to select an appropriate vector database for our research, we evaluated several options based on\n",
      "four key criteria: multiple index types, billion-scale vector support, hybrid search, and cloud-native\n",
      "database\n",
      "multiple\n",
      "index type\n",
      "billion-\n",
      "scale\n",
      "hybrid\n",
      "search\n",
      "cloud-\n",
      "native\n",
      "weaviate\n",
      "✗\n",
      "✗\n",
      "✓\n",
      "✓\n",
      "faiss\n",
      "✓\n",
      "✗\n",
      "✗\n",
      "✗\n",
      "chroma\n",
      "✗\n",
      "✗\n",
      "✓\n",
      "✓\n",
      "qdrant\n",
      "✗\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "milvus\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "table 5: comparison of various vector databases\n",
      "✗\n",
      "✗\n",
      "✗\n",
      "chroma\n",
      "✗\n",
      "✗\n",
      "✓\n",
      "✓\n",
      "qdrant\n",
      "✗\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "milvus\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "table 5: comparison of various vector databases\n",
      "capabilities. these criteria were chosen for their\n",
      "impact on flexibility, scalability, and ease of\n",
      "deployment in modern, cloud-based infrastruc-\n",
      "tures. multiple index types provide the flexibil-\n",
      "ity to optimize searches based on different data\n",
      "characteristics and use cases. billion-scale vec-\n",
      "tor support is crucial for handling large datasets\n",
      "in llm applications. hybrid search combines\n",
      "tor support is crucial for handling large datasets\n",
      "in llm applications. hybrid search combines\n",
      "vector search with traditional keyword search,\n",
      "enhancing retrieval accuracy. finally, cloud-\n",
      "native capabilities ensure seamless integration, scalability, and management in cloud environments.\n",
      "table 5 presents a detailed comparison of five open-source vector databases: weaviate, faiss,\n",
      "chroma, qdrant, and milvus.\n",
      "chroma, qdrant, and milvus.\n",
      "our evaluation indicates that milvus stands out as the most comprehensive solution among the\n",
      "databases evaluated, meeting all the essential criteria and outperforming other open-source options.\n",
      "6https://github.com/flagopen/flagembedding\n",
      "7https://huggingface.co/datasets/namespace-pt/msmarco\n",
      "8https://huggingface.co/datasets/namespace-pt/msmarco-corpus\n",
      "6\n",
      "method\n",
      "trec dl19\n",
      "trec dl20\n",
      "map\n",
      "ndcg@10\n",
      "r@50\n",
      "r@1k\n",
      "latency\n",
      "map\n",
      "ndcg@10\n",
      "r@50\n",
      "r@1k\n",
      "latency\n",
      "unsupervised\n",
      "bm25\n",
      "30.13\n",
      "50.58\n",
      "38.32\n",
      "75.01\n",
      "0.07\n",
      "28.56\n",
      "47.96\n",
      "46.18\n",
      "78.63\n",
      "0.29\n",
      "contriever\n",
      "23.99\n",
      "44.54\n",
      "37.54\n",
      "74.59\n",
      "3.06\n",
      "23.98\n",
      "42.13\n",
      "43.81\n",
      "75.39\n",
      "0.98\n",
      "supervised\n",
      "llm-embedder\n",
      "44.66\n",
      "70.20\n",
      "49.06\n",
      "84.48\n",
      "2.61\n",
      "45.60\n",
      "68.76\n",
      "61.36\n",
      "84.41\n",
      "0.71\n",
      "+ query rewriting\n",
      "44.56\n",
      "67.89\n",
      "51.45\n",
      "85.35\n",
      "7.80\n",
      "45.16\n",
      "65.62\n",
      "59.63\n",
      "83.45\n",
      "2.06\n",
      "+ query decomposition\n",
      "41.93\n",
      "66.10\n",
      "48.66\n",
      "82.62\n",
      "14.98\n",
      "43.30\n",
      "64.95\n",
      "57.74\n",
      "84.18\n",
      "2.01\n",
      "+ hyde\n",
      "50.87\n",
      "75.44\n",
      "54.93\n",
      "41.93\n",
      "66.10\n",
      "48.66\n",
      "82.62\n",
      "14.98\n",
      "43.30\n",
      "64.95\n",
      "57.74\n",
      "84.18\n",
      "2.01\n",
      "+ hyde\n",
      "50.87\n",
      "75.44\n",
      "54.93\n",
      "88.76\n",
      "7.21\n",
      "50.94\n",
      "73.94\n",
      "63.80\n",
      "88.03\n",
      "2.14\n",
      "+ hybrid search\n",
      "47.14\n",
      "72.50\n",
      "51.13\n",
      "89.08\n",
      "3.20\n",
      "47.72\n",
      "69.80\n",
      "64.32\n",
      "88.04\n",
      "0.77\n",
      "+ hyde + hybrid search\n",
      "52.13\n",
      "73.34\n",
      "55.38\n",
      "90.42\n",
      "11.16\n",
      "53.13\n",
      "72.72\n",
      "66.14\n",
      "90.67\n",
      "2.95\n",
      "table 6: results for different retrieval methods on trec dl19/20. the best result for each method\n",
      "is made bold and the second is underlined.\n",
      "configuration\n",
      "trec dl19\n",
      "trec dl20\n",
      "map\n",
      "ndcg@10\n",
      "r@50\n",
      "r@1k\n",
      "latency\n",
      "map\n",
      "ndcg@10\n",
      "configuration\n",
      "trec dl19\n",
      "trec dl20\n",
      "map\n",
      "ndcg@10\n",
      "r@50\n",
      "r@1k\n",
      "latency\n",
      "map\n",
      "ndcg@10\n",
      "r@50\n",
      "r@1k\n",
      "latency\n",
      "hyde\n",
      "w/ 1 pseudo-doc\n",
      "48.77\n",
      "72.49\n",
      "53.20\n",
      "87.73\n",
      "8.08\n",
      "51.31\n",
      "70.37\n",
      "63.28\n",
      "87.81\n",
      "2.09\n",
      "w/ 1 pseudo-doc + query\n",
      "50.87\n",
      "75.44\n",
      "54.93\n",
      "88.76\n",
      "7.21\n",
      "50.94\n",
      "73.94\n",
      "63.80\n",
      "88.03\n",
      "2.14\n",
      "w/ 8 pseudo-doc + query\n",
      "51.64\n",
      "75.12\n",
      "54.51\n",
      "89.17\n",
      "14.15\n",
      "53.14\n",
      "73.65\n",
      "65.79\n",
      "88.67\n",
      "3.44\n",
      "table 7: hyde with different concatenation of hypothetical documents and queries.\n",
      "3.4\n",
      "retrieval methods\n",
      "3.4\n",
      "retrieval methods\n",
      "given a user query, the retrieval module selects the top-k relevant documents from a pre-built corpus\n",
      "based on the similarity between the query and the documents. the generation model then uses\n",
      "these documents to formulate an appropriate response to the query. however, original queries often\n",
      "underperform due to poor expression and lack of semantic information [6], negatively impacting the\n",
      "underperform due to poor expression and lack of semantic information [6], negatively impacting the\n",
      "retrieval process. to address these issues, we evaluated three query transformation methods using the\n",
      "llm-embedder recommended in section 3.2 as the query and document encoder:\n",
      "• query rewriting: query rewriting refines queries to better match relevant documents. inspired\n",
      "by the rewrite-retrieve-read framework [9], we prompt an llm to rewrite queries to enhance\n",
      "performance.\n",
      "performance.\n",
      "• query decomposition: this approach involves retrieving documents based on sub-questions\n",
      "derived from the original query, which is more complex to comprehend and handle.\n",
      "• pseudo-documents generation: this approach generates a hypothetical document based on the\n",
      "user query and uses the embedding of hypothetical answers to retrieve similar documents. one\n",
      "notable implement is hyde [10],\n",
      "notable implement is hyde [10],\n",
      "recent studies, such as [44], indicate that combining lexical-based search with vector search signifi-\n",
      "cantly enhances performance. in this study, we use bm25 for sparse retrieval and contriever [45], an\n",
      "unsupervised contrastive encoder, for dense retrieval, serving as two robust baselines based on thakur\n",
      "et al. [46].\n",
      "3.4.1\n",
      "results for different retrieval methods\n",
      "we evaluated the performance of different search methods on the trec dl 2019 and 2020 passage\n",
      "we evaluated the performance of different search methods on the trec dl 2019 and 2020 passage\n",
      "ranking datasets. the results presented in table 6 show that supervised methods significantly\n",
      "outperformed unsupervised methods. combining with hyde and hybrid search, llm-embedder\n",
      "achieves the highest scores. however, query rewriting and query decomposition did not enhance\n",
      "retrieval performance as effectively. considering the best performance and tolerated latency, we\n",
      "retrieval performance as effectively. considering the best performance and tolerated latency, we\n",
      "recommend hybrid search with hyde as the default retrieval method. taking efficiency into\n",
      "consideration, hybrid search combines sparse retrieval (bm25) and dense retrieval (original\n",
      "embedding) and achieves notable performance with relatively low latency.\n",
      "3.4.2\n",
      "hyde with different concatenation of documents and query\n",
      "3.4.2\n",
      "hyde with different concatenation of documents and query\n",
      "table 7 shows the impact of different concatenation strategies for hypothetical documents and queries\n",
      "using hyde. concatenating multiple pseudo-documents with the original query can significantly\n",
      "7\n",
      "hyperparameter\n",
      "trec dl19\n",
      "trec dl20\n",
      "map\n",
      "ndcg@10\n",
      "r@50\n",
      "r@1k\n",
      "latency\n",
      "map\n",
      "ndcg@10\n",
      "r@50\n",
      "r@1k\n",
      "latency\n",
      "hybrid search\n",
      "α = 0.1\n",
      "46.00\n",
      "70.87\n",
      "49.24\n",
      "88.89\n",
      "2.98\n",
      "46.54\n",
      "69.05\n",
      "63.36\n",
      "87.32\n",
      "0.90\n",
      "α = 0.3\n",
      "47.14\n",
      "72.50\n",
      "51.13\n",
      "89.08\n",
      "3.20\n",
      "47.72\n",
      "69.80\n",
      "64.32\n",
      "88.04\n",
      "0.77\n",
      "α = 0.5\n",
      "47.36\n",
      "72.24\n",
      "52.71\n",
      "88.09\n",
      "3.02\n",
      "47.19\n",
      "68.12\n",
      "64.90\n",
      "87.86\n",
      "0.87\n",
      "α = 0.7\n",
      "47.21\n",
      "71.89\n",
      "52.40\n",
      "88.01\n",
      "3.15\n",
      "45.82\n",
      "67.30\n",
      "64.23\n",
      "87.92\n",
      "1.02\n",
      "α = 0.9\n",
      "46.35\n",
      "70.67\n",
      "52.64\n",
      "88.22\n",
      "2.74\n",
      "44.02\n",
      "65.55\n",
      "63.22\n",
      "87.76\n",
      "1.20\n",
      "3.15\n",
      "45.82\n",
      "67.30\n",
      "64.23\n",
      "87.92\n",
      "1.02\n",
      "α = 0.9\n",
      "46.35\n",
      "70.67\n",
      "52.64\n",
      "88.22\n",
      "2.74\n",
      "44.02\n",
      "65.55\n",
      "63.22\n",
      "87.76\n",
      "1.20\n",
      "table 8: results of hybrid search with different alpha values.\n",
      "method\n",
      "ms marco passage ranking\n",
      "base model\n",
      "# params\n",
      "mrr@1\n",
      "mrr@10\n",
      "mrr@1k\n",
      "hit rate@10\n",
      "latency\n",
      "w/o reranking\n",
      "random ordering\n",
      "-\n",
      "-\n",
      "0.011\n",
      "0.027\n",
      "0.068\n",
      "0.092\n",
      "-\n",
      "bm25\n",
      "-\n",
      "-\n",
      "6.52\n",
      "11.65\n",
      "12.59\n",
      "24.63\n",
      "-\n",
      "dlm reranking\n",
      "monot5\n",
      "t5-base\n",
      "220m\n",
      "21.62\n",
      "31.78\n",
      "32.40\n",
      "54.07\n",
      "4.5\n",
      "monobert\n",
      "bert-large\n",
      "340m\n",
      "21.65\n",
      "31.69\n",
      "32.35\n",
      "53.38\n",
      "15.8\n",
      "rankllama\n",
      "llama-2-7b\n",
      "7b\n",
      "22.08\n",
      "32.40\n",
      "54.07\n",
      "4.5\n",
      "monobert\n",
      "bert-large\n",
      "340m\n",
      "21.65\n",
      "31.69\n",
      "32.35\n",
      "53.38\n",
      "15.8\n",
      "rankllama\n",
      "llama-2-7b\n",
      "7b\n",
      "22.08\n",
      "32.35\n",
      "32.97\n",
      "54.53\n",
      "82.4\n",
      "tilde reranking\n",
      "tildev2\n",
      "bert-base\n",
      "110m\n",
      "18.57\n",
      "27.83\n",
      "28.60\n",
      "49.07\n",
      "0.02\n",
      "table 9: results of different reranking methods on the dev set of the ms marco passage ranking\n",
      "dataset. for each query, the top-1000 candidate passages retrieved by bm25 are reranked. latency is\n",
      "measured in seconds per query.\n",
      "measured in seconds per query.\n",
      "enhance retrieval performance, though at the cost of increased latency, suggesting a trade-off between\n",
      "retrieval effectiveness and efficiency. however, indiscriminately increasing the number of hypothetical\n",
      "documents does not yield significant benefits and substantially raises latency, indicating that using a\n",
      "single hypothetical document is sufficient.\n",
      "3.4.3\n",
      "hybrid search with different weight on sparse retrieval\n",
      "3.4.3\n",
      "hybrid search with different weight on sparse retrieval\n",
      "table 8 presents the impact of different α values in hybrid search, where α controls the weighting\n",
      "between sparse retrieval and dense retrieval components. the relevance score is calculated as follows:\n",
      "sh = α · ss + sd\n",
      "(1)\n",
      "where ss, sd are the normalized relevance scores from sparse retrieval and dense retrieval respectively,\n",
      "and sh is the total retrieval score.\n",
      "and sh is the total retrieval score.\n",
      "we evaluated five different α values to determine their influence on performance. the results indicate\n",
      "that an α value of 0.3 yields the best performance, demonstrating that appropriate adjustment of\n",
      "α can enhance retrieval effectiveness to a certain extent. therefore, we selected α = 0.3 for our\n",
      "retrieval and main experiments. additional implementation details are presented in appendix a.2.\n",
      "3.5\n",
      "reranking methods\n",
      "3.5\n",
      "reranking methods\n",
      "after the initial retrieval, a reranking phase is employed to enhance the relevance of the retrieved\n",
      "documents, ensuring that the most pertinent information appears at the top of the list. this phase uses\n",
      "more precise and time-intensive methods to reorder documents effectively, increasing the similarity\n",
      "between the query and the top-ranked documents.\n",
      "we consider two approaches in our reranking module: dlm reranking, which utilizes classifi-\n",
      "we consider two approaches in our reranking module: dlm reranking, which utilizes classifi-\n",
      "cation, and tilde reranking, which focuses on query likelihoods. these approaches prioritize\n",
      "performance and efficiency, respectively.\n",
      "• dlm reranking:\n",
      "this method leverages deep language models (dlms) [25–27] for reranking.\n",
      "these models are fine-tuned to classify document relevancy to a query as “true” or “false”. during\n",
      "these models are fine-tuned to classify document relevancy to a query as “true” or “false”. during\n",
      "fine-tuning, the model is trained with concatenated query and document inputs, labeled by relevancy.\n",
      "at inference, documents are ranked based on the probability of the “true” token.\n",
      "• tilde reranking:\n",
      "tilde [28, 29] calculates the likelihood of each query term independently\n",
      "by predicting token probabilities across the model’s vocabulary. documents are scored by summing\n",
      "8\n",
      "method\n",
      "nq\n",
      "tqa\n",
      "hotpotqa\n",
      "avg.\n",
      "avg. token\n",
      "f1\n",
      "#token\n",
      "f1\n",
      "#token\n",
      "f1\n",
      "#token\n",
      "w/o summarization\n",
      "origin prompt\n",
      "27.07\n",
      "124\n",
      "33.61\n",
      "152\n",
      "33.92\n",
      "141\n",
      "31.53\n",
      "139\n",
      "extractive method\n",
      "bm25\n",
      "27.97\n",
      "40\n",
      "32.44\n",
      "59\n",
      "28.00\n",
      "63\n",
      "29.47\n",
      "54\n",
      "contriever\n",
      "23.62\n",
      "42\n",
      "33.79\n",
      "65\n",
      "23.64\n",
      "60\n",
      "27.02\n",
      "56\n",
      "recomp (extractive)\n",
      "27.84\n",
      "34\n",
      "35.32\n",
      "60\n",
      "29.46\n",
      "58\n",
      "30.87\n",
      "51\n",
      "abstractive method\n",
      "selectivecontext\n",
      "25.05\n",
      "65\n",
      "34.25\n",
      "70\n",
      "34.43\n",
      "66\n",
      "31.24\n",
      "67\n",
      "longllmlingua\n",
      "21.32\n",
      "51\n",
      "32.81\n",
      "56\n",
      "30.79\n",
      "57\n",
      "28.29\n",
      "55\n",
      "recomp (abstractive)\n",
      "33.68\n",
      "59\n",
      "35.87\n",
      "61\n",
      "29.01\n",
      "57\n",
      "32.85\n",
      "59\n",
      "21.32\n",
      "51\n",
      "32.81\n",
      "56\n",
      "30.79\n",
      "57\n",
      "28.29\n",
      "55\n",
      "recomp (abstractive)\n",
      "33.68\n",
      "59\n",
      "35.87\n",
      "61\n",
      "29.01\n",
      "57\n",
      "32.85\n",
      "59\n",
      "table 10: comparison between different summarization methods.\n",
      "the pre-calculated log probabilities of query tokens, allowing for rapid reranking at inference.\n",
      "tildev2 improves this by indexing only document-present tokens, using nce loss, and expanding\n",
      "documents, thus enhancing efficiency and reducing index size.\n",
      "our experiments were conducted on the ms marco passage ranking dataset [47], a large-scale\n",
      "our experiments were conducted on the ms marco passage ranking dataset [47], a large-scale\n",
      "dataset for machine reading comprehension. we follow and make modifications to the implementation\n",
      "provided by pygaggle [26] and tilde [28], using the models monot5, monobert, rankllama\n",
      "and tildev2. reranking results are shown in table 9. we recommend monot5 as a comprehensive\n",
      "method balancing performance and efficiency. rankllama is suitable for achieving the best\n",
      "method balancing performance and efficiency. rankllama is suitable for achieving the best\n",
      "performance, while tildev2 is ideal for the quickest experience on a fixed collection. details on\n",
      "the experimental setup and results are presented in appendix a.3.\n",
      "3.6\n",
      "document repacking\n",
      "the performance of subsequent processes, such as llm response generation, may be affected by the\n",
      "order documents are provided. to address this issue, we incorporate a compact repacking module into\n",
      "order documents are provided. to address this issue, we incorporate a compact repacking module into\n",
      "the workflow after reranking, featuring three repacking methods: “forward”, “reverse” and “sides”.\n",
      "the “forward” method repacks documents by descending relevancy scores from the reranking phase,\n",
      "whereas the “reverse” arranges them in ascending order. inspired by liu et al. [48], concluding that\n",
      "optimal performance is achieved when relevant information is placed at the head or tail of the input,\n",
      "we also include a “sides” option.\n",
      "since the repacking method primarily affects subsequent modules, we select the best repacking\n",
      "method in section 4 by testing it in combination with other modules. in this section, we choose the\n",
      "“sides” method as the default repacking method.\n",
      "3.7\n",
      "summarization\n",
      "retrieval results may contain redundant or unnecessary information, potentially preventing llms\n",
      "from generating accurate responses. additionally, long prompts can slow down the inference process.\n",
      "from generating accurate responses. additionally, long prompts can slow down the inference process.\n",
      "therefore, efficient methods to summarize retrieved documents are crucial in the rag pipeline.\n",
      "summarization tasks can be extractive or abstractive. extractive methods segment text into sen-\n",
      "tences, then score and rank them based on importance. abstractive compressors synthesize infor-\n",
      "mation from multiple documents to rephrase and generate a cohesive summary. these tasks can be\n",
      "mation from multiple documents to rephrase and generate a cohesive summary. these tasks can be\n",
      "query-based or non-query-based. in this paper, as rag retrieves information relevant to queries, we\n",
      "focus exclusively on query-based methods.\n",
      "• recomp:\n",
      "recomp [23] has extractive and abstractive compressors. the extractive compressor\n",
      "selects useful sentences, while the abstractive compressor synthesizes information from multiple\n",
      "documents.\n",
      "• longllmlingua:\n",
      "documents.\n",
      "• longllmlingua:\n",
      "longllmlingua [49] improves llmlingua by focusing on key informa-\n",
      "tion related to the query.\n",
      "• selective context\n",
      "selective context enhances llm efficiency by identifying and removing\n",
      "redundant information in the input context. it evaluates the informativeness of lexical units using\n",
      "9\n",
      "self-information computed by a base causal language model. this method is non-query-based,\n",
      "allowing a comparison between query-based and non-query-based approaches.\n",
      "we evaluate these methods on three benchmark datasets: nq, triviaqa, and hotpotqa. comparative\n",
      "results of different summarization methods are shown in table 10. we recommend recomp for\n",
      "its outstanding performance. longllmlingua does not perform well but demonstrates better\n",
      "its outstanding performance. longllmlingua does not perform well but demonstrates better\n",
      "generalization capabilities as it was not trained on these experimental datasets. therefore, we consider\n",
      "it as an alternative method. additional implementation details and discussions on non-query-based\n",
      "methods are provided in appendix a.4.\n",
      "3.8\n",
      "generator fine-tuning\n",
      "in this section, we focus on fine-tuning the generator while leaving retriever fine-tuning for future\n",
      "exploration. we aim to investigate the impact of fine-tuning, particularly the influence of relevant or\n",
      "irrelevant contexts on the generator’s performance.\n",
      "formally, we denote x as the query fed into the rag system, and d as the contexts for this input.\n",
      "the fine-tuning loss of the generator is the negative log-likelihood of the ground-truth output y.\n",
      "to explore the impact of fine-tuning, especially relevant and irrelevant contexts, we define dgold as a\n",
      "context relevant to the query, and drandom as a randomly retrieved context. we train the model by\n",
      "varying the composition of d as follows:\n",
      "• dg: the augmented context consists of query-relevant documents, denoted as dg = {dgold}.\n",
      "• dr: the context contains one randomly sampled document, denoted as dr = {drandom}.\n",
      "• dgr: the augmented context comprises a relevant document and a randomly-selected one, denoted\n",
      "as dgr = {dgold, drandom}.\n",
      "as dgr = {dgold, drandom}.\n",
      "• dgg: the augmented context consists of two copies of a query-relevant document, denoted as\n",
      "dgg = {dgold, dgold}.\n",
      "we denote the base lm generator not fine-tuned as mb , and the model fine-tuned under the\n",
      "corresponding d as mg, mr, mgr, mgg. we fine-tuned our model on several qa and reading\n",
      "d\n",
      "dg\n",
      "dr\n",
      "dgr\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "coverage score\n",
      "models trained with different method\n",
      "mb\n",
      "mg\n",
      "mr\n",
      "mgr\n",
      "mgg\n",
      "figure 3: results of generator fine-tuning.\n",
      "models trained with different method\n",
      "mb\n",
      "mg\n",
      "mr\n",
      "mgr\n",
      "mgg\n",
      "figure 3: results of generator fine-tuning.\n",
      "comprehension datasets. ground-truth coverage\n",
      "is used as our evaluation metric since qa task\n",
      "answers are relatively short. we select llama-2-\n",
      "7b [50] as the base model. similar to training,\n",
      "we evaluate all trained models on validation sets\n",
      "with dg, dr, dgr, and d∅, where d∅indicates\n",
      "inference without retrieval. figure 3 presents\n",
      "our main results. models trained with a mix of\n",
      "inference without retrieval. figure 3 presents\n",
      "our main results. models trained with a mix of\n",
      "relevant and random documents (mgr) perform\n",
      "best when provided with either gold or mixed\n",
      "contexts. this suggests that mixing relevant and\n",
      "random contexts during training can enhance the\n",
      "generator’s robustness to irrelevant information\n",
      "while ensuring effective utilization of relevant\n",
      "contexts. therefore, we identify the practice of\n",
      "while ensuring effective utilization of relevant\n",
      "contexts. therefore, we identify the practice of\n",
      "augmenting with a few relevant and randomly-selected documents during training as the best\n",
      "approach. detailed dataset information, hyperparameters and experimental results can be found in\n",
      "appendix a.5.\n",
      "4\n",
      "searching for best rag practices\n",
      "in the following section, we investigate the optimal practices for implementing rag. to begin\n",
      "in the following section, we investigate the optimal practices for implementing rag. to begin\n",
      "with, we used the default practice identified in section 3 for each module. following the workflow\n",
      "depicted in figure 1, we sequentially optimized individual modules and selected the most effective\n",
      "option among alternatives. this iterative process continued until we determined the best method for\n",
      "implementing the final summarization module. based on section 3.8, we used the llama2-7b-chat\n",
      "implementing the final summarization module. based on section 3.8, we used the llama2-7b-chat\n",
      "model fine-tuned where each query was augmented by a few random-selected and relevant documents\n",
      "10\n",
      "method\n",
      "commonsense\n",
      "fact check\n",
      "odqa\n",
      "multihop\n",
      "medical\n",
      "rag\n",
      "avg.\n",
      "acc\n",
      "acc\n",
      "em\n",
      "f1\n",
      "em\n",
      "f1\n",
      "acc\n",
      "score\n",
      "score\n",
      "f1\n",
      "latency\n",
      "classification module , hybrid with hyde, monot5, sides, recomp\n",
      "w/o classification\n",
      "0.719\n",
      "0.505\n",
      "0.391\n",
      "0.450\n",
      "0.212\n",
      "0.255\n",
      "0.528\n",
      "0.540\n",
      "0.465\n",
      "0.353\n",
      "16.58\n",
      "+ classification\n",
      "0.727\n",
      "0.595\n",
      "0.393\n",
      "0.450\n",
      "0.207\n",
      "0.257\n",
      "0.460\n",
      "0.580\n",
      "0.478\n",
      "0.353\n",
      "11.71\n",
      "with classification,\n",
      "retrieval module , monot5, sides, recomp\n",
      "+ hyde\n",
      "0.718\n",
      "0.595\n",
      "0.320\n",
      "0.373\n",
      "0.170\n",
      "0.213\n",
      "0.400\n",
      "0.545\n",
      "0.443\n",
      "0.293\n",
      "11.58\n",
      "+ original\n",
      "0.721\n",
      "0.585\n",
      "+ hyde\n",
      "0.718\n",
      "0.595\n",
      "0.320\n",
      "0.373\n",
      "0.170\n",
      "0.213\n",
      "0.400\n",
      "0.545\n",
      "0.443\n",
      "0.293\n",
      "11.58\n",
      "+ original\n",
      "0.721\n",
      "0.585\n",
      "0.300\n",
      "0.350\n",
      "0.153\n",
      "0.197\n",
      "0.390\n",
      "0.486\n",
      "0.428\n",
      "0.273\n",
      "1.44\n",
      "+ hybrid\n",
      "0.718\n",
      "0.595\n",
      "0.347\n",
      "0.397\n",
      "0.190\n",
      "0.240\n",
      "0.750\n",
      "0.498\n",
      "0.477\n",
      "0.318\n",
      "1.45\n",
      "+ hybrid with hyde\n",
      "0.727\n",
      "0.595\n",
      "0.393\n",
      "0.450\n",
      "0.207\n",
      "0.257\n",
      "0.460\n",
      "0.580\n",
      "0.478\n",
      "0.353\n",
      "11.71\n",
      "with classification, hybrid with hyde, reranking module , sides, recomp\n",
      "w/o reranking\n",
      "0.720\n",
      "0.591\n",
      "0.365\n",
      "0.429\n",
      "0.211\n",
      "0.260\n",
      "0.512\n",
      "0.530\n",
      "0.470\n",
      "0.334\n",
      "10.31\n",
      "+ monot5\n",
      "0.727\n",
      "0.595\n",
      "0.393\n",
      "0.450\n",
      "0.207\n",
      "0.591\n",
      "0.365\n",
      "0.429\n",
      "0.211\n",
      "0.260\n",
      "0.512\n",
      "0.530\n",
      "0.470\n",
      "0.334\n",
      "10.31\n",
      "+ monot5\n",
      "0.727\n",
      "0.595\n",
      "0.393\n",
      "0.450\n",
      "0.207\n",
      "0.257\n",
      "0.460\n",
      "0.580\n",
      "0.478\n",
      "0.353\n",
      "11.71\n",
      "+ monobert\n",
      "0.723\n",
      "0.593\n",
      "0.383\n",
      "0.443\n",
      "0.217\n",
      "0.259\n",
      "0.482\n",
      "0.551\n",
      "0.475\n",
      "0.351\n",
      "11.65\n",
      "+ rankllama\n",
      "0.723\n",
      "0.597\n",
      "0.382\n",
      "0.443\n",
      "0.197\n",
      "0.240\n",
      "0.454\n",
      "0.558\n",
      "0.470\n",
      "0.342\n",
      "13.51\n",
      "+ tildev2\n",
      "0.725\n",
      "0.588\n",
      "0.394\n",
      "0.456\n",
      "0.209\n",
      "0.255\n",
      "0.486\n",
      "0.536\n",
      "0.476\n",
      "0.355\n",
      "11.26\n",
      "with classification, hybrid with hyde, monot5, repacking module , recomp\n",
      "+ sides\n",
      "0.727\n",
      "0.595\n",
      "0.393\n",
      "0.450\n",
      "0.207\n",
      "0.257\n",
      "0.460\n",
      "0.580\n",
      "+ sides\n",
      "0.727\n",
      "0.595\n",
      "0.393\n",
      "0.450\n",
      "0.207\n",
      "0.257\n",
      "0.460\n",
      "0.580\n",
      "0.478\n",
      "0.353\n",
      "11.71\n",
      "+ forward\n",
      "0.722\n",
      "0.599\n",
      "0.379\n",
      "0.437\n",
      "0.215\n",
      "0.260\n",
      "0.472\n",
      "0.542\n",
      "0.474\n",
      "0.349\n",
      "11.68\n",
      "+ reverse\n",
      "0.728\n",
      "0.592\n",
      "0.387\n",
      "0.445\n",
      "0.219\n",
      "0.263\n",
      "0.532\n",
      "0.560\n",
      "0.483\n",
      "0.354\n",
      "11.70\n",
      "with classification, hybrid with hyde, monot5, reverse, summarization module\n",
      "w/o summarization\n",
      "0.729\n",
      "0.591\n",
      "0.402\n",
      "0.457\n",
      "0.205\n",
      "0.252\n",
      "0.528\n",
      "0.533\n",
      "0.480\n",
      "0.355\n",
      "10.97\n",
      "+ recomp\n",
      "0.728\n",
      "0.592\n",
      "0.387\n",
      "0.445\n",
      "0.219\n",
      "0.263\n",
      "0.532\n",
      "0.560\n",
      "0.483\n",
      "0.354\n",
      "11.70\n",
      "+ longllmlingua\n",
      "0.713\n",
      "0.581\n",
      "0.362\n",
      "0.728\n",
      "0.592\n",
      "0.387\n",
      "0.445\n",
      "0.219\n",
      "0.263\n",
      "0.532\n",
      "0.560\n",
      "0.483\n",
      "0.354\n",
      "11.70\n",
      "+ longllmlingua\n",
      "0.713\n",
      "0.581\n",
      "0.362\n",
      "0.423\n",
      "0.199\n",
      "0.245\n",
      "0.530\n",
      "0.539\n",
      "0.466\n",
      "0.334\n",
      "16.17\n",
      "table 11: results of the search for optimal rag practices. modules enclosed in a boxed module\n",
      "are under investigation to determine the best method. the underlined method represents the selected\n",
      "implementation. the “avg” (average score) is calculated based on the acc, em, and rag scores for\n",
      "implementation. the “avg” (average score) is calculated based on the acc, em, and rag scores for\n",
      "all tasks, while the average latency is measured in seconds per query. the best scores are highlighted\n",
      "in bold.\n",
      "as the generator. we used milvus to build a vector database that includes 10 million text of english\n",
      "wikipedia and 4 million text of medical data. we also investigated the impact of removing the query\n",
      "classification, reranking, and summarization modules to assess their contributions.\n",
      "4.1\n",
      "classification, reranking, and summarization modules to assess their contributions.\n",
      "4.1\n",
      "comprehensive evaluation\n",
      "we conducted extensive experiments across various nlp tasks and datasets to assess the perfor-\n",
      "mance of rag systems. specifically: (i) commonsense reasoning; (ii) fact checking; (iii)\n",
      "open-domain qa; (iv) multihop qa; (v) medical qa. for further details on the tasks and\n",
      "their corresponding datasets, please refer to appendix a.6. furthermore, we evaluated the rag\n",
      "their corresponding datasets, please refer to appendix a.6. furthermore, we evaluated the rag\n",
      "capabilities on subsets extracted from these datasets, employing the metrics recommended in ra-\n",
      "gas [51], including faithfulness, context relevancy, answer relevancy, and answer correctness.\n",
      "additionally, we measured retrieval similarity by computing the cosine similarity between retrieved\n",
      "documents and gold documents.\n",
      "documents and gold documents.\n",
      "we used accuracy as the evaluation metric for the tasks of commonsense reasoning, fact checking,\n",
      "and medical qa. for open-domain qa and multihop qa, we employed token-level f1 score and\n",
      "exact match (em) score. the final rag score was calculated by averaging the aforementioned five\n",
      "rag capabilities. we followed trivedi et al. [52] and sub-sampled up to 500 examples from each\n",
      "dataset.\n",
      "4.2\n",
      "results and analysis\n",
      "dataset.\n",
      "4.2\n",
      "results and analysis\n",
      "based on the experimental results presented in table 11, the following key insights emerge:\n",
      "• query classification module: this module is referenced and contributes to both effectiveness\n",
      "and efficiency, leading to an average improvement in the overall score from 0.428 to 0.443 and a\n",
      "reduction in latency time from 16.41 to 11.58 seconds per query.\n",
      "11\n",
      "• retrieval module: while the “hybrid with hyde” method attained the highest rag score of\n",
      "0.58, it does so at a considerable computational cost with 11.71 second per query. consequently,\n",
      "the “hybrid” or “original” methods are recommended, as they reduce latency while maintaining\n",
      "comparable performance.\n",
      "• reranking module: the absence of a reranking module led to a noticeable drop in performance,\n",
      "highlighting its necessity. monot5 achieved the highest average score, affirming its efficacy in\n",
      "highlighting its necessity. monot5 achieved the highest average score, affirming its efficacy in\n",
      "augmenting the relevance of retrieved documents. this indicates the critical role of reranking in\n",
      "enhancing the quality of generated responses.\n",
      "• repacking module: the reverse configuration exhibited superior performance, achieving an\n",
      "rag score of 0.560. this indicates that positioning more relevant context closer to the query leads\n",
      "to optimal outcomes.\n",
      "to optimal outcomes.\n",
      "• summarization module: recomp demonstrated superior performance, although achieving compa-\n",
      "rable results with lower latency was possible by removing the summarization module. nevertheless,\n",
      "recomp remains the preferred choice due to its capability to address the generator’s maximum\n",
      "length constraints. in time-sensitive applications, removing summarization could effectively reduce\n",
      "response time.\n",
      "response time.\n",
      "the experimental results demonstrate that each module contributes uniquely to the overall perfor-\n",
      "mance of the rag system. the query classification module enhances accuracy and reduces latency,\n",
      "while the retrieval and reranking modules significantly improve the system’s ability to handle diverse\n",
      "queries. the repacking and summarization modules further refine the system’s output, ensuring\n",
      "high-quality responses across different tasks.\n",
      "5\n",
      "discussion\n",
      "5.1\n",
      "high-quality responses across different tasks.\n",
      "5\n",
      "discussion\n",
      "5.1\n",
      "best practices for implementing rag\n",
      "according to our experimental findings, we suggest two distinct recipes or practices for implementing\n",
      "rag systems, each customized to address specific requirements: one focusing on maximizing\n",
      "performance, and the other on striking a balance between efficiency and efficacy.\n",
      "best performance practice: to achieve the highest performance, it is recommended to incorporate\n",
      "best performance practice: to achieve the highest performance, it is recommended to incorporate\n",
      "query classification module, use the “hybrid with hyde” method for retrieval, employ monot5 for\n",
      "reranking, opt for reverse for repacking, and leverage recomp for summarization. this configuration\n",
      "yielded the highest average score of 0.483, albeit with a computationally-intensive process.\n",
      "balanced efficiency practice: in order to achieve a balance between performance and efficiency,\n",
      "balanced efficiency practice: in order to achieve a balance between performance and efficiency,\n",
      "it is recommended to incorporate the query classification module, implement the hybrid method\n",
      "for retrieval, use tildev2 for reranking, opt for reverse for repacking, and employ recomp for\n",
      "summarization. given that the retrieval module accounts for the majority of processing time in the\n",
      "system, transitioning to the hybrid method while keeping other modules unchanged can substantially\n",
      "system, transitioning to the hybrid method while keeping other modules unchanged can substantially\n",
      "reduce latency while preserving a comparable performance.\n",
      "5.2\n",
      "multimodal extension\n",
      "we have extended rag to multimodal applications. specifically, we have incorporated text2image\n",
      "and image2text retrieval capabilities into the system with a substantial collection of paired image and\n",
      "textual descriptions as a retrieval source. as depicted in figure 4, the text2image capability speeds\n",
      "up the image generation process when a user query aligns well with the textual descriptions of stored\n",
      "images (i.e., “retrieval as generation” strategy), while the image2text functionality comes into play\n",
      "when a user provides an image and engages in conversation about the input image. these multimodal\n",
      "rag capabilities offer the following advantages:\n",
      "• groundedness: retrieval methods provide information from verified multimodal materials, thereby\n",
      "• groundedness: retrieval methods provide information from verified multimodal materials, thereby\n",
      "ensuring authenticity and specificity. in contrast, on-the-fly generation relies on models to generate\n",
      "new content, which can occasionally result in factual errors or inaccuracies.\n",
      "• efficiency: retrieval methods are typically more efficient, especially when the answer already\n",
      "exists in stored materials. conversely, generation methods may require more computational\n",
      "exists in stored materials. conversely, generation methods may require more computational\n",
      "resources to produce new content, particularly for images or lengthy texts.\n",
      "12\n",
      "a dog is sleeping\n",
      "retrieval\n",
      "retrieval\n",
      "retrieval\n",
      "a dog is sleeping.\n",
      "a dog is drinking water\n",
      "a dog is sleeping\n",
      "retrieval\n",
      "a dog is sleeping\n",
      "image2text retrieval\n",
      "text2image retrieval\n",
      "high similarity\n",
      "low similarity\n",
      "user query\n",
      "image caption model\n",
      "image generation model\n",
      "figure 4: workflow of multimodal retrieval. the upper section illustrates the text-to-image retrieval\n",
      "process. initially, a text query is used to find images in the database with the highest similarity. if a\n",
      "high similarity is found, the image is returned directly. if not, an image generation model is employed\n",
      "to create and return an appropriate image. the lower section demonstrates the image-to-text retrieval\n",
      "process. here, a user-provided image is matched with images in the database to find the highest\n",
      "similarity. if a high similarity is identified, the pre-stored caption of the matching image is returned.\n",
      "otherwise, an image captioning model generates and returns a new caption.\n",
      "otherwise, an image captioning model generates and returns a new caption.\n",
      "• maintainability: generation models often necessitate careful fine-tuning to tailor them for new\n",
      "applications. in contrast, retrieval-based methods can be improved to address new demands by\n",
      "simply enlarging the size and enhancing the quality of retrieval sources.\n",
      "we plan to broaden the application of this strategy to include other modalities, such as video and\n",
      "we plan to broaden the application of this strategy to include other modalities, such as video and\n",
      "speech, while also exploring efficient and effective cross-modal retrieval techniques.\n",
      "6\n",
      "conclusion\n",
      "in this study, we aim to identify optimal practices for implementing retrieval-augmented generation\n",
      "in order to improve the quality and reliability of content produced by large language models. we\n",
      "systematically assessed a range of potential solutions for each module within the rag framework\n",
      "systematically assessed a range of potential solutions for each module within the rag framework\n",
      "and recommended the most effective approach for each module. furthermore, we introduced a\n",
      "comprehensive evaluation benchmark for rag systems and conducted extensive experiments to\n",
      "determine the best practices among various alternatives. our findings not only contribute to a deeper\n",
      "understanding of retrieval-augmented generation systems but also establish a foundation for future\n",
      "research.\n",
      "limitations\n",
      "research.\n",
      "limitations\n",
      "we have evaluated the impact of various methods for fine-tuning llm generators. previous studies\n",
      "have demonstrated the feasibility of training both the retriever and generator jointly. we would\n",
      "like to explore this possibility in the future. in this study, we embraced the principle of modular\n",
      "13\n",
      "design to simplify the search for optimal rag implementations, thereby reducing complexity. due\n",
      "to the daunting costs associated with constructing vector databases and conducting experiments, our\n",
      "evaluation was limited to investigating the effectiveness and influence of representative chunking\n",
      "techniques within the chunking module. it would be intriguing to further explore the impact of\n",
      "different chunking techniques on the entire rag systems. while we have discussed the application of\n",
      "different chunking techniques on the entire rag systems. while we have discussed the application of\n",
      "rag in the domain of nlp and extended its scope to image generation, an enticing avenue for future\n",
      "exploration would involve expanding this research to other modalities such as speech and video.\n",
      "acknowledgments\n",
      "the authors would like to thank the anonymous reviewers for their valuable comments. this work\n",
      "was supported by national natural science foundation of china (no. 62076068).\n",
      "references\n",
      "was supported by national natural science foundation of china (no. 62076068).\n",
      "references\n",
      "[1] long ouyang, jeff wu, xu jiang, diogo almeida, carroll l. wainwright, pamela mishkin,\n",
      "chong zhang, sandhini agarwal, katarina slama, alex ray, john schulman, jacob hilton,\n",
      "fraser kelton, luke miller, maddie simens, amanda askell, peter welinder, paul christiano,\n",
      "jan leike, and ryan lowe. training language models to follow instructions with human\n",
      "jan leike, and ryan lowe. training language models to follow instructions with human\n",
      "feedback. in proceedings of the conference on neural information processing systems (neurips\n",
      "2022), 2022.\n",
      "[2] rafael rafailov, archit sharma, eric mitchell, stefano ermon, christopher d manning, and\n",
      "chelsea finn. direct preference optimization: your language model is secretly a reward model.\n",
      "arxiv preprint arxiv:2305.18290, 2023.\n",
      "arxiv preprint arxiv:2305.18290, 2023.\n",
      "[3] yao zhao, rishabh joshi, tianqi liu, misha khalman, mohammad saleh, and peter j liu. slic-\n",
      "hf: sequence likelihood calibration with human feedback. arxiv preprint arxiv:2305.10425,\n",
      "2023.\n",
      "[4] zheng yuan, hongyi yuan, chuanqi tan, wei wang, songfang huang, and fei huang. rrhf:\n",
      "rank responses to align language models with human feedback without tears. arxiv preprint\n",
      "arxiv:2304.05302, 2023.\n",
      "arxiv:2304.05302, 2023.\n",
      "[5] wenhao liu, xiaohua wang, muling wu, tianlong li, changze lv, zixuan ling, jianhao zhu,\n",
      "cenyuan zhang, xiaoqing zheng, and xuanjing huang. aligning large language models with\n",
      "human preferences through representation engineering. arxiv preprint arxiv:2312.15997, 2023.\n",
      "[6] yunfan gao, yun xiong, xinyu gao, kangxiang jia, jinliu pan, yuxi bi, yi dai, jiawei sun,\n",
      "and haofen wang. retrieval-augmented generation for large language models: a survey. arxiv\n",
      "and haofen wang. retrieval-augmented generation for large language models: a survey. arxiv\n",
      "preprint arxiv:2312.10997, 2023.\n",
      "[7] huayang li, yixuan su, deng cai, yan wang, and lemao liu. a survey on retrieval-augmented\n",
      "text generation. arxiv preprint arxiv:2202.01110, 2022.\n",
      "[8] deng cai, yan wang, lemao liu, and shuming shi. recent advances in retrieval-augmented\n",
      "text generation. in proceedings of the 45th international acm sigir conference on research\n",
      "text generation. in proceedings of the 45th international acm sigir conference on research\n",
      "and development in information retrieval, pages 3417–3419, 2022.\n",
      "[9] xinbei ma, yeyun gong, pengcheng he, hai zhao, and nan duan. query rewriting for\n",
      "retrieval-augmented large language models. arxiv preprint arxiv:2305.14283, 2023.\n",
      "[10] luyu gao, xueguang ma, jimmy lin, and jamie callan. precise zero-shot dense retrieval\n",
      "without relevance labels. arxiv preprint arxiv:2212.10496, 2022.\n",
      "without relevance labels. arxiv preprint arxiv:2212.10496, 2022.\n",
      "[11] liang wang, nan yang, xiaolong huang, binxing jiao, linjun yang, daxin jiang, rangan\n",
      "majumder, and furu wei. text embeddings by weakly-supervised contrastive pre-training.\n",
      "arxiv preprint arxiv:2212.03533, 2022.\n",
      "[12] shitao xiao, zheng liu, peitian zhang, and niklas muennighoff. c-pack: packaged resources\n",
      "to advance general chinese embedding, 2023.\n",
      "14\n",
      "[13] openai. gpt-4 technical report. corr, abs/2303.08774, 2023. doi: 10.48550/arxiv.2303.\n",
      "08774. url https://doi.org/10.48550/arxiv.2303.08774.\n",
      "[14] hugo touvron, thibaut lavril, gautier izacard, xavier martinet, marie-anne lachaux, timo-\n",
      "thée lacroix, baptiste rozière, naman goyal, eric hambro, faisal azhar, et al. llama: open\n",
      "and efficient foundation language models. arxiv preprint arxiv:2302.13971, 2023.\n",
      "and efficient foundation language models. arxiv preprint arxiv:2302.13971, 2023.\n",
      "[15] yue zhang, yafu li, leyang cui, deng cai, lemao liu, tingchen fu, xinting huang, enbo\n",
      "zhao, yu zhang, yulong chen, et al. siren’s song in the ai ocean: a survey on hallucination in\n",
      "large language models. arxiv preprint arxiv:2309.01219, 2023.\n",
      "[16] xiaohua wang, yuliang yan, longtao huang, xiaoqing zheng, and xuan-jing huang. halluci-\n",
      "[16] xiaohua wang, yuliang yan, longtao huang, xiaoqing zheng, and xuan-jing huang. halluci-\n",
      "nation detection for generative large language models by bayesian sequential estimation. in\n",
      "proceedings of the 2023 conference on empirical methods in natural language processing,\n",
      "pages 15361–15371, 2023.\n",
      "[17] liang wang, nan yang, and furu wei. query2doc: query expansion with large language\n",
      "models. arxiv preprint arxiv:2303.07678, 2023.\n",
      "models. arxiv preprint arxiv:2303.07678, 2023.\n",
      "[18] gangwoo kim, sungdong kim, byeongguk jeon, joonsuk park, and jaewoo kang. tree of\n",
      "clarifications: answering ambiguous questions with retrieval-augmented large language models.\n",
      "arxiv preprint arxiv:2310.14696, 2023.\n",
      "[19] jerry liu. llamaindex, 11 2022. url https://github.com/jerryjliu/llama_index.\n",
      "[20] peitian zhang, shitao xiao, zheng liu, zhicheng dou, and jian-yun nie. retrieve anything to\n",
      "[20] peitian zhang, shitao xiao, zheng liu, zhicheng dou, and jian-yun nie. retrieve anything to\n",
      "augment large language models. arxiv preprint arxiv:2310.07554, 2023.\n",
      "[21] zehan li, xin zhang, yanzhao zhang, dingkun long, pengjun xie, and meishan zhang.\n",
      "towards general text embeddings with multi-stage contrastive learning.\n",
      "arxiv preprint\n",
      "arxiv:2308.03281, 2023.\n",
      "[22] huiqiang jiang, qianhui wu, chin-yew lin, yuqing yang, and lili qiu.\n",
      "llmlingua:\n",
      "[22] huiqiang jiang, qianhui wu, chin-yew lin, yuqing yang, and lili qiu.\n",
      "llmlingua:\n",
      "compressing prompts for accelerated inference of large language models. arxiv preprint\n",
      "arxiv:2310.05736, 2023.\n",
      "[23] fangyuan xu, weijia shi, and eunsol choi. recomp: improving retrieval-augmented lms with\n",
      "compression and selective augmentation. arxiv preprint arxiv:2310.04408, 2023.\n",
      "[24] zhiruo wang, jun araki, zhengbao jiang, md rizwan parvez, and graham neubig. learning\n",
      "[24] zhiruo wang, jun araki, zhengbao jiang, md rizwan parvez, and graham neubig. learning\n",
      "to filter context for retrieval-augmented generation. arxiv preprint arxiv:2311.08377, 2023.\n",
      "[25] rodrigo nogueira, wei yang, kyunghyun cho, and jimmy lin. multi-stage document ranking\n",
      "with bert. arxiv preprint arxiv:1910.14424, 2019.\n",
      "[26] rodrigo nogueira, zhiying jiang, and jimmy lin.\n",
      "document ranking with a pretrained\n",
      "sequence-to-sequence model. arxiv preprint arxiv:2003.06713, 2020.\n",
      "sequence-to-sequence model. arxiv preprint arxiv:2003.06713, 2020.\n",
      "[27] xueguang ma, liang wang, nan yang, furu wei, and jimmy lin. fine-tuning llama for\n",
      "multi-stage text retrieval. arxiv preprint arxiv:2310.08319, 2023.\n",
      "[28] shengyao zhuang and guido zuccon. tilde: term independent likelihood model for passage\n",
      "re-ranking. in proceedings of the 44th international acm sigir conference on research and\n",
      "development in information retrieval, pages 1483–1492, 2021.\n",
      "development in information retrieval, pages 1483–1492, 2021.\n",
      "[29] shengyao zhuang and guido zuccon. fast passage re-ranking with contextualized exact term\n",
      "matching and efficient passage expansion. arxiv preprint arxiv:2108.08513, 2021.\n",
      "[30] hongyin luo, yung-sung chuang, yuan gong, tianhua zhang, yoon kim, xixin wu, danny\n",
      "fox, helen m. meng, and james r. glass. sail: search-augmented instruction learning.\n",
      "in conference on empirical methods in natural language processing, 2023. url https:\n",
      "in conference on empirical methods in natural language processing, 2023. url https:\n",
      "//api.semanticscholar.org/corpusid:258865283.\n",
      "15\n",
      "[31] tianjun zhang, shishir g. patil, naman jain, sheng shen, matei a. zaharia, ion stoica,\n",
      "and joseph e. gonzalez. raft: adapting language model to domain specific rag. arxiv,\n",
      "abs/2403.10131, 2024.\n",
      "[32] zihan liu, wei ping, rajarshi roy, peng xu, chankyu lee, mohammad shoeybi, and bryan\n",
      "catanzaro. chatqa: surpassing gpt-4 on conversational qa and rag. 2024. url https:\n",
      "//api.semanticscholar.org/corpusid:267035133.\n",
      "//api.semanticscholar.org/corpusid:267035133.\n",
      "[33] gautier izacard, patrick lewis, maria lomeli, lucas hosseini, fabio petroni, timo schick,\n",
      "jane a. yu, armand joulin, sebastian riedel, and edouard grave. few-shot learning with\n",
      "retrieval augmented language models. arxiv, abs/2208.03299, 2022.\n",
      "[34] lingxi zhang, yue yu, kuan wang, and chao zhang. arl2: aligning retrievers for black-box\n",
      "large language models via self-guided adaptive relevance labeling. arxiv, abs/2402.13542,\n",
      "2024.\n",
      "large language models via self-guided adaptive relevance labeling. arxiv, abs/2402.13542,\n",
      "2024.\n",
      "[35] weijia shi, sewon min, michihiro yasunaga, minjoon seo, rich james, mike lewis, luke\n",
      "zettlemoyer, and wen-tau yih. replug: retrieval-augmented black-box language models. arxiv\n",
      "preprint arxiv:2301.12652, 2023.\n",
      "[36] kelvin guu, kenton lee, zora tung, panupong pasupat, and ming-wei chang.\n",
      "realm:\n",
      "retrieval-augmented language model pre-training. arxiv, abs/2002.08909, 2020.\n",
      "realm:\n",
      "retrieval-augmented language model pre-training. arxiv, abs/2002.08909, 2020.\n",
      "[37] xi victoria lin, xilun chen, mingda chen, weijia shi, maria lomeli, rich james, pedro\n",
      "rodriguez, jacob kahn, gergely szilvasy, mike lewis, luke zettlemoyer, and scott yih.\n",
      "ra-dit: retrieval-augmented dual instruction tuning. arxiv, abs/2310.01352, 2023.\n",
      "[38] hamed zamani and michael bendersky. stochastic rag: end-to-end retrieval-augmented gen-\n",
      "[38] hamed zamani and michael bendersky. stochastic rag: end-to-end retrieval-augmented gen-\n",
      "eration through expected utility maximization. 2024. url https://api.semanticscholar.\n",
      "org/corpusid:269605438.\n",
      "[39] yizheng huang and jimmy huang. a survey on retrieval-augmented text generation for large\n",
      "language models. arxiv preprint arxiv:2404.10981, 2024.\n",
      "[40] ruochen zhao, hailin chen, weishi wang, fangkai jiao, xuan long do, chengwei qin,\n",
      "[40] ruochen zhao, hailin chen, weishi wang, fangkai jiao, xuan long do, chengwei qin,\n",
      "bosheng ding, xiaobao guo, minzhi li, xingxuan li, et al. retrieving multimodal information\n",
      "for augmented generation: a survey. arxiv preprint arxiv:2303.10868, 2023.\n",
      "[41] penghao zhao, hailin zhang, qinhan yu, zhengren wang, yunteng geng, fangcheng fu, ling\n",
      "yang, wentao zhang, and bin cui. retrieval-augmented generation for ai-generated content: a\n",
      "survey. arxiv preprint arxiv:2402.19473, 2024.\n",
      "survey. arxiv preprint arxiv:2402.19473, 2024.\n",
      "[42] michael günther, jackmin ong, isabelle mohr, alaeddine abdessalem, tanguy abel, moham-\n",
      "mad kalim akram, susana guzman, georgios mastrapas, saba sturua, bo wang, et al. jina\n",
      "embeddings 2: 8192-token general-purpose text embeddings for long documents. arxiv preprint\n",
      "arxiv:2310.19923, 2023.\n",
      "[43] llamaindex. llamaindex website. https://www.llamaindex.com. accessed: 2024-06-08.\n",
      "[43] llamaindex. llamaindex website. https://www.llamaindex.com. accessed: 2024-06-08.\n",
      "[44] kunal sawarkar, abhilasha mangal, and shivam raj solanki. blended rag: improving rag\n",
      "(retriever-augmented generation) accuracy with semantic search and hybrid query-based retriev-\n",
      "ers. arxiv preprint arxiv:2404.07220, 2024.\n",
      "[45] gautier izacard, mathilde caron, lucas hosseini, sebastian riedel, piotr bojanowski, armand\n",
      "[45] gautier izacard, mathilde caron, lucas hosseini, sebastian riedel, piotr bojanowski, armand\n",
      "joulin, and edouard grave. unsupervised dense information retrieval with contrastive learning.\n",
      "arxiv preprint arxiv:2112.09118, 2021.\n",
      "[46] nandan thakur, nils reimers, andreas rücklé, abhishek srivastava, and iryna gurevych. beir:\n",
      "a heterogenous benchmark for zero-shot evaluation of information retrieval models. arxiv\n",
      "preprint arxiv:2104.08663, 2021.\n",
      "preprint arxiv:2104.08663, 2021.\n",
      "[47] payal bajaj, daniel campos, nick craswell, li deng, jianfeng gao, xiaodong liu, rangan\n",
      "majumder, andrew mcnamara, bhaskar mitra, tri nguyen, et al. ms marco: a human\n",
      "generated machine reading comprehension dataset. arxiv preprint arxiv:1611.09268, 2016.\n",
      "16\n",
      "[48] nelson f liu, kevin lin, john hewitt, ashwin paranjape, michele bevilacqua, fabio petroni,\n",
      "and percy liang. lost in the middle: how language models use long contexts. transactions of\n",
      "the association for computational linguistics, 12:157–173, 2024.\n",
      "[49] huiqiang jiang, qianhui wu, xufang luo, dongsheng li, chin-yew lin, yuqing yang, and\n",
      "lili qiu. longllmlingua: accelerating and enhancing llms in long context scenarios via prompt\n",
      "compression. arxiv preprint arxiv:2310.06839, 2023.\n",
      "compression. arxiv preprint arxiv:2310.06839, 2023.\n",
      "[50] hugo touvron, louis martin, kevin r. stone, peter albert, amjad almahairi, yasmine babaei,\n",
      "nikolay bashlykov, soumya batra, prajjwal bhargava, shruti bhosale, daniel m. bikel, lukas\n",
      "blecher, cristian cantón ferrer, moya chen, guillem cucurull, david esiobu, jude fernandes,\n",
      "jeremy fu, wenyin fu, brian fuller, cynthia gao, vedanuj goswami, naman goyal, anthony s.\n",
      "jeremy fu, wenyin fu, brian fuller, cynthia gao, vedanuj goswami, naman goyal, anthony s.\n",
      "hartshorn, saghar hosseini, rui hou, hakan inan, marcin kardas, viktor kerkez, madian\n",
      "khabsa, isabel m. kloumann, a. v. korenev, punit singh koura, marie-anne lachaux, thibaut\n",
      "lavril, jenya lee, diana liskovich, yinghai lu, yuning mao, xavier martinet, todor mihaylov,\n",
      "pushkar mishra, igor molybog, yixin nie, andrew poulton, jeremy reizenstein, rashi rungta,\n",
      "pushkar mishra, igor molybog, yixin nie, andrew poulton, jeremy reizenstein, rashi rungta,\n",
      "kalyan saladi, alan schelten, ruan silva, eric michael smith, r. subramanian, xia tan, binh\n",
      "tang, ross taylor, adina williams, jian xiang kuan, puxin xu, zhengxu yan, iliyan zarov,\n",
      "yuchen zhang, angela fan, melanie kambadur, sharan narang, aurelien rodriguez, robert\n",
      "stojnic, sergey edunov, and thomas scialom. llama 2: open foundation and fine-tuned chat\n",
      "models. arxiv, abs/2307.09288, 2023.\n",
      "models. arxiv, abs/2307.09288, 2023.\n",
      "[51] es shahul, jithin james, luis espinosa anke, and steven schockaert. ragas: automated\n",
      "evaluation of retrieval augmented generation. in conference of the european chapter of the\n",
      "association for computational linguistics, 2023. url https://api.semanticscholar.org/\n",
      "corpusid:263152733.\n",
      "[52] harsh trivedi, niranjan balasubramanian, tushar khot, and ashish sabharwal. musique:\n",
      "[52] harsh trivedi, niranjan balasubramanian, tushar khot, and ashish sabharwal. musique:\n",
      "multihop questions via single-hop question composition. transactions of the association\n",
      "for computational linguistics, page 539–554, may 2022. doi: 10.1162/tacl_a_00475. url\n",
      "http://dx.doi.org/10.1162/tacl_a_00475.\n",
      "[53] mike conover, matt hayes, ankit mathur, jianwei xie, jun wan, sam shah, ali ghodsi,\n",
      "patrick wendell, matei zaharia, and reynold xin. free dolly: introducing the world’s first truly\n",
      "patrick wendell, matei zaharia, and reynold xin. free dolly: introducing the world’s first truly\n",
      "open instruction-tuned llm, 2023. url https://www.databricks.com/blog/2023/04/12/\n",
      "dolly-first-open-commercially-viable-instruction-tuned-llm.\n",
      "[54] nick craswell, bhaskar mitra, emine yilmaz, daniel fernando campos, and ellen m. voorhees.\n",
      "overview of the trec 2019 deep learning track. arxiv, abs/2003.07820, 2020. url https:\n",
      "//api.semanticscholar.org/corpusid:253234683.\n",
      "//api.semanticscholar.org/corpusid:253234683.\n",
      "[55] nick craswell, bhaskar mitra, emine yilmaz, daniel fernando campos, and ellen m. voorhees.\n",
      "overview of the trec 2020 deep learning track. arxiv, abs/2102.07662, 2021. url https:\n",
      "//api.semanticscholar.org/corpusid:212737158.\n",
      "[56] jimmy lin, xueguang ma, sheng-chieh lin, jheng-hong yang, ronak pradeep, and rodrigo\n",
      "nogueira. pyserini: a python toolkit for reproducible information retrieval research with sparse\n",
      "nogueira. pyserini: a python toolkit for reproducible information retrieval research with sparse\n",
      "and dense representations. in proceedings of the 44th international acm sigir conference on\n",
      "research and development in information retrieval, pages 2356–2362, 2021.\n",
      "[57] tom kwiatkowski, jennimaria palomaki, olivia redfield, michael collins, ankur p. parikh,\n",
      "chris alberti, danielle epstein, illia polosukhin, jacob devlin, kenton lee, kristina toutanova,\n",
      "chris alberti, danielle epstein, illia polosukhin, jacob devlin, kenton lee, kristina toutanova,\n",
      "llion jones, matthew kelcey, ming-wei chang, andrew m. dai, jakob uszkoreit, quoc v. le,\n",
      "and slav petrov. natural questions: a benchmark for question answering research. transactions\n",
      "of the association for computational linguistics, 7:453–466, 2019.\n",
      "[58] mandar joshi, eunsol choi, daniel s. weld, and luke zettlemoyer. triviaqa: a large scale\n",
      "[58] mandar joshi, eunsol choi, daniel s. weld, and luke zettlemoyer. triviaqa: a large scale\n",
      "distantly supervised challenge dataset for reading comprehension. arxiv, abs/1705.03551,\n",
      "2017.\n",
      "[59] zhilin yang, peng qi, saizheng zhang, yoshua bengio, william w cohen, ruslan salakhut-\n",
      "dinov, and christopher d manning. hotpotqa: a dataset for diverse, explainable multi-hop\n",
      "question answering. arxiv preprint arxiv:1809.09600, 2018.\n",
      "17\n",
      "[60] ivan stelmakh, yi luan, bhuwan dhingra, and ming-wei chang. asqa: factoid questions meet\n",
      "long-form answers. arxiv, abs/2204.06092, 2022.\n",
      "[61] tomáš koˇ\n",
      "cisk`\n",
      "y, jonathan schwarz, phil blunsom, chris dyer, karl moritz hermann, gábor\n",
      "melis, and edward grefenstette. the narrativeqa reading comprehension challenge. transac-\n",
      "tions of the association for computational linguistics, 6:317–328, 2018.\n",
      "[62] pranav rajpurkar, jian zhang, konstantin lopyrev, and percy liang. squad: 100,000+ questions\n",
      "[62] pranav rajpurkar, jian zhang, konstantin lopyrev, and percy liang. squad: 100,000+ questions\n",
      "for machine comprehension of text. arxiv preprint arxiv:1606.05250, 2016.\n",
      "[63] stephanie lin, jacob hilton, and owain evans. truthfulqa: measuring how models mimic\n",
      "human falsehoods. arxiv preprint arxiv:2109.07958, 2021.\n",
      "[64] j. edward hu, yelong shen, phillip wallis, zeyuan allen-zhu, yuanzhi li, shean wang, and\n",
      "[64] j. edward hu, yelong shen, phillip wallis, zeyuan allen-zhu, yuanzhi li, shean wang, and\n",
      "weizhu chen. lora: low-rank adaptation of large language models. arxiv, abs/2106.09685,\n",
      "2021.\n",
      "[65] dan hendrycks, collin burns, steven basart, andy zou, mantas mazeika, dawn song, and\n",
      "jacob steinhardt. measuring massive multitask language understanding. cornell university -\n",
      "arxiv,cornell university - arxiv, sep 2020.\n",
      "arxiv,cornell university - arxiv, sep 2020.\n",
      "[66] peter clark, isaac cowhey, oren etzioni, tushar khot, ashish sabharwal, carissa schoenick,\n",
      "and oyvind tafjord. think you have solved question answering? try arc, the ai2 reasoning chal-\n",
      "lenge. arxiv, abs/1803.05457, 2018. url https://api.semanticscholar.org/corpusid:\n",
      "3922816.\n",
      "[67] todor mihaylov, peter clark, tushar khot, and ashish sabharwal.\n",
      "can a suit of armor\n",
      "3922816.\n",
      "[67] todor mihaylov, peter clark, tushar khot, and ashish sabharwal.\n",
      "can a suit of armor\n",
      "conduct electricity? a new dataset for open book question answering. in proceedings of\n",
      "the 2018 conference on empirical methods in natural language processing, jan 2018. doi:\n",
      "10.18653/v1/d18-1260. url http://dx.doi.org/10.18653/v1/d18-1260.\n",
      "[68] james thorne, andreas vlachos, christos christodoulopoulos, and arpit mittal. fever: a\n",
      "[68] james thorne, andreas vlachos, christos christodoulopoulos, and arpit mittal. fever: a\n",
      "large-scale dataset for fact extraction and verification. arxiv, abs/1803.05355, 2018. url\n",
      "https://api.semanticscholar.org/corpusid:4711425.\n",
      "[69] tianhua zhang, hongyin luo, yung-sung chuang, wei fang, luc gaitskell, thomas\n",
      "hartvigsen, xixin wu, danny fox, helen m. meng, and james r. glass. interpretable unified\n",
      "language checking. arxiv, abs/2304.03728, 2023. url https://api.semanticscholar.\n",
      "language checking. arxiv, abs/2304.03728, 2023. url https://api.semanticscholar.\n",
      "org/corpusid:258041307.\n",
      "[70] jonathan berant, andrew chou, roy frostig, and percy liang. semantic parsing on freebase\n",
      "from question-answer pairs. empirical methods in natural language processing,empirical\n",
      "methods in natural language processing, oct 2013.\n",
      "[71] xanh ho, a. nguyen, saku sugawara, and akiko aizawa. constructing a multi-hop qa\n",
      "[71] xanh ho, a. nguyen, saku sugawara, and akiko aizawa. constructing a multi-hop qa\n",
      "dataset for comprehensive evaluation of reasoning steps. arxiv, abs/2011.01060, 2020. url\n",
      "https://api.semanticscholar.org/corpusid:226236740.\n",
      "[72] ofir press, muru zhang, sewon min, ludwig schmidt, noaha. smith, and mike lewis.\n",
      "measuring and narrowing the compositionality gap in language models. oct 2022.\n",
      "[73] qiao jin, bhuwan dhingra, zhengping liu, william w. cohen, and xinghua lu. pubmedqa: a\n",
      "[73] qiao jin, bhuwan dhingra, zhengping liu, william w. cohen, and xinghua lu. pubmedqa: a\n",
      "dataset for biomedical research question answering. in conference on empirical methods in\n",
      "natural language processing, 2019. url https://api.semanticscholar.org/corpusid:\n",
      "202572622.\n",
      "[74] akari asai, zeqiu wu, yizhong wang, avirup sil, and hannaneh hajishirzi. self-rag: learning\n",
      "to retrieve, generate, and critique through self-reflection. arxiv preprint arxiv:2310.11511,\n",
      "2023.\n",
      "18\n",
      "a\n",
      "experimental details\n",
      "in this section, we provide detailed experimental settings for each module, covering dataset specifics,\n",
      "training parameters, and any additional experimental results.\n",
      "a.1\n",
      "query classification\n",
      "datasets\n",
      "we utilized a subset of the databricks-dolly-15k [53] and generated additional data\n",
      "using gpt-4.the prompt template for generating questions is shown in table 14.\n",
      "implementation details\n",
      "we choose bert-base-multilingual-cased as our classifier, with a batch\n",
      "implementation details\n",
      "we choose bert-base-multilingual-cased as our classifier, with a batch\n",
      "size of 16 and a learning rate of 1e-5. the evaluation of results is showcased in table 1.\n",
      "a.2\n",
      "experimental details of retrieval methods\n",
      "implementation details of the comparative experiments of different retrieval methods are as below:\n",
      "datasets\n",
      "we use the trec dl 2019 [54] and 2020 [55] passage ranking datasets to evaluate the\n",
      "performance of different retrieval methods.\n",
      "metrics\n",
      "performance of different retrieval methods.\n",
      "metrics\n",
      "widely-used evaluation metrics for retrieval include map, ndcg@10, r@50 and r@1k.\n",
      "both map and ndcg@10 are order-aware metrics that take the ranking of search results into account.\n",
      "in contrast, r@k is an order-unaware metric. we also report the average latency incurred by each\n",
      "method per query.\n",
      "implementation details\n",
      "for sparse retrieval, we use the bm25 algorithm, which relies on the tf-\n",
      "implementation details\n",
      "for sparse retrieval, we use the bm25 algorithm, which relies on the tf-\n",
      "idf algorithm. for dense retrieval, we employ contriever as our unsupervised contrastive text encoder.\n",
      "based on our evaluation of embedding models, we implement our supervised dense retrieval using\n",
      "llm-embedder. we use the default implementation of bm25 and contriever from pyserini [56].\n",
      "the bm25 index is constructed using lucene on ms marco collections, while the dense vector\n",
      "the bm25 index is constructed using lucene on ms marco collections, while the dense vector\n",
      "index is generated with faiss employing flat configuration on the same dataset. for query rewriting,\n",
      "we prompt zephyr-7b-alpha9, a model trained to act as a helpful assistant, to rewrite the original\n",
      "query. for query decomposition, we employ gpt-3.5-turbo-0125 to break down the original query\n",
      "into multiple sub-queries. we closely follow the implementation from hyde [10], utilizing the more\n",
      "into multiple sub-queries. we closely follow the implementation from hyde [10], utilizing the more\n",
      "advanced instruction-following language model, gpt-3.5-turbo-instruct, to generate hypothetical\n",
      "answers. the model infers with a default temperature of 0.7, sampling up to a maximum of 512\n",
      "tokens. retrieval experiments and evaluation are conducted using the pyserini toolkit.\n",
      "a.3\n",
      "experimental details of reranking methods\n",
      "datasets\n",
      "a.3\n",
      "experimental details of reranking methods\n",
      "datasets\n",
      "our experiments utilize the ms marco passage ranking dataset, a substantial corpus\n",
      "designed for machine reading comprehension tasks. this dataset comprises over 8.8 million passages\n",
      "and 1 million queries. the training set contains approximately 398m tuples of queries paired with\n",
      "corresponding positive and negative passages, while the development set comprises 6,980 queries,\n",
      "corresponding positive and negative passages, while the development set comprises 6,980 queries,\n",
      "paired with their bm25 retrieval results, and preserves the top-1000 ranked candidate passages for\n",
      "each query. we evaluate the effectiveness of the methods on the development set, as the test set is not\n",
      "publicly available.\n",
      "metrics\n",
      "the evaluation metrics mrr@1, mrr@10, mrr@1k and hit rate@10 are used.\n",
      "mrr@10 is the official metric proposed by ms marco.\n",
      "implementation details\n",
      "mrr@10 is the official metric proposed by ms marco.\n",
      "implementation details\n",
      "we follow and make modifications to the implementation provided by\n",
      "pygaggle [26] and tilde [28]. for dlm-based reranking, we use monot5 [26] based on t5-base,\n",
      "monobert [25] based on bert-large and rankllama [27] based on llama-2-7b. for tilde\n",
      "reranking, we use tildev2 [29] based on bert-base.\n",
      "typically, 50 documents are retrieved as input for the reranking module. the documents remaining\n",
      "typically, 50 documents are retrieved as input for the reranking module. the documents remaining\n",
      "after the reranking and repacking phase can be further concentrated by assigning a top-k value or a\n",
      "relevancy score threshold.\n",
      "result analysis\n",
      "reranking results are shown in table 9. we compare our results with a randomly\n",
      "shuffled ordering and the bm25 retrieval baseline. all reranking methods demonstrate a notable\n",
      "9https://huggingface.co/huggingfaceh4/zephyr-7b-alpha\n",
      "19\n",
      "context\n",
      "model\n",
      "nq\n",
      "triviaqa\n",
      "hotpotqa\n",
      "asqa\n",
      "avg.\n",
      "d∅\n",
      "mb\n",
      "29.78\n",
      "60.44\n",
      "23.73\n",
      "37.89\n",
      "37.96\n",
      "mg\n",
      "26.23\n",
      "58.26\n",
      "26.67\n",
      "32.30\n",
      "35.87\n",
      "mr\n",
      "31.10\n",
      "61.37\n",
      "28.40\n",
      "39.96\n",
      "40.21\n",
      "mgr\n",
      "25.92\n",
      "57.62\n",
      "26.43\n",
      "32.99\n",
      "35.70\n",
      "mgg\n",
      "26.69\n",
      "58.07\n",
      "27.04\n",
      "33.75\n",
      "36.39\n",
      "dg\n",
      "mb\n",
      "44.78\n",
      "79.90\n",
      "56.72\n",
      "71.64\n",
      "63.26\n",
      "mg\n",
      "85.72\n",
      "88.16\n",
      "79.82\n",
      "85.51\n",
      "84.80\n",
      "mr\n",
      "60.98\n",
      "80.20\n",
      "65.73\n",
      "67.49\n",
      "68.60\n",
      "mgr\n",
      "87.60\n",
      "87.94\n",
      "81.07\n",
      "87.58\n",
      "86.05\n",
      "mgg\n",
      "86.72\n",
      "88.35\n",
      "79.59\n",
      "83.44\n",
      "84.53\n",
      "dr\n",
      "mb\n",
      "16.49\n",
      "50.03\n",
      "21.57\n",
      "28.79\n",
      "29.22\n",
      "mg\n",
      "22.15\n",
      "46.98\n",
      "24.36\n",
      "29.40\n",
      "30.72\n",
      "mr\n",
      "36.92\n",
      "58.42\n",
      "29.64\n",
      "39.54\n",
      "41.13\n",
      "mgr\n",
      "23.63\n",
      "50.03\n",
      "21.57\n",
      "28.79\n",
      "29.22\n",
      "mg\n",
      "22.15\n",
      "46.98\n",
      "24.36\n",
      "29.40\n",
      "30.72\n",
      "mr\n",
      "36.92\n",
      "58.42\n",
      "29.64\n",
      "39.54\n",
      "41.13\n",
      "mgr\n",
      "23.63\n",
      "45.01\n",
      "24.17\n",
      "27.95\n",
      "30.19\n",
      "mgg\n",
      "21.08\n",
      "43.83\n",
      "23.23\n",
      "27.33\n",
      "28.87\n",
      "dgr\n",
      "mb\n",
      "34.65\n",
      "81.27\n",
      "52.75\n",
      "65.42\n",
      "58.52\n",
      "mg\n",
      "85.00\n",
      "87.33\n",
      "78.18\n",
      "83.02\n",
      "83.38\n",
      "mr\n",
      "60.28\n",
      "79.32\n",
      "63.82\n",
      "67.29\n",
      "67.68\n",
      "mgr\n",
      "87.63\n",
      "87.14\n",
      "79.95\n",
      "87.78\n",
      "85.63\n",
      "mgg\n",
      "86.31\n",
      "86.90\n",
      "78.10\n",
      "83.85\n",
      "83.79\n",
      "table 12: results of the model augmented with different contexts on various qa datasets.\n",
      "83.79\n",
      "table 12: results of the model augmented with different contexts on various qa datasets.\n",
      "increase in performance across all metrics. approximately equal performance is achieved by monot5\n",
      "and monobert, and rankllama performs best, each ascending in latency. tildev2 is the fastest,\n",
      "taking approximately 10 to 20 milliseconds per query at the cost of performance. additionally,\n",
      "tildev2 requires that the passages reranked be identically included in the previously indexed\n",
      "tildev2 requires that the passages reranked be identically included in the previously indexed\n",
      "collection. preprocessing must be redone at inference for new unseen passages, negating the efficiency\n",
      "advantages.\n",
      "a.4\n",
      "experimental details of summarization methods\n",
      "selective context\n",
      "selective context enhances llm efficiency by identifying and removing\n",
      "redundant information in the input context. it evaluates the informativeness of lexical units using\n",
      "redundant information in the input context. it evaluates the informativeness of lexical units using\n",
      "self-information computed by a base causal language model. this method is non-query-based,\n",
      "allowing a comparison between query-based and non-query-based approaches.\n",
      "datasets\n",
      "we evaluated these methods on three datasets: natural questions (nq) [57], trivi-\n",
      "aqa [58], and hotpotqa [59].\n",
      "metrics\n",
      "evaluation metrics include the f1 score and the number of tokens changed after summa-\n",
      "metrics\n",
      "evaluation metrics include the f1 score and the number of tokens changed after summa-\n",
      "rization to measure conciseness.\n",
      "implementation details\n",
      "for all methods, we use llama3-8b-instruct as the generator model\n",
      "and set a summarization ratio of 0.4. for extractive methods, importance scores determine the\n",
      "sentences retained. for abstractive methods, we control the maximum generation length using the\n",
      "sentences retained. for abstractive methods, we control the maximum generation length using the\n",
      "summarization ratio to align with extractive methods. experiments are conducted on the nq test set,\n",
      "triviaqa test set, and hotpotqa development set.\n",
      "a.5\n",
      "experimental details of generator fine-tuning\n",
      "datasets\n",
      "we fine-tune our model on several question answering(qa) and reading comprehension\n",
      "datasets, including asqa [60], hotpotqa [59], narrativeqa [61], nq [57], squad [62], trivi-\n",
      "datasets, including asqa [60], hotpotqa [59], narrativeqa [61], nq [57], squad [62], trivi-\n",
      "aqa [58], truthfulqa [63]. we use their train splits (for those containing significantly more data\n",
      "20\n",
      "[instruction]\n",
      "please generate ten descriptions for the continuation task.\n",
      "[context]\n",
      "for example:\n",
      "1.“french.washington played a crucial role in the american revolutionary war, leading\n",
      "the continental army against the british.” please continue writing the above paragraph.\n",
      "2.“the discovery of the double helix structure of dna by james watson and francis\n",
      "crick revolutionized the field of genetics, laying the foundation for modern molecular\n",
      "crick revolutionized the field of genetics, laying the foundation for modern molecular\n",
      "biology and biotechnology.” please continue by discussing recent developments in\n",
      "genetic research, such as crispr gene editing, and their potential ethical implications.\n",
      "table 14: template for generating task classification data.\n",
      "entries than others, we conducted a random sample). for evaluation, asqa [60], hotpotqa [59],\n",
      "entries than others, we conducted a random sample). for evaluation, asqa [60], hotpotqa [59],\n",
      "nq [57], triviaqa [58] are used. we evaluate our model on their validation splits or manually split a\n",
      "dataset\n",
      "#train\n",
      "#eval\n",
      "asqa\n",
      "2, 090\n",
      "483\n",
      "hotpotqa\n",
      "15, 000\n",
      "7, 405\n",
      "triviaqa\n",
      "9, 000\n",
      "6, 368\n",
      "nq\n",
      "15, 000\n",
      "8, 006\n",
      "narrativeqa\n",
      "7, 000\n",
      "−−\n",
      "squad\n",
      "67, 00\n",
      "−−\n",
      "truthfulqa\n",
      "817\n",
      "−−\n",
      "table 13: number of examples in each dataset\n",
      "used in the fine-tuning experiments.\n",
      "subset from the training set to avoid overlapping.\n",
      "used in the fine-tuning experiments.\n",
      "subset from the training set to avoid overlapping.\n",
      "the exact number of entries in each train and\n",
      "test set is detailed in table 13.\n",
      "we use the dataset-provided documents as dgold\n",
      "for each data entry. to obtain drandom we sam-\n",
      "ple the context of different entries within the\n",
      "same dataset, to make sure the distributions of\n",
      "drandom and dgold are roughly similar.\n",
      "metrics\n",
      "we use the ground-truth coverage\n",
      "as our evaluation metric, considering that the\n",
      "metrics\n",
      "we use the ground-truth coverage\n",
      "as our evaluation metric, considering that the\n",
      "answers of qa tasks are relatively short, while\n",
      "the generation length of the model is sometimes hard to limit.\n",
      "implementation details\n",
      "we select llama-2-7b [50] as the base model. for efficiency, we use\n",
      "lora [64] and int8 quantization during training. the prompt templates used for fine-tuning and\n",
      "evaluation mainly follow lin et al. [37]. we train our generator for 3 epochs and constrain the\n",
      "evaluation mainly follow lin et al. [37]. we train our generator for 3 epochs and constrain the\n",
      "maximum length of the sequence to 1600, using a batch size of 4 and a learning rate of 5e-5. during\n",
      "testing, we use a zero-shot setting.\n",
      "detailed results\n",
      "table 12 shows our evaluation results on each dataset.\n",
      "a.6\n",
      "experimental details of comprehensive evaluation\n",
      "tasks and datasets\n",
      "we conducted extensive experiments across various nlp tasks and datasets to\n",
      "tasks and datasets\n",
      "we conducted extensive experiments across various nlp tasks and datasets to\n",
      "assess the performance of rag systems. specifically: (1) commonsense reasoning: we evaluated\n",
      "on mmlu [65], arc-challenge [66], and openbookqa [67] datasets. (2) fact checking: our\n",
      "evaluation encompassed the fever [68] and pubhealth [69] datasets. (3) open-domain qa:\n",
      "we assessed on nq [57], triviaqa [58], and webquestions [70] datasets. (4) multihop qa:\n",
      "we assessed on nq [57], triviaqa [58], and webquestions [70] datasets. (4) multihop qa:\n",
      "our evaluation included the hotpotqa [59], 2wikimultihopqa [71], and musique [52] datasets.\n",
      "for musique, we followed the approach outlined in [72] and focused solely on answerable 2-hop\n",
      "questions. (5) medical qa: we also assessed on the pubmedqa [73] dataset. in each dataset, we\n",
      "randomly sub-sample 500 entries from the test set for our experiments. for datasets without test set,\n",
      "we use develop set instead.\n",
      "we use develop set instead.\n",
      "to assess rag capabilities, we evenly collect a total of 500 entries from nq, triviaqa, hotpotqa,\n",
      "2wikimultihopqa and musique. each entry is a “question, gold document, gold answer” triple.\n",
      "metrics\n",
      "we use token-level f1 score and em score for open-domain qa and multihop qa tasks,\n",
      "and accuracy for others. we use a more lenient em score, which evaluates performance based on\n",
      "whether the model generations include gold answers instead of strictly exact matching [74].\n",
      "whether the model generations include gold answers instead of strictly exact matching [74].\n",
      "towards rag capabilities evaluation, we adopt four metrics from ragas, including faithfulness,\n",
      "context relevancy, answer relevancy, and answer correctness. faithfulness measures how\n",
      "factually consistent the generated answer is with the retrieved context. an answer is considered\n",
      "faithful if all claims made can be directly inferred from the provided context. context relevancy\n",
      "faithful if all claims made can be directly inferred from the provided context. context relevancy\n",
      "evaluates how relevant the retrieved context is to the original query. answer relevancy assesses the\n",
      "21\n",
      "pertinence of the generated answer to the original query. answer correctness involves the accuracy\n",
      "of the generated answer when compared to the ground truth. for example, context relevancy is\n",
      "calculated from the proportion of sentences within the retrieved context that are relevant for answering\n",
      "the given question to all sentences:\n",
      "context relevancy =\n",
      "|s|\n",
      "|total|\n",
      "(2)\n",
      "where |s| denotes the number of relevant sentences, |total| denotes the total number of sentences\n",
      "where |s| denotes the number of relevant sentences, |total| denotes the total number of sentences\n",
      "retrieved. all these metrics are evaluated using the ragas framework, with gpt-4 serving as the\n",
      "judge.\n",
      "additionally, we compute the cosine similarity between the retrieved document and the gold document\n",
      "as retrieval similarity. the retrieved document and gold document are fed into an embedding\n",
      "model, then the resulting embeddings are used to compute the cosine similarity.\n",
      "implementation details\n",
      "implementation details\n",
      "for open-domain qa and multihop qa datasets, we set the generation\n",
      "model’s maximum new token number to 100 tokens. for other datasets, we set it to 50 tokens. to\n",
      "deal with excessively long retrieved documents, we truncated the documents to 2048 words when\n",
      "evaluating rankllama and longllmlingua.\n",
      "for all datasets, we use greedy decoding during generation. to better compare the capabilities of\n",
      "for all datasets, we use greedy decoding during generation. to better compare the capabilities of\n",
      "different rag modules, we adopt the 0-shot evaluation setting, i.e., no in-context examples are\n",
      "offered. in the multiple choice and fact checking tasks, answers generated by the model may take\n",
      "a variety of forms (e.g., “the answer is a” instead of “a”). therefore, we preprocess the responses\n",
      "generated by the model, applying regular expression templates to match them with gold labels.\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "for doc in documents:\n",
    "    text_chunks = splitter.split_text(doc.page_content)\n",
    "    lower_chunks = [chunk.lower() for chunk in text_chunks]\n",
    "    chunks.extend(lower_chunks)\n",
    "\n",
    "for chunk in chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\musta\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\musta\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\musta\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "c:\\Users\\musta\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "docs = [Document(page_content=chunk) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "db2 = Chroma.from_documents(docs, embedding=embedding, persist_directory=\"./chroma_db\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\musta\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 0.4. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "db3 = Chroma(persist_directory=\"./chroma_db\", embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db3.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = BM25Retriever.from_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword.k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BM25Retriever(vectorizer=<rank_bm25.BM25Okapi object at 0x000001C00BD91450>, k=3)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[keyword, retriever],\n",
    "    retriever_weights=[0.5, 0.5] \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hybrid_chain = RetrievalQA.from_chain_type(\n",
    "#     llm, retriever=ensemble_retriever\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'what is the Best Practices for Implementing RAG?',\n",
       " 'result': 'According to the text, there are two distinct recipes or practices for implementing RAG systems:\\n\\n1. **Best Performance Practice**: To achieve the highest performance, it is recommended to incorporate the query classification module and implement the hybrid method for retrieval.\\n2. **Balanced Efficiency Practice**: To achieve a balance between performance and efficiency, it is recommended to incorporate the query classification module, use the hybrid method for retrieval, opt for TildeV2 for reranking, choose Reverse for repacking, and employ Recomp for summarization.\\n\\nThese practices are based on the experimental findings of this study.'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hybrid_chain.invoke(\"what is the Best Practices for Implementing RAG?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "COHERE_API_KEY = \"Q1tNApphbMywrTviu1WfdEYa3DfNr8NtwhlGAiYh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain_cohere import CohereRerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "api_key = getpass.getpass(\"Enter your Cohere API key: \")\n",
    "compressor = CohereRerank(cohere_api_key=api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=ensemble_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_chain = RetrievalQA.from_chain_type(\n",
    "    llm, retriever=compression_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'explain that topic with easy words and additionally add example for better understand:Hybrid Search with Different Weight on Sparse Retrieval?',\n",
       " 'result': 'Let me try to break it down in simpler terms:\\n\\n**What is Hybrid Search?**\\n\\nImagine you\\'re searching for a book online. You have two ways to find the book: one way is to look at the book\\'s title, author, and summary (like a \"sparse\" search), and another way is to analyze the book\\'s contents, like its keywords and themes (like a \"dense\" search). Hybrid Search combines these two methods to get the best results.\\n\\n**What does \"Different Weight on Sparse Retrieval\" mean?**\\n\\nIn Hybrid Search, you can adjust how much each method contributes to the final result. The weight refers to how much more important one method is compared to the other. For example:\\n\\n* If you set a high weight for sparse retrieval (looking at titles and summaries), it means that method will be more influential in determining the search results.\\n* If you set a low weight for sparse retrieval, the dense retrieval method (analyzing contents) will have more influence.\\n\\nThink of it like cooking. You\\'re making a recipe with two main ingredients: chicken and vegetables. You can adjust how much chicken versus veggies go into the dish by changing the \"weight\" of each ingredient. If you want more veggie flavor, you\\'d use more veggies!\\n\\nIn this context, the researchers are experimenting with different weights to find the best balance between sparse (title/summary) and dense (content-based) retrieval methods.\\n\\n**Example:**\\n\\nSuppose you\\'re searching for a passage about space exploration. You can set the weight of sparse retrieval high if you want the search results to prioritize passages that match the title \"Space Exploration\" or have relevant keywords like \"NASA\" or \"Mars\". If you set the weight low, the dense retrieval method (analyzing contents) will be more influential, and you might get results that discuss space exploration concepts, even if they don\\'t exactly match those specific keywords.\\n\\nDoes that help?'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_chain.invoke(\"explain that topic with easy words and additionally add example for better understand:Hybrid Search with Different Weight on Sparse Retrieval?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
